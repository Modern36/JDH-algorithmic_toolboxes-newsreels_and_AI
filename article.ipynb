{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "title"
    ]
   },
   "source": [
    "# Algorithmic toolboxes for the study of the filmic past—on newsreels & AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "contributor"
    ]
   },
   "source": [
    "### Mathias Johansson [![orcid](https://orcid.org/sites/default/files/images/orcid_16x16.png)](https://orcid.org/0000-0002-3338-0551) \n",
    "Department of Arts and Cultural Sciences, Lund University"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "contributor"
    ]
   },
   "source": [
    " ### Robert Aspenskog [![orcid](https://orcid.org/sites/default/files/images/orcid_16x16.png)](https://orcid.org/0009-0005-4720-3352) \n",
    "\n",
    "Department of Cultural Sciences, Linnaeus University \n",
    "\n",
    "Department of Cultural Sciences, Lund University\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "contributor"
    ]
   },
   "source": [
    "### Johan Malmstedt [![orcid](https://orcid.org/sites/default/files/images/orcid_16x16.png)](https://orcid.org/0000-0001-5207-4296) \n",
    "\n",
    "GRIDH, University of Gothenburg \n",
    "\n",
    "metaLAB, Harvard University"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "contributor"
    ]
   },
   "source": [
    "### Emil Stjernholm [![orcid](https://orcid.org/sites/default/files/images/orcid_16x16.png)](https://orcid.org/0000-0001-9871-5191) \n",
    "Department of Communication, Lund University"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "contributor"
    ]
   },
   "source": [
    "### Pelle Snickars [![orcid](https://orcid.org/sites/default/files/images/orcid_16x16.png)](https://orcid.org/0000-0001-5122-1549) \n",
    "Department of Arts and Cultural Sciences, Lund University"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "copyright"
    ]
   },
   "source": [
    "[![cc-by](https://licensebuttons.net/l/by/4.0/88x31.png)](https://creativecommons.org/licenses/by/4.0/) \n",
    "©<AUTHOR or ORGANIZATION / FUNDER>. Published by De Gruyter in cooperation with the University of Luxembourg Centre for Contemporary and Digital History. This is an Open Access article distributed under the terms of the [Creative Commons Attribution License CC-BY](https://creativecommons.org/licenses/by/4.0/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "cover"
    ]
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "metadata={\n",
    "    \"jdh\": {\n",
    "        \"module\": \"object\",\n",
    "        \"tags\": [\"figure-Gunnar-*\"],\n",
    "        \"object\": {\n",
    "            \"type\":\"image\",\n",
    "            \"source\": [\n",
    "                \"Gunnar Skoglund (1899–1983)—once hailed in the Swedish press \"\n",
    "                \"as a “speaking artist”—did hundreds of voice-over for the \"\n",
    "                \"national SF newsreel. At the time, his voice was arguably \"\n",
    "                \"the most familiar in Sweden. Skoglund was also a skilled \"\n",
    "                \"director, actor and producer of some thirty short films, \"\n",
    "                \"dating from the late 1920s to the 1950s.\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "display(Image(\"./media/img1.png\", width=1000), metadata=metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    " (optional) This article was orginally published (...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "keywords"
    ]
   },
   "source": [
    "FirstKeyword, SecondKeyword, AlwaysSeparatedByAComma, Media History, Newsreels, Multimodal analysis,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "abstract"
    ]
   },
   "source": [
    "Using a computational film studies framework, this article examines a major\n",
    "Swedish newsreel archive—the Journal Digital collection—deploying both signal\n",
    "archaeology, named entity recognition, and geocoding. We also apply a specific\n",
    "algorithmic toolbox on text extraction of intertitles, as well as a tweaked\n",
    "tool, _SweScribe_, based on automatic speech recognition, in order to both\n",
    "transcribe and timestamp speech from the collection’s sound films. Our basic\n",
    "idea is to construct a number of mid-sized datasets from the Journal Digital\n",
    "collection in different modalities, and proceed with an examination using\n",
    "various approaches. Consequently, our intention is to increase the scholarly\n",
    "capacity of media historical sources, while at the same time critically\n",
    "scrutinizing AI and algorithmic toolboxes for the study of the past. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "At the archive of Swedish Television (SVT) and Radio Sweden (SR) there are some\n",
    "thirty volumes of so-called speaker lists used for voice-over in the production\n",
    "of the national SF newsreel--_Svensk Filmindustris Veckorevy_ (_Svensk\n",
    "Filmindustri’s Weekly Review_). During the 1960s, the film company Svensk\n",
    "Filmindustri (SF) sold its non-fiction film and newsreel archive to public\n",
    "service radio and tv. The making of newsreels, however, had begun already in\n",
    "1914, when SF started producing a weekly newsreel in a similar fashion and\n",
    "format as in other countries. These newsreels were nationally distributed in\n",
    "dozens of copies across Sweden; during the silent era they naturally included\n",
    "textual intertitles. From 1932 and onwards sound was added; the preserved\n",
    "voice-over scripts in the SVT and SR vaults hence range from 1932 until 1959.\n",
    "These lists were as detailed as plentiful; today each volume in the archive\n",
    "includes some 150 manuscripts, making the total number of speaker lists to\n",
    "approximately 5,000. In a literal sense they were production manuscripts,\n",
    "almost all contained handwritten edits, and small commentary. All likely they\n",
    "served as the final manuscript for the person who did the voice-over commentary\n",
    "in the film studio, describing the length in seconds when text should be\n",
    "spoken, while also indicating what type of shot the edited newsreel depicted\n",
    "during each particular sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "display(Image(\"./media/img2.png\", width=1000), metadata={\n",
    "    \"jdh\": {\n",
    "        \"module\": \"object\",\n",
    "        \"object\": {\n",
    "            \"tags\": [\"figure-first-newsreel-*\"],\n",
    "            \"type\":\"image\",\n",
    "            \"source\": [\n",
    "                \"The first episode in a SF-newsreel from March 1933 depicted \"\n",
    "                \"the Swedish frigate Vanadis, a naval training ship. \"\n",
    "                \"The commentary was read by Gunnar Skoglund, and as is \"\n",
    "                \"evident from the preserved speaker list, it was meticulous, \"\n",
    "                \"customized in seconds, and almost identical to what was \"\n",
    "                \"heard in the film. In English translation Skoglund rapidly \"\n",
    "                \"announced: “Since the beginning of the 20th century, the \"\n",
    "                \"venerable frigate Vanadis has been anchored at Skeppsholmen \"\n",
    "                \"in Stockholm as a lodging and training ship. \"\n",
    "                \"In its disrigged hull, two generations of sailors have slept \"\n",
    "                \"the warrior’s heavy and well-deserved sleep in hammocks. \"\n",
    "                \"Lying in a bunk it is called in sailor’s language, but it is \"\n",
    "                \"not quite the same as sleeping in a bed at home. \"\n",
    "                \"If nothing else, the alarm clock is a little louder here”— \"\n",
    "                \"trumpet immediately!\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "For digital media historians, trying to analyse speaker lists from the SF\n",
    "archive as a textual dataset poses challenges. If one, for example, uploads an\n",
    "archival sample of a speaker lists to an AI-powered engine such as Perplexity,\n",
    "with the prompt to OCR a particular section, the result is not particularly\n",
    "impressive. The AI-model does a fair job with all typed Swedish text, but fails\n",
    "with handwritten notes as well as words that have been manually crossed out.\n",
    "Occasionally, Swedish words and letters even turn into Cyrillic script: “8 sek.\n",
    "Strömmen. Alltsedan mitte av 1890-talet karxxіях кирракX Momenxxxxsixxxkxxmxx”.\n",
    "Trying to work with the SF speaker lists in digitised form is hence scholarly\n",
    "difficult. These lists exhibit many of the common traits that archival\n",
    "documents often encompass; combined typed and handwritten text with corrections\n",
    "often hinders OCR, and structuring of data in correct ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "1pjw9": [
       {
        "id": "22783102/UK5SM8C2",
        "source": "zotero"
       }
      ]
     }
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "AI hallucination of a Swedish voice-over into Russian can of course also be\n",
    "seen as a token of how large language models generate information that is both\n",
    "historically inaccurate—and even fabricated. By now it is well known among\n",
    "historians that LLM’s often hallucinate about the past, particularly when more\n",
    "specific questions are posed. While it is true that generative AI does give apt\n",
    "textual answers, such models have repeatedly been critiqued when it comes to\n",
    "producing historical images. Fabian Offert has stressed that models such as\n",
    "CLIP or DALL-E find themselves in ”a triple bind: they suffer from syntactic\n",
    "invariability in the case of _generally_ historical prompts, semantic arbitrarity\n",
    "in the case of _specifically historical_ prompts, and superficial, corporate\n",
    "censorship that affects both” <cite id=\"1pjw9\"><a href=\"#zotero%7C22783102%2FUK5SM8C2\">(Offert, 2023)</a></cite>. The latter is,\n",
    "of course, particularly problematic. Nearly all forms of generative AI are\n",
    "circumscribed by commercial constraints; encoded values are neither epistemic\n",
    "nor scholarly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "568sd": []
     }
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Despite these AI shortcomings regarding the past, in this article we aim to\n",
    "analyse the SF-archive—today usually referred to as the Journal Digital\n",
    "collection—with a diverse set of algorithmic tools. Given our initial\n",
    "discussion, we have however refrained from digitising textual sources (such as\n",
    "the preserved SF speaker lists), and instead focused on the audiovisual\n",
    "material _per se_. Using a computational film studies framework <cite\n",
    "id=\"568sd\"><a href=\"#zotero%7C22783102%2FH4XMMWYS\">(Oiva et al.,\n",
    "2024)</a></cite>, this article hence examines the Journal Digital collection\n",
    "deploying both signal archaeology, named entity recognition and geocoding. We\n",
    "will also apply a specific algorithmic toolbox, developed for this article, on\n",
    "text extraction of intertitles from silent newsreels—the application is called\n",
    "_stum_—as well as using a tweaked tool, _SweScribe_, based on automatic speech\n",
    "recognition, in order to both transcribe, and timestamp speech from the\n",
    "collection’s sound films."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "copyright"
    ]
   },
   "source": [
    "## Start of placeholders.\n",
    "TODO: Remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "cover"
    ]
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(\"./media/placeholder.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "disclaimer"
    ]
   },
   "source": [
    " (optional) This article was orginally published (...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "keywords"
    ]
   },
   "source": [
    "FirstKeyword, SecondKeyword, AlwaysSeparatedByAComma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hermeneutics"
    ]
   },
   "source": [
    "This is a hermeneutic paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "jdh": {
     "module": "object",
     "object": {
      "source": [
       "table 1: label table 1"
      ]
     }
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "table-1"
    ]
   },
   "source": [
    "Editor|1641|1798|1916\n",
    "---|---|---|---\n",
    "Senan|0.55|0.4|0.3\n",
    "Henry|0.71|0.5|0.63"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hidden"
    ]
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check your Python version\n",
    "from platform import python_version\n",
    "python_version()\n",
    "\n",
    "#!python -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pandas package needs to be added to the requirements.txt 's file\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from itertools import pairwise\n",
    "import cv2\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import time\n",
    "import threading\n",
    "import os\n",
    "from pathlib import Path\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "from IPython.display import Video, display\n",
    "import plotly.express as px\n",
    "from wordcloud import WordCloud\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "from IPython.display import Video, display\n",
    "\n",
    "from plotly import colors\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# TODO: Isort, check versions, add to requirements.txt\n",
    "root = Path('.')\n",
    "data = root / 'script'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/lux-org/lux-datasets/master/data/college.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## End of placeholders.\n",
    "TODO: Remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the bare minimum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hermeneutics header about video quality.\n",
    "... showing resolution and dpi from a frame and the previous image?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "metadata={\n",
    "    \"jdh\": {\n",
    "        \"module\": \"object\",\n",
    "        \"tags\": [\"video-composition-*\",],\n",
    "        \"object\": {\n",
    "            \"type\":\"video\",\n",
    "            \"source\": [\n",
    "                \"It is indeed difficult to visualise a dataset with thousands \"\n",
    "                \"of nonfiction films, still a small _quick and dirty_ \"\n",
    "                \"compilation of 28 silent films (from the 1920s) gives an \"\n",
    "                \"impression of the vivid depiction of Swedish society—and \"\n",
    "                \"modernity—that the SF-archive encompasses.\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "display(Video(\"./media/vid1.mp4\", width=1000), metadata=metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "guc1d": [],
      "l41c8": [
       {
        "id": "22783102/7HB6GAHN",
        "source": "zotero"
       }
      ],
      "v6zpe": [
       {
        "id": "22783102/VL4UHIWF",
        "source": "zotero"
       }
      ]
     }
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Our work is rooted in the research project Modern Times 1936, that explores\n",
    "what software sees, hears and perceives when technologies for pattern\n",
    "recognition are applied to media historical sources. Within this research\n",
    "project we have prior been interested in the the historical gaze of generative\n",
    "AI <cite id=\"l41c8\"><a href=\"#zotero%7C22783102%2F7HB6GAHN\">(Stjernholm et al., 2025)</a></cite>, algorithmic scaling of early cinema on YouTube <cite\n",
    "id=\"guc1d\"><a href=\"#zotero%7C22783102%2FTV37XT5P\">(Stjernholm &#38; Snickars,\n",
    "2024)</a></cite>, and techniques for assessing photorealism in synthetic images\n",
    "<cite id=\"v6zpe\"><a href=\"#zotero%7C22783102%2FVL4UHIWF\">(Eriksson, 2024)</a></cite>. If Charlie Chaplin once in _Modern Times_ (1936) struggled to\n",
    "comprehend an industrialised world with giant machines, a common denominator in\n",
    "our research project has been to explore how computational methods can help us\n",
    "understand modernity in new ways. In this article, the idea is hence to\n",
    "construct a number of mid-sized datasets from the Journal Digital collection in\n",
    "different modalities, and proceed with an examination using various\n",
    "computational approaches. Consequently, our intention is to increase the\n",
    "scholarly capacity of media historical sources, while at the same time\n",
    "critically scrutinizing AI and algorithmic toolboxes for the study of the past."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(Image(\"./media/img3.png\", width=600), metadata={\n",
    "    \"jdh\": {\n",
    "        \"module\": \"object\",\n",
    "        \"object\": {\n",
    "            \"tags\": [\"figure-SF-facilities-*\"],\n",
    "            \"type\":\"image\",\n",
    "            \"source\": [\n",
    "                \"The Swedish SF film company had its production facilities \"\n",
    "                \"with studio and film laboratory located in the so called \"\n",
    "                \"Film-City (Filmstaden) in the suburb of Råsunda \"\n",
    "                \"(north of Stockholm), from 1920 until 1969. As is evident \"\n",
    "                \"from these late 1920s and 1930s photographs, the production \"\n",
    "                \"of newsreels was a practical craft; it involved editing, \"\n",
    "                \"cleaning, and copying film, as well as synchronising added \"\n",
    "                \"sound. The film itself was nitrate-based celluloid, known \"\n",
    "                \"both for its high image quality—and dangerous flammability. \"\n",
    "                \"Illustrations from the Swedish Film Institute and the \"\n",
    "                \"Swedish National Museum of Science and Technology. \"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biography of a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "09k1h": [
       {
        "id": "22783102/BNUCIK7T",
        "source": "zotero"
       }
      ],
      "2paaf": [
       {
        "id": "22783102/BQLCIUEW",
        "source": "zotero"
       }
      ],
      "vw0o9": [
       {
        "id": "22783102/6APYGVMY",
        "source": "zotero"
       }
      ]
     }
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "During the late 1950s, the film manuscript writer Gardar Sahlberg (1908–1983)\n",
    "started to take an increased interest in the film archive at Svensk\n",
    "Filmindustri (SF). At the time, however, the company archive of nitrate films,\n",
    "such as newsreels and short films, was in dire need to be restored (and\n",
    "preserved). Most of the oldest footage was filmed by cinematographers from\n",
    "Swedish Biograph, a company that in 1919 merged into SF <cite id=\"vw0o9\"><a href=\"#zotero%7C22783102%2F6APYGVMY\">(Olsson, 2022)</a></cite>. From the\n",
    "beginning of the 1920s until the 1960s, SF had been the leading producer of\n",
    "newsreels, educational cinema, and other types of useful films, distributed in\n",
    "both theatrical and non-theatrical venues <cite id=\"2paaf\"><a href=\"#zotero%7C22783102%2FBQLCIUEW\">(Stjernholm &#38; Florin Persson, 2019)</a></cite>. As a way to finance a reconstruction and improvement of the\n",
    "SF archive, Sahlberg and SF decided to produce historical compilation films\n",
    "based on the same old film material. In 1961, for example, the documentary _När\n",
    "seklet var ungt_ (_When the century was young_) had its premiere. Critics endorsed\n",
    "the film—but audiences did not. Instead, SF started negotiations with Radio\n",
    "Sweden (SR), which at the time was also responsible for national public service\n",
    "television. During the winter of 1964, it was announced that SR had acquired\n",
    "the whole newsreel archive from SF; one million meters of film dating from 1897\n",
    "to 1960 was purchased <cite id=\"09k1h\"><a href=\"#zotero%7C22783102%2FBNUCIK7T\">(Snickars, 2024)</a></cite>. The deal was\n",
    "explosive, not only because of the number of preserved nitrate prints; the\n",
    "SF-archive would now find a novel audience in another medium: television."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(Image(\"./media/img4.png\", width=500), metadata={\n",
    "    \"jdh\": {\n",
    "        \"module\": \"object\",\n",
    "        \"object\": {\n",
    "            \"tags\": [\"figure-seklet-*\"],\n",
    "            \"type\":\"image\",\n",
    "            \"source\": [\n",
    "                \"The compilation film, När seklet var ungt \"\n",
    "                \"(When the century was young) from 1961, was \"\n",
    "                \"based on the oldest nonfiction films and \"\n",
    "                \"newsreels within the SF-archive. In February \"\n",
    "                \"1962, the director Gardar Sahlberg stated in \"\n",
    "                \"a letter to the national archivist in Sweden, \"\n",
    "                \"that the intention of the film was to \"\n",
    "                \"show “the qualitatively impressive archival film material” \"\n",
    "                \"to a culturally interested audience. \"\n",
    "                \"At SF we had the belief, he admitted, \"\n",
    "                \"that curiosity of this type of older films \"\n",
    "                \"would be so great “that the money received \"\n",
    "                \"from the box office would make it possible \"\n",
    "                \"to go ahead, and finance the renewal of \"\n",
    "                \"the [SF-]archive”. But unfortunately that \"\n",
    "                \"was not the case—despite excellent reviews. \"\n",
    "                \"When the film was shown \"\n",
    "                \"“in a few places in the countryside, there was no audience,” \"\n",
    "               'he sadly concluded '\n",
    "               '<cite id=\"8l0z1\"><a href=\"#zotero%7C22783102%2FPECABWGW\">(Sahlberg, 1962)</a></cite>'\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "nx09j": [
       {
        "id": "22783102/DSU6R48K",
        "source": "zotero"
       }
      ],
      "ziu8k": [
       {
        "id": "22783102/DVKUX2N4",
        "source": "zotero"
       }
      ]
     }
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "As part of the deal, Sahlberg and a few colleagues at SF were hired by SR to\n",
    "work with safeguarding the SF-archive. During the 1960s and 1970s, the archive\n",
    "was preserved, duplicated, catalogued, and subsequently re-used in numerous\n",
    "television programs. In fact, SR made a remarkable cultural-historical effort\n",
    "in saving this film archive. From the old nitrate prints, Sahlberg had three\n",
    "different film materials made: a master copy on 35 mm (for preservation), a\n",
    "duplicate negative on 35 mm to obtain new copies, and a 16 mm display copy for\n",
    "program producers at SR. Sahlberg also took personal responsibility to\n",
    "catalogue the entire SF-archive, manual work he basically did on his own.\n",
    "Notably, all films from SF were catalogued under a specific SF-number; SF2001\n",
    "for example was the oldest film in the archive, dating from 1897. Since SR took\n",
    "excellent care of these films, the company was able to acquire other film\n",
    "collections as well. Some 400 films were purchased from Kinocentralen in the\n",
    "mid 1960s, a company that had produced short and industrial films from the\n",
    "early 1920s. Other similar films were donated to SR from, for example, the\n",
    "Swedish Film Library (Filmhistoriska Samlingarna), the Salvation Army Sweden,\n",
    "the Stockholm City Museum, and the film archive at the Swedish State Railways\n",
    "(SJ)—the latter collection contained almost two hundred nonfiction films\n",
    "produced by SJ between 1920 and 1960. SR also bought the newsreel Nuet (Now)\n",
    "produced by the film company Nordisk Tonefilm during a few years in the mid\n",
    "1950s <cite id=\"nx09j\"><a href=\"#zotero%7C22783102%2FDSU6R48K\">(Asp, 2014)</a></cite>. Finally, in 1969 SR also decided to acquire all short films\n",
    "produced by SF, a deal that made the entire collection at SR amount to more\n",
    "than five thousand films. As a consequence, the initial SF-archive came to\n",
    "contain a range of different types of documentary film, with different\n",
    "provenances. It should be noted, however, that the rationale behind all these\n",
    "film acquisitions was the potential usage of old films in new tv-programs.\n",
    "Still, SR was also interested in developing a sales organisation for\n",
    "tv-programs, with film rights as a prospective revenue stream. An interesting\n",
    "aspect of the initial purchase of the SF-archive hence concerned what type of\n",
    "rights (to old footage) that SF actually sold to SR, since the archive also\n",
    "contained films of foreign origin (such as Pathé and Gaumont). In a memo from\n",
    "1964, the head of the film archive at SR therefore urged a certain degree of\n",
    "caution when it came to the reuse of foreign films <cite id=\"ziu8k\"><a href=\"#zotero%7C22783102%2FDVKUX2N4\">(Norrlander, 1964)</a></cite>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(Video(\"./media/vid2.mp4\", width=500),\n",
    "        metadata={\n",
    "    \"jdh\": {\n",
    "        \"module\": \"object\",\n",
    "        \"tags\": [\"video-composition2-*\",],\n",
    "        \"object\": {\n",
    "            \"type\":\"video\",\n",
    "            \"source\": [\n",
    "                \"The film collection at SR grew steadily during the 1960s—and \"\n",
    "                \"it was indeed heterogenous. The collection included early \"\n",
    "                \"cinema, silent newsreels from Swedish Biograph, Svensk \"\n",
    "                \"Filmindustris Veckorevy, films from the Swedish State \"\n",
    "                \"Railways (SJ) and their film archive, short films from \"\n",
    "                \"Kinocentralen, and the 1950s newsreel Nuet, produced by \"\n",
    "                \"Nordisk Tonefilm.\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "7cqtj": [
       {
        "id": "22783102/2RDITF4M",
        "source": "zotero"
       }
      ],
      "bsd9e": [
       {
        "id": "22783102/DW4S37Y5",
        "source": "zotero"
       }
      ],
      "ujcdl": [
       {
        "id": "22783102/24HEASLG",
        "source": "zotero"
       }
      ]
     }
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "From the mid 1960s, Swedish public service television appropriated the films\n",
    "that Sahlberg and his colleagues had preserved, and catalogued. Footage from\n",
    "the SF-archive and the other film collections at SR was reused in thousands of\n",
    "tv programs <cite id=\"ujcdl\"><a href=\"#zotero%7C22783102%2F24HEASLG\">(Eriksson et al., 2024)</a></cite>. In many ways these moving images shaped the ways that\n",
    "Swedes perceived their past <cite id=\"7cqtj\"><a href=\"#zotero%7C22783102%2F2RDITF4M\">(Eriksson et al., 2022)</a></cite>. In the\n",
    "1980s, the first steps towards digitising the catalogue of films and metadata\n",
    "were taken. Interestingly, the motivation for both microfilming the catalogue,\n",
    "and developing a rudimentary database of information about the content of old\n",
    "newsreels, were financial. When the new head of the TV archive (as it was now\n",
    "called), Birgitta Lagnell, was interviewed in the mid 1980s, her major quest\n",
    "was how to monetise the film archive: ”She will make the gold mine of\n",
    "television profitable”, headlines stated <cite id=\"bsd9e\"><a href=\"#zotero%7C22783102%2FDW4S37Y5\">(Bergman, 1986)</a></cite>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "wfa1f": [
       {
        "id": "22783102/928EZRZ4",
        "source": "zotero"
       }
      ]
     }
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Ten years later, an increased academic interest in the SF-archive resulted in\n",
    "an externally funded research project with the aim to make the old film\n",
    "collections more accessible. As a result, the department of cinema studies at\n",
    "Stockholm University started a collaboration with the TV archive, granting\n",
    "access to scholars and PhD students interested in the SF-archive. One of us\n",
    "(Snickars) started his PhD training in cinema studies in 1995 by examining 16\n",
    "mm prints from the SF-archive at a Steenbeck editing desk located in the vaults\n",
    "of the TV archive. In parallel, and on the agenda at the time, Swedish public\n",
    "service television began developing digital technology for terrestrial\n",
    "television. A governmental report, SOU 1996:25—_From mass media to multi media:\n",
    "how to digitise Swedish television_—laid the groundwork, and described in detail\n",
    "the technical requirements. Since the digitisation of the SF-archive was\n",
    "foremost media archival work, the TV archive contacted the publicly funded\n",
    "Swedish National Archive of Recorded Sound and Moving Images (ALB) with a\n",
    "request to scan the SF-archive to video—with the prospective to later transfer\n",
    "content to digital tape <cite id=\"wfa1f\"><a href=\"#zotero%7C22783102%2F928EZRZ4\">(ALB, 1996)</a></cite>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "ivltg": [
       {
        "id": "22783102/R86YMEXD",
        "source": "zotero"
       }
      ],
      "j8r7k": [
       {
        "id": "22783102/M63TBZ7V",
        "source": "zotero"
       }
      ]
     }
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "During the summer of 1997, ALB started scanning; the deal was to transfer five\n",
    "hundred nonfiction films every year <cite id=\"ivltg\"><a href=\"#zotero%7C22783102%2FR86YMEXD\">(ALB, 1997)</a></cite>. Additional\n",
    "external funding was secured, and ALB also decided to transfer all catalogue\n",
    "information to machine readable formats. ALB had been inaugurated in the late\n",
    "1970s, due to an extension in the Swedish deposit law that came to include\n",
    "audiovisual material as well. ALB was in many ways a video archive in the\n",
    "service of academic research; public service broadcasts were stored on magnetic\n",
    "tape. Yet, in the mid 1990s it became all too apparent that video and tape\n",
    "recordings would not sustain content for longer periods of time. In digital\n",
    "format, however, it was likely that the same content could be preserved—for\n",
    "ALB, the digitisation of the SF-archive hence developed into a case study of\n",
    "how to proceed with such a major technical, and media archival transition. In a\n",
    "description (for an application) from 1998, The digital newsreel archive, it\n",
    "was stated that ALB was now planning “to digitise all scanned video tapes\n",
    "\\[from the SF-archive\\]. The digitised recordings will then be stored \\[on\\]\n",
    "discs in an automated archive”. Converted catalog information would also be\n",
    "linked to each film. “This will allow the user to perform catalog searches, and\n",
    "also view the requested film directly online on a computer, which would\n",
    "effectively streamline research usage” <cite id=\"j8r7k\"><a href=\"#zotero%7C22783102%2FM63TBZ7V\">(ALB, 1998)</a></cite>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(Image(\"./media/img5.png\", width=800), metadata={\n",
    "    \"jdh\": {\n",
    "        \"module\": \"object\",\n",
    "        \"object\": {\n",
    "            \"tags\": [\"figure-ALB-*\"],\n",
    "            \"type\":\"image\",\n",
    "            \"source\": [\n",
    "                \"In 2003, the Swedish National Archive of Recorded Sound and \"\n",
    "                \"Moving Images (formerly ALB) launched Journal Digital—a \"\n",
    "                \"collection with more than 5,000 digitised films and linked \"\n",
    "                \"metadata, accessible through a user-friendly interface. \"\n",
    "                \"At the same time, one percent of the films from Journal \"\n",
    "                \"Digital were also made available online on the web, a site \"\n",
    "                \"that rapidly became popular among (foremost elderly) Swedes. \"\n",
    "                \"Today, twenty years later, at the portal filmarkivet.se—a \"\n",
    "                \"joint venture between the National Library of Sweden and the \"\n",
    "                \"Swedish Film Institute also—some three hundred SF-newsreels \"\n",
    "                \"are openly available for anyone to watch.\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "nnebq": [
       {
        "id": "22783102/BXDUCBBT",
        "source": "zotero"
       }
      ]
     }
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The digitisation work proceeded in the coming years—albeit slowly. At a board\n",
    "meeting in late 1999, the head of ALB, Sven Allerstrand, had to confess that\n",
    "“unfortunately, as resources are lacking for an R&D function within ALB,\n",
    "development work \\[with the SF-archive\\] is progressing very slowly. Resources\n",
    "must be taken from ordinary operations, which has a negative impact on the\n",
    "overall result.” However, Allerstrand stated, at our media archive we are still\n",
    "“convinced that the only possible solution to ensure the long-term preservation\n",
    "of ALB’s material is automated processing in a digital mass storage system”\n",
    "<cite id=\"nnebq\"><a href=\"#zotero%7C22783102%2FBXDUCBBT\">(ALB, 1999)</a></cite>. A year later, ALB finally secured funding from the\n",
    "government, and the transfer of the SF-archive into digital format proceeded\n",
    "with a more rapid pace. In 2002, all newsreels and short films had finally been\n",
    "digitised. However, since the film collection included not only films from\n",
    "SF—but also films from Kinocentralen, the Swedish State Railways, and newsreels\n",
    "from Nordisk Tonefilm—ALB decided to change the name of the digitised\n",
    "collection to Journal Digital. A new search interface for the collection,\n",
    "publicly accessible on computers at ALB, was developed, as well as a web site\n",
    "with one percent of the collection online (through permission from SVT). In\n",
    "all, Journal Digital gave access to 4,348 newsreels and short films from Svensk\n",
    "Filmindustri, 421 nonfiction films from Kinocentralen, 267 Nuet-newsreels from\n",
    "Nordisk Tonefilm, and 170 SJ-documentary films from the Swedish State\n",
    "Railways—in all 5,206 films dating from 1896 to the mid 1960s. Consequently,\n",
    "that is the amount of films that our dataset—the Journal Digital collection—is\n",
    "based upon. It should be stressed, however, that in the following we will often\n",
    "write about the analyses of newsreels, since they make up the major part of our\n",
    "dataset. Yet, we are aware that our examined film material also contains other\n",
    "genres.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hermeneutics"
    ]
   },
   "outputs": [],
   "source": [
    "display(Image(\"./media/img6.png\", width=800),\n",
    "        metadata={\n",
    "    \"jdh\": {\n",
    "        \"module\": \"object\",\n",
    "        \"object\": {\n",
    "            \"tags\": [\"figure-box-*\"],\n",
    "            \"type\":\"image\",\n",
    "            \"source\": [\n",
    "                \"Via the content management system Box, Swedish researchers \"\n",
    "                \"can today remotely access the audiovisual collections at the \"\n",
    "                \"National Library of Sweden. While sound recordings are fine \"\n",
    "                \"and acceptable, this is hardly the case for film or \"\n",
    "                \"television content—which is displayed in low resolution \"\n",
    "                \"formats, 704 x 576 pixels, in a small square on the screen \"\n",
    "                \"measuring nine times seven centimetres. \"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hermeneutics"
    ]
   },
   "source": [
    "Formats tend to stick, and remain the same: when the Swedish National Archive\n",
    "of Recorded Sound and Moving Images (ALB)—in the year 2000 digitised the\n",
    "SF-archive, all films were converted into MPEG-2-format, a digital video and\n",
    "audio compression standard developed during the 1990s by the Moving Picture\n",
    "Experts Group (MPEG). The MPEG-2-versions were preserved on digital tape at\n",
    "ALB. From these preservation files, another video converter compressed films\n",
    "into a browse copy in MPEG-1-format, a lossy compression format with files\n",
    "stored on a hard drive for instant access via an interface. 25 years ago,\n",
    "MPEG-1 was a standard resolution for compressed video online. Yet, this is\n",
    "obviously not the case any more. If film archivist Gardar Sahlberg during the\n",
    "1960s made sure to preserve the SF-archive on both a master copy on 35 mm, a\n",
    "duplicate negative on 35 mm, and a 16 mm display copy, this is far from how\n",
    "researchers today are confronted with the SF-archive in the digital domain.\n",
    "Decisions taken back then still linger; researchers working with audiovisual\n",
    "materials today must still be satisfied with late 1990s low resolution copies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hermeneutics"
    ]
   },
   "outputs": [],
   "source": [
    "display(Video(\"./media/vid2.mp4\", width=500),\n",
    "        metadata={\n",
    "    \"jdh\": {\n",
    "        \"module\": \"object\",\n",
    "        \"tags\": [\"video-filmarkivet-*\",],\n",
    "        \"object\": {\n",
    "            \"type\":\"video\",\n",
    "            \"source\": [\n",
    "                \"Proper filmic heritage is naturally dependent on digital \"\n",
    "                \"quality; both sequences display the Stockholm exhibition in \"\n",
    "                \"1897, shot by a Lumière cinematographer (SF2001). \"\n",
    "                \"The version to the right is low resolution with pixels \"\n",
    "                \"clearly visible if the frame is enlarged—the speed is also \"\n",
    "                \"not adjusted. Sadly, this is the version that academic \"\n",
    "                \"researchers are confronted with in the Box interface at the \"\n",
    "                \"National Library of Sweden. In the version to the \"\n",
    "                \"left—visible in the public interface filmarkivet.se—speed is \"\n",
    "                \"adjusted, and the sequence is displayed in MPEG-2, a still \"\n",
    "                \"acceptable digital resolution.\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "zmeiv": [
       {
        "id": "22783102/583554VC",
        "source": "zotero"
       }
      ]
     }
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hermeneutics"
    ]
   },
   "source": [
    "Decisions made two decades ago still hold sway. Consequently, in this article\n",
    "we have been working with low resolution (MPEG-1) copies from Journal Digital.\n",
    "As stated, the National Library of Sweden and the Swedish Film Institute also\n",
    "gives online access to some three hundred SF-newsreels in high resolution,\n",
    "MPEG-2-versions, at the portal filmarkivet.se—thus, occasionally we will\n",
    "illustrate our arguments with these film versions as well. Then again, there is\n",
    "a sharp contrast between the highly curated selection of restored\n",
    "high-resolution newsreels available through filmarkivet.se, and the low-quality\n",
    "digitisation of a vast majority of the SF newsreel archive. In addition,\n",
    "filmarkivet.se provides limited metadata, little contextualization using film\n",
    "historical sources, and no possibility to analyze the content at hand as data,\n",
    "effectively limiting the scholarly utility of the site <cite id=\"zmeiv\"><a href=\"#zotero%7C22783102%2F583554VC\">(Snickars, 2015)</a></cite>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load audio stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Signal archeology of the audiovisual past"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "u4gve": [
       {
        "id": "22783102/WHSQVQAB",
        "source": "zotero"
       }
      ]
     }
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Annotating content within 5,205 nonfiction films is already a demanding\n",
    "task, but doing so with precision across both the sonic and visual domains\n",
    "requires even greater care and effort. Previous research has emphasized the\n",
    "time-consuming nature of audiovisual annotation <cite id=\"u4gve\"><a href=\"#zotero%7C22783102%2FWHSQVQAB\">(Guyot et al., 2019)</a></cite>.\n",
    "Nevertheless, to fully understand how these films are structured, both audio\n",
    "and visuals must be taken into consideration. Not only do image and sound play\n",
    "crucial roles within newsreels—naturally, after the introduction of sound in\n",
    "the early 1930s—but the genre also provides a valuable window into the\n",
    "historical formation of specific ways of juxtaposing these modalities within\n",
    "the Journal Digital collection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "9ahou": [
       {
        "id": "22783102/K4N8PIGZ",
        "source": "zotero"
       }
      ]
     }
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Importantly, approaching this film collection through an archaeology of the\n",
    "audiovisual signal invites a shift in perspective: from meaning to trace, from\n",
    "representation to inscription <cite id=\"9ahou\"><a href=\"#zotero%7C22783102%2FK4N8PIGZ\">(Malmstedt, 2025)</a></cite>. Each film\n",
    "can be understood as a layered field of signals that bears the marks of its\n",
    "technological circumstances, institutional routines, and cultural habits. Sound\n",
    "and image are, then, less regarded as vehicles of meaning, but rather\n",
    "historical artefacts that carry within their textures and fluctuations the\n",
    "sedimented practices of production and aesthetics. Accordingly, the initial\n",
    "experiments conducted on this dataset account for both sound and image,\n",
    "complementing human interpretation with automated annotations. This approach\n",
    "enables the generation of first-level segmentations for each modality, allowing\n",
    "for both separate and integrated navigation within large, untagged archival\n",
    "collections. It also establishes a foundation for comparative analysis of the\n",
    "relationship between the visual and auditory modalities, to be explored in the\n",
    "following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "\\[Comment: MJ moved this cell **from** _hermeneutics_ to make space for a commentary closer to the code. This is still a very detached high-level explanation.\\]\n",
    "To analyse this aspect of audiovisual communication, we employ computational\n",
    "methods that can register both the visual and sonic dimensions of newsreels.\n",
    "Specifically, we use transformer-based models such as Moondream, a\n",
    "vision-language model for object detection and vision tasks (Korrapati 2025),\n",
    "and an AST (Audio Spectrogram Transformer, Gong et al. 2021). Each model\n",
    "processes its respective sensory channel independently. Yet their embeddings\n",
    "can be aligned to enable cross-modal comparison. To illustrate this workflow,\n",
    "the following film segment has been annotated in both the audio and visual\n",
    "domains. Both models were used in their publicly available, pre-trained forms,\n",
    "which have demonstrated high general accuracy across a wide range of\n",
    "audiovisual tasks. Moondream, a transformer-based vision-language model, is\n",
    "designed for lightweight image understanding and captioning, enabling efficient\n",
    "object detection and scene parsing even on modest computational resources.\n",
    "However, we made several adjustments to tailor their performance to the\n",
    "specific requirements of historical newsreel material. For the visual analysis,\n",
    "we limited the number of detected objects in each frame to emphasize overall\n",
    "compositional and semantic features rather than exhaustive enumeration. Without\n",
    "this constraint, the model tended to generate redundant detections, for\n",
    "instance, identifying every instance of a person separately, which led to\n",
    "overly cluttered outputs. In the audio domain, we refined the results by\n",
    "filtering out misleading detections, such as static noise being misclassified\n",
    "as environmental sounds like rain. These calibrations allowed us to focus on\n",
    "the broader audiovisual texture of the material rather than on granular or\n",
    "noisy details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hermeneutics"
    ]
   },
   "source": [
    "### Filtering for Useful Audio\n",
    "\n",
    "Many of the 5,205 audio files of _journal digital_ do not contain _useful_ audio for audio-processing. That is, many of them contain no audio-track at all, and other have an audio track that that is either completely silent, white noise or loud sounds at seeminlgy random places. To avoid processing the white-noise and having random noises entering our dataset, we have implementedt he following filtering step to weed out the files with little to no chance having any useful audio.\n",
    "\n",
    "In order we check that each video file has:\n",
    "    1. An audio track\n",
    "    2. That the audio track's length, in seconds, is longer than the threshold value of `2s`.\n",
    "    3. We then calculate the ratio of silence to sound is not greater than the default threshold of `0.98`\n",
    "    \n",
    "If any of these three steps fail, the video does not contain useful audio. \n",
    "\n",
    "We then use two different approaches to check the loudness of the present audio track. If either test passes we have loud enough audio to pass on to the\n",
    "audio processing pipeline.\n",
    "\n",
    "Finally, we compare this list of files with useful audio to the SMDB-metadata for each file and create a 'whitelist' of the intersection of both sets. This whitelist consists of 2,319 out of the full 5,205 set of video files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add ref to file\n",
    "def has_meaningful_audio(\n",
    "    video_path: Path,\n",
    "    min_duration_sec: float = 2.0,\n",
    "    min_integrated_lufs: float = -45.0,\n",
    "    min_lra_lu: float = 1.0,\n",
    "    min_mean_dbfs: float = -40.0,\n",
    "    max_peak_dbfs: float = -6.0,\n",
    "    max_silence_ratio: float = 0.98,\n",
    ") -> bool:\n",
    "    \"\"\"Determine if video file has meaningful audio content.\n",
    "\n",
    "    Checks for:\n",
    "    - Presence of audio stream\n",
    "    - Minimum duration\n",
    "    - Silence ratio below threshold\n",
    "    - EBU R128 loudness metrics OR volume detection metrics\n",
    "\n",
    "    Args:\n",
    "        video_path: Path to video file.\n",
    "        min_duration_sec: Minimum audio duration (default 2.0s).\n",
    "        min_integrated_lufs: Minimum integrated loudness (default -45.0 LUFS).\n",
    "        min_lra_lu: Minimum loudness range (default 1.0 LU).\n",
    "        min_mean_dbfs: Minimum mean volume (default -40.0 dB).\n",
    "        max_peak_dbfs: Maximum peak volume (default -6.0 dB).\n",
    "        max_silence_ratio: Maximum silence ratio (default 0.98).\n",
    "\n",
    "    Returns:\n",
    "        True if video has meaningful audio, False otherwise.\n",
    "\n",
    "    Example:\n",
    "        >>> has_meaningful_audio(Path(\"video.mp4\"))\n",
    "        True\n",
    "    \"\"\"\n",
    "    # Call 1\n",
    "    if not _any_audio_stream(video_path):\n",
    "        return False\n",
    "    # Call 2\n",
    "    dur = _probe_duration_seconds(video_path)\n",
    "    if dur is None or dur < min_duration_sec:\n",
    "        return False\n",
    "    # Call 3\n",
    "    sil = _silence_ratio(video_path)\n",
    "    if sil is not None and sil >= max_silence_ratio:\n",
    "        return False\n",
    "    # Call 4\n",
    "    ebu = _ebur128_stats(video_path)\n",
    "    if ebu:\n",
    "        I_LUFS, LRA_LU = ebu\n",
    "        if I_LUFS > min_integrated_lufs and LRA_LU >= min_lra_lu:\n",
    "            return True\n",
    "    # Call 5\n",
    "    vd = _volumedetect_stats(video_path)\n",
    "    if vd:\n",
    "        mean_db, max_db = vd\n",
    "        if mean_db > min_mean_dbfs or max_db > max_peak_dbfs:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Function 1\n",
    "def _any_audio_stream(video_path: Path) -> bool:\n",
    "    \"\"\"Check if video has any audio stream.\"\"\"\n",
    "    return _any_audio_stream_json(\n",
    "        video_path\n",
    "    ) or _any_audio_stream_ffmpeg_fallback(video_path)\n",
    "\n",
    "# Function 2\n",
    "def _probe_duration_seconds(video_path: Path):\n",
    "    \"\"\"Get audio/video duration in seconds.\"\"\"\n",
    "    for s in _ffprobe_streams(video_path):\n",
    "        if s.get(\"codec_type\") == \"audio\" and s.get(\"duration\"):\n",
    "            try:\n",
    "                d = float(s[\"duration\"])\n",
    "                if math.isfinite(d) and d > 0:\n",
    "                    return d\n",
    "            except (ValueError, TypeError):\n",
    "                pass\n",
    "    rc, out, err = _run(\n",
    "        [\n",
    "            \"ffprobe\",\n",
    "            \"-v\",\n",
    "            \"error\",\n",
    "            \"-show_entries\",\n",
    "            \"format=duration\",\n",
    "            \"-of\",\n",
    "            \"default=nw=1:nk=1\",\n",
    "            str(video_path),\n",
    "        ]\n",
    "    )\n",
    "    try:\n",
    "        d = float(out.strip())\n",
    "        return d if math.isfinite(d) and d > 0 else None\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "# Function 3\n",
    "def _silence_ratio(\n",
    "    video_path: Path, noise_floor_db: float = -40.0, min_silence_d: float = 0.3\n",
    "):\n",
    "    \"\"\"Calculate ratio of silence in audio track.\n",
    "\n",
    "    Args:\n",
    "        video_path: Path to video file.\n",
    "        noise_floor_db: Noise floor threshold in dB.\n",
    "        min_silence_d: Minimum silence duration to count (seconds).\n",
    "\n",
    "    Returns:\n",
    "        Ratio of silence (0.0 to 1.0) or None if duration unavailable.\n",
    "    \"\"\"\n",
    "    dur = _probe_duration_seconds(video_path)\n",
    "    if not dur or dur <= 0:\n",
    "        return None\n",
    "    af = f\"silencedetect=noise={noise_floor_db}dB:d={min_silence_d}\"\n",
    "    rc, out, err = _run(\n",
    "        [\n",
    "            \"ffmpeg\",\n",
    "            \"-hide_banner\",\n",
    "            \"-nostats\",\n",
    "            \"-i\",\n",
    "            str(video_path),\n",
    "            \"-af\",\n",
    "            af,\n",
    "            \"-f\",\n",
    "            \"null\",\n",
    "            \"-\",\n",
    "        ]\n",
    "    )\n",
    "    total_sil = 0.0\n",
    "    last_start = None\n",
    "    for line in err.splitlines():\n",
    "        if \"silence_start:\" in line:\n",
    "            m = re.search(r\"silence_start:\\s*([0-9.]+)\", line)\n",
    "            if m:\n",
    "                last_start = float(m.group(1))\n",
    "        elif \"silence_end:\" in line:\n",
    "            md = re.search(r\"silence_duration:\\s*([0-9.]+)\", line)\n",
    "            if md:\n",
    "                total_sil += float(md.group(1))\n",
    "            else:\n",
    "                me = re.search(r\"silence_end:\\s*([0-9.]+)\", line)\n",
    "                if me and last_start is not None:\n",
    "                    total_sil += max(0.0, float(me.group(1)) - last_start)\n",
    "            last_start = None\n",
    "    if last_start is not None:\n",
    "        total_sil += max(0.0, dur - last_start)\n",
    "    return min(1.0, max(0.0, total_sil / dur))\n",
    "\n",
    "# Function 4\n",
    "def _ebur128_stats(video_path: Path):\n",
    "    \"\"\"Get EBU R128 loudness stats (integrated loudness, loudness range).\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (I_LUFS, LRA_LU) or None if stats unavailable.\n",
    "    \"\"\"\n",
    "    rc, out, err = _run(\n",
    "        [\n",
    "            \"ffmpeg\",\n",
    "            \"-hide_banner\",\n",
    "            \"-nostats\",\n",
    "            \"-i\",\n",
    "            str(video_path),\n",
    "            \"-vn\",\n",
    "            \"-sn\",\n",
    "            \"-filter_complex\",\n",
    "            \"ebur128=peak=true\",\n",
    "            \"-f\",\n",
    "            \"null\",\n",
    "            \"-\",\n",
    "        ]\n",
    "    )\n",
    "    text = err\n",
    "    I_LUFS = None\n",
    "    LRA_LU = None\n",
    "    for pat in [\n",
    "        r\"\\bI:\\s*([-+]?\\d+(?:[.,]\\d+)?)\\s*LUFS\",\n",
    "        r\"\\bIntegrated loudness:\\s*([-+]?\\d+(?:[.,]\\d+)?)\\s*LUFS\",\n",
    "    ]:\n",
    "        m = re.search(pat, text, re.I)\n",
    "        if m:\n",
    "            I_LUFS = _parse_float_any(m.group(1))\n",
    "            break\n",
    "    for pat in [\n",
    "        r\"\\bLRA:\\s*([-+]?\\d+(?:[.,]\\d+)?)\\s*LU\\b\",\n",
    "        r\"\\bLoudness range:\\s*([-+]?\\d+(?:[.,]\\d+)?)\\s*LU\\b\",\n",
    "    ]:\n",
    "        m = re.search(pat, text, re.I)\n",
    "        if m:\n",
    "            LRA_LU = _parse_float_any(m.group(1))\n",
    "            break\n",
    "    if I_LUFS is not None and LRA_LU is not None:\n",
    "        return I_LUFS, LRA_LU\n",
    "    return None\n",
    "\n",
    "# Function 5\n",
    "def _volumedetect_stats(video_path: Path):\n",
    "    \"\"\"Get volume detection stats (mean_volume, max_volume) in dB.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (mean_dB, max_dB) or None if stats unavailable.\n",
    "    \"\"\"\n",
    "    rc, out, err = _run(\n",
    "        [\n",
    "            \"ffmpeg\",\n",
    "            \"-hide_banner\",\n",
    "            \"-nostats\",\n",
    "            \"-i\",\n",
    "            str(video_path),\n",
    "            \"-vn\",\n",
    "            \"-sn\",\n",
    "            \"-af\",\n",
    "            \"volumedetect\",\n",
    "            \"-f\",\n",
    "            \"null\",\n",
    "            \"-\",\n",
    "        ]\n",
    "    )\n",
    "    m_mean = re.search(r\"mean_volume:\\s*([-+]?\\d+(?:[.,]\\d+)?)\\s*dB\", err)\n",
    "    m_max = re.search(r\"max_volume:\\s*([-+]?\\d+(?:[.,]\\d+)?)\\s*dB\", err)\n",
    "    if m_mean and m_max:\n",
    "        return _parse_float_any(m_mean.group(1)), _parse_float_any(\n",
    "            m_max.group(1)\n",
    "        )\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hermeneutics"
    ]
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Traces of sonic experiments "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "To understand how content is distributed, the first thing to consider is the\n",
    "nature of the dataset itself. The SF-archive—and the subsequent Journal Digital\n",
    "collection—varies widely in scope and condition; the latter have during decades\n",
    "been conditioned by differences in film production, and later by dissimilar and\n",
    "uneven processes of preservation. Some nitrate prints have hence only survived\n",
    "in partial form, and (after the introduction of sound) many films still lack a\n",
    "complete soundtrack, usually because the audio has deteriorated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Figure\n",
    "audio_files_year = pd.read_csv(data / 'year_audio_files.tsv', sep='\\t').set_index('Year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_audio_bars():\n",
    "    fig =    px.bar(\n",
    "        audio_files_year,\n",
    "        y='audio',\n",
    "        title='Files with Useful Sound per Year (≥1930)')\n",
    "\n",
    "    fig.update_layout(\n",
    "    xaxis_title=\"Year\",\n",
    "    yaxis_title=\"# of files with useful audio\",\n",
    "    xaxis=dict(\n",
    "        tickmode='linear',\n",
    "        tick0=1930,\n",
    "        dtick=10\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        tickmode='linear',\n",
    "        tick0=0,\n",
    "        dtick=10\n",
    "    )\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "display(\n",
    "    get_audio_bars(),\n",
    "metadata={\n",
    "    \"jdh\": {\n",
    "        \"module\": \"object\",\n",
    "        \"object\": {\n",
    "            \"tags\": [\"figure-audio_bars-*\"],\n",
    "            \"type\":\"image\",\n",
    "            \"source\": [\n",
    "                \"The graph shows the overall distribution of audiovisual films \"\n",
    "                \"in the Journal Digital collection—with the digital emphasis \"\n",
    "                \"on files with sound As is evident, the distribution of files \"\n",
    "                \"is uneven, with a gradual increase during the 1930s.\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "kr6es": [
       {
        "id": "22783102/TEPVQNE3",
        "source": "zotero"
       }
      ]
     }
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Tracing and tracking sound, the pattern above is expected: early in the 1930s,\n",
    "sound production of newsreels was limited by technical constraints and\n",
    "substantial costs. However, the data also offers a revealing media-historical\n",
    "insight into the gradual incorporation of sound into the newsreel format.\n",
    "Whereas the arrival of sound in cinema is often portrayed as a sudden and\n",
    "decisive transformation, usually pinpointing the talkie musical drama, _The Jazz\n",
    "Singer_ (1927)—featuring six songs performed by Al Jolson—evidence from the\n",
    "Journal Digital collection indicate a more gradual and slower media migration,\n",
    "that is a slightly uneven process of sound integration <cite id=\"kr6es\"><a href=\"#zotero%7C22783102%2FTEPVQNE3\">(Beck, 2011)</a></cite>. Furthermore, the\n",
    "second half of the graph above shows a gradual decline. At first glance, this\n",
    "might seem surprising, but it is more likely a reflection of the structure of\n",
    "the film collection than of an actual historical trend. If one instead plots\n",
    "the proportion of films containing sound for each year, a more consistent\n",
    "pattern of integration appears, evident in the graph below. It also provides a\n",
    "clearer foundation for our following analysis, where we examine how image and\n",
    "sound interact across different phases of newsreel development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "audio_files_year['share'] = audio_files_year['audio'] / audio_files_year['files']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_audio_line():\n",
    "    fig =    px.line(\n",
    "        audio_files_year,\n",
    "        y='share',\n",
    "        title='Share of Files with Useful Sound per Year (≥1930)')\n",
    "\n",
    "    fig.update_layout(\n",
    "    xaxis_title=\"Year\",\n",
    "    yaxis_title=\"# of files with useful audio\",\n",
    "    xaxis=dict(\n",
    "        tickmode='linear',\n",
    "        tick0=1930,\n",
    "        dtick=10\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        tickmode='linear',\n",
    "        tick0=0,\n",
    "        dtick=10\n",
    "    )\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "display(\n",
    "    get_audio_line(),\n",
    "metadata={\n",
    "    \"jdh\": {\n",
    "        \"module\": \"object\",\n",
    "        \"object\": {\n",
    "            \"tags\": [\"figure-audio_shares-*\"],\n",
    "            \"type\":\"image\",\n",
    "            \"source\": [\n",
    "                \"Departing from newsreels with sound in the Journal Digital \"\n",
    "                \"Corpus, the graph shows the share of digital film files with \"\n",
    "                \"sound from 1930 until 1966. The drop in 1963 is the result of \"\n",
    "                \"SF-newsreel production coming to a complete end.\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "eqhsq": [],
      "gu35x": [],
      "reghd": [
       {
        "id": "22783102/NEKVNJJD",
        "source": "zotero"
       }
      ],
      "u2cy7": [
       {
        "id": "22783102/2IBV4AM6",
        "source": "zotero"
       }
      ]
     }
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "One observation to be made, is that the graph makes the process of audiovisual\n",
    "integration in newsreel production appear almost linear, a steady movement\n",
    "toward the normalization of sound. But it is also striking to observe that\n",
    "silent films continued to appear well into the 1940s. The data therefore resist\n",
    "any neat periodisation. What can be seen instead is a slow and uneven adoption\n",
    "that depends as much on institutional practice and local conditions, as on\n",
    "technological possibilities. It is worth keeping in mind, and a reminiscence\n",
    "that patterns one might observe later are shaped by underlying imbalances in\n",
    "the film archive itself. But the gradual nature of sound transition is\n",
    "interesting for another reason as well. It suggests that the Journal Digital\n",
    "collection does not simply document the use of sound, but also the process\n",
    "through which sound was being tested, adjusted, and creatively incorporated\n",
    "into the newsreel format. In other words, films capture a moment of\n",
    "experimentation, starting in the early 1930s when the vocabulary of sound in\n",
    "nonfiction film production was still being invented, including the documented\n",
    "ambivalence to the invention of audio supplementation within moving images.\n",
    "When sound was finally incorporated, film critics such as Béla Balázs or Rudolf\n",
    "Arnheim, and filmmakers such as Sergei Eisenstein and Vsevolod Pudovin,\n",
    "lamented that it destroyed the purity of cinematic realism (<cite id=\"reghd\"><a href=\"#zotero%7C22783102%2FNEKVNJJD\">(Arnheim, 1957)</a></cite>, <cite\n",
    "id=\"eqhsq\"><a href=\"#zotero%7C22783102%2FPCBNCFMD\">Eisenstein et al.,\n",
    "1994</a></cite>, <cite id=\"u2cy7\"><a href=\"#zotero%7C22783102%2F2IBV4AM6\">(Balázs, 2010)</a></cite>, <cite\n",
    "id=\"gu35x\"><a href=\"#zotero%7C22783102%2FUVAHYVZ2\">Balázs, 2017</a></cite>).\n",
    "While others celebrated the shift as a deepening of the medium’s evidential\n",
    "power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hermeneutics"
    ]
   },
   "source": [
    "For this experiment we pass all 2,319 video files with useful audio\n",
    "through two separate pipelines. The first of which is an Audio Spectrogram Transformer (AST) pipeline, we segment the audio streams and use the AST model to automatically attach multiple labels each of to the different segments. Each suggested label is associated with a probability, a confidence score that tells us how _certain_ the model is about attaching the respective label to the snippet.\n",
    "\n",
    "We save both the raw output (1 below) as an .npz file and a filtered version, keeping only the top five (sorted by probability) labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Add ref to file `audio_extraction.py`\n",
    "# HuggingFace model for audio classification\n",
    "HF_MODEL = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    "\n",
    "# Audio processing parameters\n",
    "SR_TARGET = 16000  # Target sample rate (Hz)\n",
    "WIN_SEC = 2.56  # Window size (seconds)\n",
    "HOP_SEC = 0.64  # Hop size (seconds)\n",
    "TOPK = 5  # Number of top predictions to save\n",
    "BATCH_SZ = 32  # Batch size for model inference\n",
    "\"\"\"\n",
    "def run_ast(video_path: Path, out_dir: Path):\n",
    "    \"\"\"Run AST model on video audio and save results.\n",
    "\n",
    "    Extracts audio in sliding windows, computes probabilities for each window,\n",
    "    and saves:\n",
    "    - ast_probs.npz: Probability matrix, window times, metadata\n",
    "    - ast_topk.jsonl: Top-k predictions per window\n",
    "\n",
    "    Args:\n",
    "        video_path: Path to input video.\n",
    "        out_dir: Directory to save outputs.\n",
    "    \"\"\"\n",
    "    wav, sr = extract_audio_16k(video_path, settings.SR_TARGET)\n",
    "    win = int(round(settings.WIN_SEC * sr))\n",
    "    hop = int(round(settings.HOP_SEC * sr))\n",
    "\n",
    "    starts = list(range(0, max(1, len(wav) - win + 1), hop))\n",
    "    if len(wav) > (starts[-1] + win):\n",
    "        starts.append(len(wav) - win)\n",
    "\n",
    "    segments = []\n",
    "    starts_sec = []\n",
    "    for s0 in starts:\n",
    "        seg = wav[s0 : s0 + win]\n",
    "        if len(seg) < win:\n",
    "            seg = np.pad(seg, (0, win - len(seg)))\n",
    "        segments.append(seg)\n",
    "        starts_sec.append(s0 / sr)\n",
    "\n",
    "    ends_sec = np.array(starts_sec) + settings.WIN_SEC\n",
    "    probs_all = np.zeros((len(segments), num_labels), dtype=np.float32)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for i in tqdm(\n",
    "            range(0, len(segments), settings.BATCH_SZ),\n",
    "            desc=f\"AST {video_path.name}\",\n",
    "            position=1,\n",
    "            leave=False,\n",
    "        ):\n",
    "            batch = segments[i : i + settings.BATCH_SZ]\n",
    "            inputs = processor(\n",
    "                batch,\n",
    "                sampling_rate=settings.SR_TARGET,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            logits = model(**inputs).logits\n",
    "            probs = torch.softmax(logits, dim=-1).cpu().numpy()\n",
    "            probs_all[i : i + len(probs)] = probs\n",
    "\n",
    "    # 1: Save probability matrix\n",
    "    np.savez(\n",
    "        out_dir / \"ast_probs.npz\",\n",
    "        probs_all=probs_all,\n",
    "        win_starts_sec=np.array(starts_sec),\n",
    "        win_ends_sec=ends_sec,\n",
    "        meta={\n",
    "            \"sr\": sr,\n",
    "            \"model_id\": settings.HF_MODEL,\n",
    "            \"win_sec\": settings.WIN_SEC,\n",
    "            \"hop_sec\": settings.HOP_SEC,\n",
    "            \"topk\": settings.TOPK,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # 2: Save top-k predictions\n",
    "    with open(out_dir / \"ast_topk.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, p in enumerate(probs_all):\n",
    "            idx = np.argsort(p)[::-1][: settings.TOPK]\n",
    "            labs = [id2label[int(j)].lower() for j in idx]\n",
    "            f.write(\n",
    "                json.dumps(\n",
    "                    {\n",
    "                        \"start\": float(starts_sec[i]),\n",
    "                        \"end\": float(ends_sec[i]),\n",
    "                        \"labels\": labs,\n",
    "                    }\n",
    "                )\n",
    "                + \"\\n\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO Figure `28` goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "audio_mix_path = data / \"audio_mix_by_year.csv\"\n",
    "audio_mix_labels_path = data / \"audio_mix_top_labels_overall.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_audio_mix(per_year_mix, top_overall_df):\n",
    "    \"\"\"\n",
    "    Create audio mix visualization showing distribution by year and top labels.\n",
    "\n",
    "    Args:\n",
    "        per_year_mix: DataFrame with columns year, speech_pct, music_pct, other_pct\n",
    "        top_overall_df: DataFrame with top labels per category\n",
    "\n",
    "    Returns:\n",
    "        fig: Plotly figure object (for display or ipywidgets)\n",
    "    \"\"\"\n",
    "    if per_year_mix.empty:\n",
    "        raise ValueError(\"Supplied DataFrame is empty.\")\n",
    "\n",
    "    years = per_year_mix[\"year\"].astype(int).tolist()\n",
    "    s = per_year_mix[\"speech_pct\"].to_numpy(dtype=float)\n",
    "    m = per_year_mix[\"music_pct\"].to_numpy(dtype=float)\n",
    "    o = per_year_mix[\"other_pct\"].to_numpy(dtype=float)\n",
    "\n",
    "    colors = {\n",
    "        \"speech\": \"#1f77b4\",  # blue\n",
    "        \"music\": \"#ff7f0e\",  # orange\n",
    "        \"other\": \"#7f7f7f\",  # gray\n",
    "    }\n",
    "\n",
    "    # Create subplot with two columns: chart (left) and annotations (right)\n",
    "    fig = make_subplots(\n",
    "        rows=1,\n",
    "        cols=2,\n",
    "        column_widths=[0.65, 0.35],\n",
    "        subplot_titles=(\"\", \"\"),  # We'll add custom title instead\n",
    "        specs=[[{\"type\": \"scatter\"}, {\"type\": \"scatter\"}]],\n",
    "        horizontal_spacing=0.05,\n",
    "    )\n",
    "\n",
    "    # Add stacked area chart (traces added in reverse order for proper stacking)\n",
    "    # Other (bottom layer)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=years,\n",
    "            y=o,\n",
    "            name=\"Other\",\n",
    "            mode=\"lines\",\n",
    "            line=dict(width=0.5, color=colors[\"other\"]),\n",
    "            fillcolor=colors[\"other\"],\n",
    "            fill=\"tozeroy\",\n",
    "            stackgroup=\"one\",\n",
    "            opacity=0.85,\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    # Music (middle layer)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=years,\n",
    "            y=m,\n",
    "            name=\"Music\",\n",
    "            mode=\"lines\",\n",
    "            line=dict(width=0.5, color=colors[\"music\"]),\n",
    "            fillcolor=colors[\"music\"],\n",
    "            fill=\"tonexty\",\n",
    "            stackgroup=\"one\",\n",
    "            opacity=0.85,\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    # Speech (top layer)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=years,\n",
    "            y=s,\n",
    "            name=\"Speech\",\n",
    "            mode=\"lines\",\n",
    "            line=dict(width=0.5, color=colors[\"speech\"]),\n",
    "            fillcolor=colors[\"speech\"],\n",
    "            fill=\"tonexty\",\n",
    "            stackgroup=\"one\",\n",
    "            opacity=0.85,\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    # Add invisible trace to right subplot to establish coordinate system\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[0, 1],\n",
    "            y=[0, 1],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(size=0, opacity=0),\n",
    "            showlegend=False,\n",
    "            hoverinfo=\"skip\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=2,\n",
    "    )\n",
    "\n",
    "    # Update left subplot (chart) layout\n",
    "    fig.update_xaxes(\n",
    "        title_text=\"Year\",\n",
    "        range=[min(years), max(years)],\n",
    "        showgrid=False,\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "    fig.update_yaxes(\n",
    "        title_text=\"Share of audio-active time (%)\",\n",
    "        range=[0, 100],\n",
    "        dtick=20,\n",
    "        showgrid=True,\n",
    "        gridwidth=0.5,\n",
    "        gridcolor=\"rgba(128, 128, 128, 0.3)\",\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    # Configure right subplot for text annotations (0-1 coordinate system)\n",
    "    fig.update_xaxes(visible=False, range=[0, 1], row=1, col=2)\n",
    "    fig.update_yaxes(visible=False, range=[0, 1], row=1, col=2)\n",
    "\n",
    "    # Add text annotations to right panel\n",
    "    y_pos = 0.98\n",
    "    line_height = 0.055\n",
    "\n",
    "    for cat, title in [\n",
    "        (\"speech\", \"Speech\"),\n",
    "        (\"music\", \"Music\"),\n",
    "        (\"other\", \"Other\"),\n",
    "    ]:\n",
    "        block = top_overall_df[top_overall_df[\"category\"] == cat]\n",
    "        if block.empty:\n",
    "            continue\n",
    "\n",
    "        # Category header\n",
    "        fig.add_annotation(\n",
    "            text=f\"<b>{title}</b>\",\n",
    "            xref=\"x2\",\n",
    "            yref=\"y2\",\n",
    "            x=0.0,\n",
    "            y=y_pos,\n",
    "            showarrow=False,\n",
    "            font=dict(size=11, color=colors.get(cat, \"black\")),\n",
    "            xanchor=\"left\",\n",
    "            yanchor=\"top\",\n",
    "        )\n",
    "        y_pos -= line_height * 0.9\n",
    "\n",
    "        # Individual labels\n",
    "        for _, r in block.iterrows():\n",
    "            label = r[\"label\"]\n",
    "            share = r[\"category_share_pct\"]\n",
    "            secs = r[\"seconds\"]\n",
    "            txt = (\n",
    "                f\"{int(r['rank']):>2d}. {label} ({share:4.1f}%, {secs:,.0f}s)\"\n",
    "            )\n",
    "            fig.add_annotation(\n",
    "                text=txt,\n",
    "                xref=\"x2\",\n",
    "                yref=\"y2\",\n",
    "                x=0.02,\n",
    "                y=y_pos,\n",
    "                showarrow=False,\n",
    "                font=dict(size=9, family=\"monospace\"),\n",
    "                xanchor=\"left\",\n",
    "                yanchor=\"top\",\n",
    "            )\n",
    "            y_pos -= line_height\n",
    "\n",
    "        y_pos -= line_height * 0.6\n",
    "\n",
    "    # Update overall layout\n",
    "    fig.update_layout(\n",
    "        title={\n",
    "            \"text\": \"Audio mix by year and overall top-5 labels per category\",\n",
    "            \"x\": 0.32,\n",
    "            \"xanchor\": \"center\",\n",
    "            \"font\": {\"size\": 14},\n",
    "        },\n",
    "        showlegend=True,\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"top\",\n",
    "            y=0.98,\n",
    "            xanchor=\"left\",\n",
    "            x=0.01,\n",
    "            bgcolor=\"rgba(255, 255, 255, 0.8)\",\n",
    "        ),\n",
    "        height=600,\n",
    "        width=1000,\n",
    "        hovermode=\"x unified\",\n",
    "    )\n",
    "\n",
    "    # Add subplot title for left panel\n",
    "    fig.add_annotation(\n",
    "        text=\"Audio content distribution by year\",\n",
    "        xref=\"x domain\",\n",
    "        yref=\"y domain\",\n",
    "        x=0.5,\n",
    "        y=1.15,\n",
    "        showarrow=False,\n",
    "        font=dict(size=12),\n",
    "        xanchor=\"center\",\n",
    "        yanchor=\"bottom\",\n",
    "    )\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(plot_audio_mix(\n",
    "    pd.read_csv(data / \"audio_mix_by_year.csv\"),\n",
    "    pd.read_csv(data / \"audio_mix_top_labels_overall.csv\")), width=1000,\n",
    "       metadata={\n",
    "        \"jdh\": {\n",
    "        \"module\": \"object\",\n",
    "        \"object\": {\n",
    "            \"tags\": [\"figure-audiomix-*\"],\n",
    "            \"type\":\"image\",\n",
    "            \"source\": [\n",
    "                \"Audio content distribution within the Journal Digital \"\n",
    "                \"collection from 1930–1967. \"\n",
    "                \"The three categories _Speech_, _Music_, and _Other_ \"\n",
    "                \"are also split into a number of subcategories.\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "            }\n",
    "       )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "When newsreels during the 1930s started to include sound—what could be heard in\n",
    "Swedish cinemas? That is, what kind of specific sounds on a general level can\n",
    "be detected in the Journal Digital collection by way of an aural, computational\n",
    "analyses? As is evident from the graph above, the majority of the sound\n",
    "consisted of speech and music. This is perhaps not entirely unexpected, and can\n",
    "be understood as an extension of the role once played by intertitles, which\n",
    "previously carried much of the newsreel’s informational value—an issue we will\n",
    "return to. There also appears to be a slight trend toward increased use of\n",
    "music over time. All of this holds true up until the final years represented in\n",
    "the dataset. However, we should be mindful of the distribution of data in this\n",
    "period, where the smaller number of film files means that percentages may be\n",
    "skewed (by only a few examples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "ga5qi": [
       {
        "id": "22783102/JSI4ETR6",
        "source": "zotero"
       }
      ],
      "mawwp": [
       {
        "id": "22783102/URY9RMYJ",
        "source": "zotero"
       }
      ],
      "rerwk": [
       {
        "id": "22783102/6AUNIX6I",
        "source": "zotero"
       }
      ]
     }
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "If speech and music are obvious categories that a sound analysis of the entire\n",
    "Journal Digital collection would detect, the third category (_Other_) is of more\n",
    "interest. It encompasses both sound effects and diegetic elements, as well as\n",
    "how they relate to actual imagery (Chion 1994). Listing the main types of\n",
    "sounds in this category immediately gives some sense of their function. A large\n",
    "number of detections are, for example, labeled as bursts or explosions. They\n",
    "should not, however, be taken as evidence of a particularly violent film\n",
    "corpus. In many cases, these are false positives: early optical audio\n",
    "recordings simply contain too much noise, and the AST model tends to interpret\n",
    "such random distortions as explosions. In fact, even to the human ear, the\n",
    "newsreel soundtracks are often so rough that it is difficult to tell whether a\n",
    "noise belongs to a recorded scene, or if it is simply an artifact of the medium\n",
    "itself. Beyond these, the data also show frequent detections of animals, cars,\n",
    "and other vehicles. A fascination for hearing modernity, and bringing the\n",
    "sounds of the streets to cinema audiences, was apparent in the early reception\n",
    "of the sound film technology. A Stockholm film critic in 1928, upon reviewing\n",
    "the emergent sound-infused newsreels (from Denmark), exclaimed: “The cars\n",
    "screeched, the horses’ hooves rattled—and far off in the distance the guard\n",
    "parade approached. There came the first real reminder of where sound film has\n",
    "its greatest significance: in the newsreels” \n",
    "<cite id=\"ga5qi\"><a href=\"#zotero%7C22783102%2FJSI4ETR6\">(Dagend Nyheter, 1928)</a></cite>. In fact,\n",
    "vehicle sounds appear especially often in our dataset, accounting for around\n",
    "eleven percent of all entries in the _Other_ category. This prevalence may merit\n",
    "closer attention. The sound of a vehicle in the 1930s would have carried very\n",
    "different connotations than it does today <cite id=\"rerwk\"><a href=\"#zotero%7C22783102%2F6AUNIX6I\">(Brownell, 1972)</a></cite>. It was less\n",
    "an everyday background noise—and more a distinctive signal of modernity. ”What\n",
    "they heard was a new kind of sound that was the product of modern technology”\n",
    "<cite id=\"mawwp\"><a href=\"#zotero%7C22783102%2FURY9RMYJ\">(Thompson, 2004)</a></cite>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hermeneutics"
    ]
   },
   "source": [
    "In the second part of the experiment we pass all 2,319 video files with useful audio through the Visual Language Model (VLM) _Moondream_.\n",
    "\n",
    "Since Moondream is made for image processing, rather than vide processing, we extract a frame every 5th second and hand it over to Moondream with the prompt:\n",
    "\n",
    "\"List up to 6 distinct objects visible in the image as a comma-separated list of short nouns only (singular form, no adjectives, no counts).\"\n",
    "\n",
    "This creates a list of 0-6 objects detected for every 5s segment of all 2,319 videos, which are comparable to the labels from the AST pipeline. By temporally aligning the audio-annotations with the visual-annotations we can compare how often sound-object pairs occur across the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# ref file\n",
    "\n",
    "# Frame extraction rate\n",
    "FRAME_FPS = 0.2  # Frames per second (0.2 = 1 frame every 5 seconds)\n",
    "\n",
    "# Image processing\n",
    "RESIZE_MAX = 256  # Maximum dimension for resizing\n",
    "JPEG_QUALITY = 60  # JPEG compression quality\n",
    "MAX_LABELS_PER_FRAME = 6  # Maximum labels to save per frame\n",
    "SAVE_EVERY = 50  # Save progress every N videos\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def process_video_file(video_path: Path, out_dir: Path) -> None:\n",
    "    \"\"\"Process single video file, extracting and labeling frames.\n",
    "\n",
    "    Args:\n",
    "        video_path: Path to input video.\n",
    "        out_dir: Directory for output files.\n",
    "    \"\"\"\n",
    "    per_frame_csv = out_dir / \"labels_per_frame.csv\"\n",
    "    per_frame_jsonl = out_dir / \"labels_per_frame.jsonl\"\n",
    "    freq_csv = out_dir / \"label_frequency.csv\"\n",
    "    uniq_txt = out_dir / \"unique_labels.txt\"\n",
    "\n",
    "    processed = load_processed_frames(per_frame_csv)\n",
    "    header_written = per_frame_csv.exists()\n",
    "    pending: List[Dict[str, Any]] = []\n",
    "\n",
    "    # 1. Extract one frame every 5 seconds\n",
    "    for fidx, t, frame in tqdm(\n",
    "        frames_from_video(video_path, settings.FRAME_FPS),\n",
    "        position=1,\n",
    "        leave=False,\n",
    "        desc=video_path.name,\n",
    "    ):\n",
    "        if fidx in processed:\n",
    "            continue\n",
    "\n",
    "        # 2. Re size frames, ensuring that they are all the same size\n",
    "        img_small = resize_keep_aspect(\n",
    "            pil_from_bgr(frame), settings.RESIZE_MAX\n",
    "        )\n",
    "\n",
    "        # 3. Prompt Moondream and record the answer\n",
    "        answer = md_query(img_small, (\n",
    "    \"List up to 6 distinct objects visible in the image as a comma-separated list \"\n",
    "    \"of short nouns only (singular form, no adjectives, no counts).\"\n",
    ")\n",
    ")[\"answer\"]\n",
    "\n",
    "        labels = normalize_visual_labels(\n",
    "            answer, cap=settings.MAX_LABELS_PER_FRAME\n",
    "        )\n",
    "        pending.append(\n",
    "            {\"frame\": fidx, \"time_s\": round(t, 3), \"labels\": \"|\".join(labels)}\n",
    "        )\n",
    "        processed.add(fidx)\n",
    "\n",
    "        if len(pending) >= settings.SAVE_EVERY:\n",
    "            header_written = append_rows_csv(\n",
    "                per_frame_csv, pending, header_written\n",
    "            )\n",
    "            append_rows_jsonl(per_frame_jsonl, pending)\n",
    "            pending.clear()\n",
    "\n",
    "    if pending:\n",
    "        header_written = append_rows_csv(\n",
    "            per_frame_csv, pending, header_written\n",
    "        )\n",
    "        append_rows_jsonl(per_frame_jsonl, pending)\n",
    "\n",
    "    compute_and_save_frequency(per_frame_csv, freq_csv, uniq_txt)\n",
    "    write_summary_json(out_dir, per_frame_csv, freq_csv, video_path)\n",
    "    print(f\"[done] {video_path.name} → {out_dir}\")\n",
    "\n",
    "def frames_from_video(\n",
    "    path: Path, fps_target: float = None\n",
    "):\n",
    "    \"\"\"Extract frames from video at specified FPS.\n",
    "\n",
    "    Args:\n",
    "        path: Path to video file.\n",
    "        fps_target: Target frames per second. Defaults to settings.FRAME_FPS.\n",
    "\n",
    "    Yields:\n",
    "        Tuples of (frame_index, time_seconds, frame_bgr).\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError: If video cannot be opened.\n",
    "    \"\"\"\n",
    "\n",
    "    fps_target = fps_target or settings.FRAME_FPS\n",
    "    cap = cv2.VideoCapture(str(path))\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(f\"Cannot open video: {path}\")\n",
    "\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
    "    step = max(1, int(round(fps / max(1e-6, fps_target))))\n",
    "    idx = 0\n",
    "\n",
    "    while True:\n",
    "        ok, frame = cap.read()\n",
    "        if not ok:\n",
    "            break\n",
    "        if idx % step == 0:\n",
    "            yield idx, idx / fps, frame\n",
    "        idx += 1\n",
    "\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO Figure `29` goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_x_pairing(df_plot, top_visuals, chosen_audio, anypair_year_min):\n",
    "    \"\"\"\n",
    "    Create X-pairing visualization showing audio-visual token pairings over years.\n",
    "\n",
    "    Args:\n",
    "        df_plot: DataFrame with visual_token, year, seconds columns\n",
    "        top_visuals: List of top visual tokens to plot\n",
    "        chosen_audio: The selected audio token name\n",
    "        anypair_year_min: Minimum year for the plot subtitle\n",
    "\n",
    "    Returns:\n",
    "        fig: Plotly figure object (for display or ipywidgets)\n",
    "    \"\"\"\n",
    "    years_sorted = sorted(df_plot[\"year\"].unique().tolist())\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for v in top_visuals:\n",
    "        yvals = []\n",
    "        for y in years_sorted:\n",
    "            s = df_plot.loc[\n",
    "                (df_plot[\"visual_token\"] == v) & (df_plot[\"year\"] == y),\n",
    "                \"seconds\",\n",
    "            ]\n",
    "            yvals.append(float(s.iloc[0]) if len(s) else 0.0)\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=years_sorted,\n",
    "                y=yvals,\n",
    "                mode=\"lines+markers\",\n",
    "                name=str(v),\n",
    "                marker=dict(size=6),\n",
    "                line=dict(width=2),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=f\"'{chosen_audio}' pairings — seconds per year (≥ {anypair_year_min})\",\n",
    "        xaxis_title=\"Year\",\n",
    "        yaxis_title=\"Seconds overlapped\",\n",
    "        showlegend=(len(top_visuals) <= 20),\n",
    "        legend=dict(\n",
    "            yanchor=\"top\",\n",
    "            y=0.99,\n",
    "            xanchor=\"left\",\n",
    "            x=1.01,\n",
    "        ),\n",
    "        hovermode=\"x unified\",\n",
    "        height=600,\n",
    "        width=1200,\n",
    "    )\n",
    "\n",
    "    # Update axes\n",
    "    fig.update_xaxes(\n",
    "        showgrid=True, gridwidth=0.5, gridcolor=\"rgba(128, 128, 128, 0.2)\"\n",
    "    )\n",
    "    fig.update_yaxes(\n",
    "        showgrid=True, gridwidth=0.5, gridcolor=\"rgba(128, 128, 128, 0.2)\"\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def csv_path_to_audioVisual_pairing_plot(csv_path, min_year=1930):\n",
    "    df_plot = pd.read_csv(csv_path)\n",
    "\n",
    "    # Extract audio token from filename\n",
    "    chosen_audio = csv_path.stem.split(\"__\")[0].replace(\"audio_\", \"\")\n",
    "    top_visuals = df_plot[\"visual_token\"].unique().tolist()\n",
    "\n",
    "    # Create and display plot\n",
    "    return plot_x_pairing(df_plot, top_visuals, chosen_audio, min_year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(csv_path_to_audioVisual_pairing_plot(data / 'audio_vehicle__visuals_topk_seconds_long.csv' ),\n",
    "        metadata={\n",
    "        \"jdh\": {\n",
    "        \"module\": \"object\",\n",
    "        \"object\": {\n",
    "            \"tags\": [\"figure-vehiclepairs-*\"],\n",
    "            \"type\":\"image\",\n",
    "            \"source\": [\n",
    "                \"Audio content of _vehicles_ from the Journal \"\n",
    "                \"Digital collection—paired with fifteen types of \"\n",
    "                \"specific imagery.\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "            }\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Another way to discover how sounds of modernity are manifested in the Journal\n",
    "Digital collection, is to take a closer look at how vehicle sounds are paired\n",
    "with particular types of images. Interestingly, none of the detected vehicle\n",
    "sounds correspond directly to images of cars. Instead, they tend to appear\n",
    "alongside signs of outdoor or on-location filming: buildings, stretches of sky,\n",
    "grass, and trees all coincide with the presence of vehicle sounds. To some\n",
    "extent, this may be an artifact of the analysis, since the model occasionally\n",
    "identifies vehicle sounds in scenes where no vehicles are visible. Yet, the\n",
    "pattern is revealing, and also demonstrates a surprisingly sophisticated way of\n",
    "representing sound. Rather than merely mirroring what is seen, the soundtrack\n",
    "adds another layer of meaning. Vehicle sounds are often paired with other kinds\n",
    "of visual information, suggesting that sound was used to evoke atmosphere,\n",
    "location, or a sense of movement rather than simply adding an ordinary sound\n",
    "track. This raises the question how sound and image were actually attached,\n",
    "both practically and conceptually, within newsreels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hermeneutics"
    ]
   },
   "source": [
    "To explore the relationship between sound and image in the Journal Digital\n",
    "collection, the following graph presents a method designed to track recurring\n",
    "pairings between audiovisual elements. This method extracts co-occurring audio\n",
    "and visual label pairs from films and summarises their temporal association\n",
    "across years. Visual labels are read at frame times, merged within a short\n",
    "window around each timestamp, and cleaned to remove placeholders, numbers,\n",
    "generic words, and banned categories. Audio categories come either from time\n",
    "aligned probability scores or—when class names are unreliable—from a stored\n",
    "list of top labels; both paths are converted into simple word tokens after the\n",
    "same cleaning. At each timestamp the present audio tokens and visual tokens\n",
    "form pairs that are tracked as segments with start time, end time, and\n",
    "duration; very short segments are dropped, and files dominated by silence are\n",
    "skipped early. Segments are then matched to a year using a name to year table,\n",
    "then aggregated to compute total seconds and the number of distinct videos for\n",
    "each pair, with a minimum total duration filter. Results are written to CSV\n",
    "files including all pairs, per year summaries, and a ranked shortlist that\n",
    "enforces per audio and per visual caps, along with a global limit so no single\n",
    "label dominates. For the shortlisted pairs the code also exports per year\n",
    "totals and percentages of each year’s total duration to support plotting and\n",
    "later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO Figure `30` goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_top_pair_share(\n",
    "    pct_long_path,\n",
    "    year_min_plot=1930,\n",
    "    exclude_audio_tokens=None,\n",
    "    exclude_visual_tokens=None,\n",
    "    exclude_any_tokens=None,\n",
    "    exclude_substrings=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create top pair share visualization showing percent per year with legend panel.\n",
    "\n",
    "    Args:\n",
    "        pct_long_path: Path to top_pairs_percent_long.csv\n",
    "        year_min_plot: Minimum year to include in plot\n",
    "        exclude_audio_tokens: Set of audio tokens to exclude\n",
    "        exclude_visual_tokens: Set of visual tokens to exclude\n",
    "        exclude_any_tokens: Set of tokens to exclude from either audio or visual\n",
    "        exclude_substrings: Set of substrings to exclude\n",
    "        legend_max_rows_per_col: Maximum rows per legend column (unused, kept for compatibility)\n",
    "        legend_col_width: Width of legend columns (unused, kept for compatibility)\n",
    "\n",
    "    Returns:\n",
    "        fig: Plotly figure object (for display or ipywidgets)\n",
    "    \"\"\"\n",
    "    exclude_audio_tokens = exclude_audio_tokens or {\n",
    "        \"speech\",\n",
    "        \"sound\",\n",
    "        \"effect\",\n",
    "    }\n",
    "    exclude_visual_tokens = exclude_visual_tokens or set()\n",
    "    exclude_any_tokens = exclude_any_tokens or set()\n",
    "    exclude_substrings = exclude_substrings or set()\n",
    "\n",
    "    def _require_file(path) -> bool:\n",
    "        if not path.exists():\n",
    "            print(f\"[skip] missing: {path}\")\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def _split_pair_label(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Expect 'pair_label' like 'audio_token × visual_token'.\"\"\"\n",
    "        a_v = df[\"pair_label\"].astype(str).str.split(\" × \", n=1, expand=True)\n",
    "        df[\"audio_token_plot\"] = a_v[0].str.lower()\n",
    "        df[\"visual_token_plot\"] = (\n",
    "            a_v[1].str.lower() if a_v.shape[1] > 1 else \"\"\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    def _apply_excludes(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Apply EXCLUDE_* rules at plotting time only.\"\"\"\n",
    "        if df.empty:\n",
    "            return df\n",
    "\n",
    "        df = _split_pair_label(df)\n",
    "\n",
    "        ex_a = {t.lower() for t in exclude_audio_tokens}\n",
    "        ex_v = {t.lower() for t in exclude_visual_tokens}\n",
    "        ex_any = {t.lower() for t in exclude_any_tokens}\n",
    "\n",
    "        mask_exact = (\n",
    "            df[\"audio_token_plot\"].isin(ex_a)\n",
    "            | df[\"visual_token_plot\"].isin(ex_v)\n",
    "            | df[\"audio_token_plot\"].isin(ex_any)\n",
    "            | df[\"visual_token_plot\"].isin(ex_any)\n",
    "        )\n",
    "\n",
    "        if exclude_substrings:\n",
    "            lowersubs = {s.lower() for s in exclude_substrings}\n",
    "\n",
    "            def _has_sub(s: str) -> bool:\n",
    "                s = s or \"\"\n",
    "                s = s.lower()\n",
    "                return any(sub in s for sub in lowersubs)\n",
    "\n",
    "            mask_sub = df[\"audio_token_plot\"].apply(_has_sub) | df[\n",
    "                \"visual_token_plot\"\n",
    "            ].apply(_has_sub)\n",
    "            mask = mask_exact | mask_sub\n",
    "        else:\n",
    "            mask = mask_exact\n",
    "\n",
    "        before = len(df)\n",
    "        df = df[~mask].copy()\n",
    "        after = len(df)\n",
    "        if before - after:\n",
    "            print(\n",
    "                f\"[filter] excluded {before - after} rows based on token rules\"\n",
    "            )\n",
    "        return df\n",
    "\n",
    "    if not _require_file(pct_long_path):\n",
    "        raise SystemExit(\n",
    "            \"Required CSV not found. Run the anypair export step first.\"\n",
    "        )\n",
    "\n",
    "    pct_long = pd.read_csv(pct_long_path)\n",
    "\n",
    "    missing = {\"pair_label\", \"year\", \"percent\"} - set(pct_long.columns)\n",
    "    if missing:\n",
    "        raise SystemExit(\n",
    "            f\"top_pairs_percent_long.csv missing columns: {sorted(missing)}\"\n",
    "        )\n",
    "\n",
    "    pct_long[\"year\"] = pd.to_numeric(pct_long[\"year\"], errors=\"coerce\")\n",
    "    pct_long = pct_long.dropna(subset=[\"year\"]).copy()\n",
    "    pct_long[\"year\"] = pct_long[\"year\"].astype(int)\n",
    "\n",
    "    if year_min_plot is not None:\n",
    "        pct_long = pct_long[pct_long[\"year\"] >= year_min_plot].copy()\n",
    "\n",
    "    pct_long = _apply_excludes(pct_long)\n",
    "\n",
    "    if pct_long.empty:\n",
    "        print(\"[stop] No percent data after filters; nothing to plot.\")\n",
    "        return None\n",
    "\n",
    "    pairs_in_long = sorted(pct_long[\"pair_label\"].unique().tolist())\n",
    "\n",
    "    years = sorted(pct_long[\"year\"].unique().tolist())\n",
    "    pct_long_f = pct_long[pct_long[\"pair_label\"].isin(pairs_in_long)].copy()\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add line plots for each pair\n",
    "    for pair in pairs_in_long:\n",
    "        yvals = []\n",
    "        for y in years:\n",
    "            v = pct_long_f.loc[\n",
    "                (pct_long_f[\"pair_label\"] == pair) & (pct_long_f[\"year\"] == y),\n",
    "                \"percent\",\n",
    "            ]\n",
    "            yvals.append(float(v.iloc[0]) if len(v) else 0.0)\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=years,\n",
    "                y=yvals,\n",
    "                mode=\"lines+markers\",\n",
    "                name=pair,\n",
    "                marker=dict(size=6),\n",
    "                line=dict(width=2),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Update layout\n",
    "    year_min_str = f\"≥ {year_min_plot}\"\n",
    "    fig.update_layout(\n",
    "        title=f\"Top pairs — share of each year's overlap (%) ({year_min_str}, exclusions applied)\".strip(),\n",
    "        xaxis_title=\"Year\",\n",
    "        yaxis_title=\"% of year's total analysed overlap\",\n",
    "        hovermode=\"x unified\",\n",
    "        showlegend=True,\n",
    "        legend=dict(\n",
    "            yanchor=\"top\",\n",
    "            y=0.99,\n",
    "            xanchor=\"left\",\n",
    "            x=1.01,\n",
    "        ),\n",
    "        height=600,\n",
    "        width=1200,\n",
    "    )\n",
    "\n",
    "    # Update axes\n",
    "    fig.update_xaxes(\n",
    "        showgrid=True, gridwidth=0.5, gridcolor=\"rgba(128, 128, 128, 0.2)\"\n",
    "    )\n",
    "    fig.update_yaxes(\n",
    "        showgrid=True, gridwidth=0.5, gridcolor=\"rgba(128, 128, 128, 0.2)\"\n",
    "    )\n",
    "\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(plot_top_pair_share(data / 'top_pairs_percent_long.csv'),\n",
    "        metadata={\n",
    "        \"jdh\": {\n",
    "        \"module\": \"object\",\n",
    "        \"object\": {\n",
    "            \"tags\": [\"figure-vehiclepairs-*\"],\n",
    "            \"type\":\"image\",\n",
    "            \"source\": [\n",
    "                \"The sound of top-pair-content within the Journal Digital collection—with vehicle x tree as a winner.\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "            }\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Exploring the relationship between sound and image, some displayed correlations\n",
    "point toward the emergence of a few specific patterns. A number of these are\n",
    "straightforward and expected, such as artillery paired with gunfire, or\n",
    "applause with crowds, or fire with scenes of gathering people. Others make\n",
    "sense only partially, like the pairing of speedboats and airplanes, which\n",
    "likely results from a misrecognition of similar sound textures. Then there are\n",
    "combinations that are more puzzling, such as the frequent link between a bell\n",
    "sound and the appearance of a flag. Overall, there seems to be a recurring\n",
    "tendency for certain cross-domain associations to appear at specific moments in\n",
    "time, rather than evenly across the dataset. It is also clear that the highest\n",
    "occurrences are those paired with _vehicle_. Looking at the broader temporal\n",
    "graph, some co-occurrences peak quite distinctly in the early 1940s. This\n",
    "cannot simply be explained by the general volume of material from that period,\n",
    "since the overall number of films was then beginning to decline. Instead, it\n",
    "suggests an evolving approach to diegetic strategy—one that rises and recedes\n",
    "rather than progressing in a straight line. To explore this issue further, we\n",
    "manually examined the most frequent pairings identified across both sound and\n",
    "image domains. From these observations we compiled a list of potential diegetic\n",
    "connections. A rise in the use of sound was then observed, yet it is not clear\n",
    "that a distinctly diegetic style ever became dominant. Sound and image appear\n",
    "instead in the Journal Digital collection to function as largely independent\n",
    "channels of information. This separation was likely shaped by practical\n",
    "limitations, especially the uneven quality of early audio recording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO Figure `31` goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_line_plot_percent(pivot_pct, year_min):\n",
    "    \"\"\"Create line plot showing percentage per year for top pairs.\"\"\"\n",
    "    years_sorted_pct = sorted(pivot_pct.columns.tolist())\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for pair in pivot_pct.index:\n",
    "        yvals = [\n",
    "            pivot_pct.at[pair, y] if y in pivot_pct.columns else 0.0\n",
    "            for y in years_sorted_pct\n",
    "        ]\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=years_sorted_pct,\n",
    "                y=yvals,\n",
    "                mode=\"lines+markers\",\n",
    "                name=f\"{pair[0]} × {pair[1]}\",\n",
    "                marker=dict(size=6),\n",
    "                line=dict(width=2),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"Top audio–vision pairs over years (% of year's total) — year ≥ {year_min}\",\n",
    "        xaxis_title=\"Year\",\n",
    "        yaxis_title=\"Share of year's overlap (%)\",\n",
    "        showlegend=True,\n",
    "        legend=dict(\n",
    "            yanchor=\"top\",\n",
    "            y=0.99,\n",
    "            xanchor=\"left\",\n",
    "            x=1.01,\n",
    "        ),\n",
    "        height=600,\n",
    "        width=1100,\n",
    "    )\n",
    "\n",
    "    fig.update_xaxes(\n",
    "        showgrid=True, gridwidth=0.5, gridcolor=\"rgba(128, 128, 128, 0.2)\"\n",
    "    )\n",
    "    fig.update_yaxes(\n",
    "        showgrid=True, gridwidth=0.5, gridcolor=\"rgba(128, 128, 128, 0.2)\"\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "def pre_pivot_top_pairs(\n",
    "    segments_csv_path,\n",
    "    pairs_overall_csv_path,\n",
    "    year_min=1930,\n",
    "    top_n_pairs=15,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create three visualizations of top audio-visual pairs.\n",
    "\n",
    "    Line plot: percentage per year\n",
    "\n",
    "    Args:\n",
    "        segments_csv_path: Path to segments_filtered CSV\n",
    "        pairs_overall_csv_path: Path to pairs_overall CSV\n",
    "        year_min: Minimum year to include in titles\n",
    "        top_n_pairs: Number of top pairs to plot\n",
    "\n",
    "    Returns:\n",
    "        Plotly figure object\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    segs = pd.read_csv(segments_csv_path)\n",
    "    pairs_overall = pd.read_csv(pairs_overall_csv_path)\n",
    "\n",
    "    # Get top pairs list\n",
    "    top_pairs_list = (\n",
    "        pairs_overall.head(top_n_pairs)[[\"audio_label\", \"vision_label\"]]\n",
    "        .apply(tuple, axis=1)\n",
    "        .tolist()\n",
    "    )\n",
    "\n",
    "    # Filter segments to top pairs\n",
    "    segs[\"pair\"] = segs[[\"audio_label\", \"vision_label\"]].apply(tuple, axis=1)\n",
    "    segs_top = segs[segs[\"pair\"].isin(top_pairs_list)]\n",
    "\n",
    "    if segs_top.empty:\n",
    "        print(\"[warn] No segments found for top pairs\")\n",
    "        return None, None, None\n",
    "\n",
    "    # Create pivot table for seconds\n",
    "    pivot = (\n",
    "        segs_top.groupby([\"pair\", \"year\"])[\"duration_s\"]\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "        .pivot(index=\"pair\", columns=\"year\", values=\"duration_s\")\n",
    "        .fillna(0.0)\n",
    "    )\n",
    "\n",
    "    # Create pivot table for percentages\n",
    "    totals_by_year = (\n",
    "        segs.groupby(\"year\", as_index=True)[\"duration_s\"].sum().astype(float)\n",
    "    )\n",
    "    pivot_pct = pivot.copy()\n",
    "    denoms = totals_by_year.reindex(pivot_pct.columns).replace(0.0, np.nan)\n",
    "    pivot_pct = (\n",
    "        (pivot_pct.divide(denoms, axis=1) * 100.0)\n",
    "        .replace([np.inf, -np.inf], np.nan)\n",
    "        .fillna(0.0)\n",
    "    )\n",
    "\n",
    "\n",
    "    return create_line_plot_percent(pivot, year_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    pre_pivot_top_pairs(\n",
    "    data / 'segments_filtered_year_ge_1930.csv',\n",
    "    data / 'pairs_overall_year_ge_1930.csv',),\n",
    "    metadata={\n",
    "        \"jdh\": {\n",
    "        \"module\": \"object\",\n",
    "        \"object\": {\n",
    "            \"tags\": [\"figure-vehiclepairs-*\"],\n",
    "            \"type\":\"image\",\n",
    "            \"source\": [\n",
    "                \" Top audio-vision pairs within the Journal Digital collection—with crowds and artillery as top scorers.\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "            }\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "cacs6": [
       {
        "id": "22783102/FDPKQ7P8",
        "source": "zotero"
       }
      ]
     }
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "In 1947, Stellan Dahlstedt (1910–1991), published an article about the sound\n",
    "studios at Svensk Filmindustri. Sound recording on film is hardly “the easiest\n",
    "procedure in filmmaking, as more than one director or producer is willing to\n",
    "attest”. During recording, it was often not possible to achieve the sound\n",
    "effects needed, Dahlstedt continued. In particular, it was difficult to “adjust\n",
    "the strength and character of the different sounds during live recording.\n",
    "Therefore, each sound is recorded separately as much as possible and mixed\n",
    "during replay”. It was no secret that such a practice increased the amount of\n",
    "film stock used. Still, it saved time because there was “no need to experiment\n",
    "with different sound effects during filming. It is the studio time that is the\n",
    "most expensive part of a film recording” <cite id=\"cacs6\"><a href=\"#zotero%7C22783102%2FFDPKQ7P8\">(Dahlstedt, 1947)</a></cite>. As director\n",
    "of film technology at SF, Dahlstedt knew what he was talking about—even if he\n",
    "was all likely writing about feature film production rather than newsreels.\n",
    "Then again, his statement to avoid experimenting with sound effects during\n",
    "filming, has relevance for our audio analyses. On-location sound in newsreel\n",
    "production seems to have been both fashionable and valued during the period,\n",
    "but also technically demanding. Most sounds were added during post-production,\n",
    "especially voice-over narration, but our analyses cannot really confirm the\n",
    "relation between diegetic and non-diegetic sound, at least not in a scholarly\n",
    "decent way. What is lacking within our signal archaeology set-up is the ability\n",
    "to align sound features more directly with visual content, as is today done in\n",
    "several other multimodal learning approaches. If newsreels did not consistently\n",
    "rely on synchronized or diegetic sound, it would be misleading for\n",
    "computational models to assume that a visible object should always have an\n",
    "audible counterpart. One way forward would be to integrate an adjacent layer of\n",
    "information—the textual. Intertitles, commentary, and other written elements\n",
    "often mediated the relationship between sound and image, and their gradual\n",
    "incorporation into the audiovisual mix hence deserves closer attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(Image(\"./media/img7.png\", width=800),\n",
    "metadata={\n",
    "    \"jdh\": {\n",
    "        \"module\": \"object\",\n",
    "        \"object\": {\n",
    "            \"tags\": [\"figure-blueprint-*\"],\n",
    "            \"type\":\"image\",\n",
    "            \"source\": [\n",
    "                \" At the Swedish SF company in the Film-City, north of \"\n",
    "                \"Stockholm, a new sound laboratory was inaugurated in the \"\n",
    "                \"late 1940s. It included a sound central (Ljudcentralen), an \"\n",
    "                \"echo-chamber (Ekorum) and a muting room (Dämpat rum). On the \"\n",
    "                \"second floor there was also a mixing room, and a specific \"\n",
    "                \"newsreel facility.\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Exploring newsreel intertitles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "ndzpp": [
       {
        "id": "22783102/8P2FL3R2",
        "source": "zotero"
       }
      ]
     }
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "While signal archaeology renders some insight into the audio dimensions of the\n",
    "Journal Digital collection, such a dataset by definition excludes early\n",
    "(silent) cinema. Yet non-visual data can also be gleaned from the latter by\n",
    "focusing on so-called intertitles. After 1900 they were inserted between scenes\n",
    "to provide important details about location, time or setting after a scene\n",
    "shift. With films growing longer, the use of intertitles also expanded. From\n",
    "1910 until the advent of sound cinema, intertitles became common practice in\n",
    "the film industry with its own conventions and codes. For examples, intertitles\n",
    "came to incorporate extended textual passages that filled the screen,\n",
    "typographic differentiation to mark hierarchies between narrator and dialogue,\n",
    "and ornamental as well as stylistic devices that underscored the intertitles\n",
    "aesthetic function within the cinematic experience <cite id=\"ndzpp\"><a href=\"#zotero%7C22783102%2F8P2FL3R2\">(Dupré La Tour, 2005)</a></cite>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "bzmpk": [
       {
        "id": "22783102/8BSVNXZQ",
        "source": "zotero"
       }
      ]
     }
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "When multi-reel feature films started to emerge, the intertitles provided a\n",
    "crucial function offering information necessary to condense the story or\n",
    "provide dialogue <cite id=\"bzmpk\"><a href=\"#zotero%7C22783102%2F8BSVNXZQ\">(Chaume, 2020)</a></cite>. In\n",
    "newsreels, meanwhile, the intertitles functioned to structure the visual\n",
    "storyline, provide factual information about time, settings and portrayed\n",
    "individuals, and offer commentary guiding audience reactions. With the\n",
    "introduction of sound, the use of intertitles certainly changed, and the\n",
    "introduction of voice-over commentary made the intertitles shorter.\n",
    "Nevertheless, this overlooked and in previous digital historical research\n",
    "neglected audiovisual artefact includes a lot of textual information. In fact,\n",
    "the Journal Digital collection contains almost fifty thousand intertitles; they\n",
    "appear in more than 4,300 nonfiction films. Emphasis in this section will be\n",
    "placed on contextualizing the newsreel _Svensk Filmindustris Veckorevy_ (_Svensk\n",
    "Filmindustri’s Weekly Review_) and the Journal Digital Corpus, describing the\n",
    "transcription pipeline, and exploring the collection using Voyant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(Image(\"./media/img8.png\", width=800), metadata={\n",
    "    \"jdh\": {\n",
    "        \"module\": \"object\",\n",
    "        \"object\": {\n",
    "            \"tags\": [\"figure-military-*\"],\n",
    "            \"type\":\"image\",\n",
    "            \"source\": [\n",
    "                \"Within the Journal Digital collection there are a number of \"\n",
    "                \"foreign nonfiction films, such as this unidentified short \"\n",
    "                \"film fragment from the mid 1910s, produced by Universal \"\n",
    "                \"Screen Magazine, with more than ten intertitles in less than \"\n",
    "                \"two minutes—albeit with short military orders!\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "8lcdf": [
       {
        "id": "22783102/NTI97CTV",
        "source": "zotero"
       }
      ],
      "nm7s1": [
       {
        "id": "22783102/WQEJMM77",
        "source": "zotero"
       }
      ],
      "v3shd": []
     }
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Though previous research on newsreel intertitles is limited, there is a\n",
    "noticeable tension regarding how the editorial comments should be interpreted\n",
    "as a historical record. Some scholars argue that newsreel intertitles were\n",
    "primarily descriptive and that it was only with the invention of sound\n",
    "commentary that newsreels actually became a news medium proper. Nicolas Pronay,\n",
    "for example, argues that the “range of social and political information which\n",
    "could be conveyed by pictures and monosyllabic captions alone, was obviously\n",
    "too restricted. The change-over to sound-film ... enabled them to cover any\n",
    "subject which was news-worthy irrespective of whether it was pictorial in\n",
    "nature” <cite id=\"8lcdf\"><a href=\"#zotero%7C22783102%2FNTI97CTV\">(Pronay, 1971)</a></cite>. Similarly, Nicholas Reeves, writing on the topic of British\n",
    "film propaganda during World War I, contends that the “dominant approach of the\n",
    "newsreels’ editorial comment as carried by the titles was factual and\n",
    "restrained” <cite id=\"nm7s1\"><a href=\"#zotero%7C22783102%2FWQEJMM77\">(Reeves, 1986)</a></cite>. From this perspective, the textual information in\n",
    "early cinema had a limited rhetorical function, due to both media technological\n",
    "conditions and established genre conventions. Recent scholarship, however, have\n",
    "argued that long before sound, newsreels included intertitles that “served to\n",
    "explain and ideologically tint the footage sandwiched between them” <cite\n",
    "id=\"v3shd\"><a href=\"#zotero%7C22783102%2FHDPCSX8E\">(Scott, 2024:\n",
    "34)</a></cite>. Hence, the rhetoric in the newsreel intertitles guided viewers’\n",
    "opinions in advance of what appeared on screen—prior to the introduction of\n",
    "sound.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "9vrve": []
     }
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "If the audio dataset of the Journal Digital collection gives us (at least some)\n",
    "insights about Swedish modernity, we argue that a dataset of some fifty\n",
    "thousand intertitles have a similar potential to provide commentary on Swedish\n",
    "society, touching on a wide variety of cultural, technological, and societal\n",
    "issues. Since intertitles were inserted into newsreeels—one textual prompt at\n",
    "the time—the amount of words is not mind-boggling. Nevertheless, our dataset\n",
    "still contains some 300,000 words. A key challenge when exploring these\n",
    "newsreel intertitles using digital methods is the frequent occurrence of\n",
    "mirrored flash intertitles. In film distribution and archiving, so-called flash\n",
    "intertitles served a distinct purpose. Notably, flash intertitles do not appear\n",
    "long enough for them to be readable. Rather they originally served as position\n",
    "markers on the negative, which in this context were used as a master copy for\n",
    "producing new prints, and hence pointing out exactly where to insert\n",
    "intertitles when producing a positive. Besides pointing out the position of the\n",
    "intertitles, the flash intertitles could also highlight the intended textual\n",
    "and visual design. Further, this practice has been commonplace in film\n",
    "distribution and archiving primarily to “save on expensive film material” <cite\n",
    "id=\"9vrve\"><a href=\"#zotero%7C22783102%2FJQZ27MNN\">(Dobringer et al., 2013:\n",
    "130)</a></cite>. Then again, to many film historians mirrored flash intertitles\n",
    "have always been a nuisance, not the least since some formats for moving images\n",
    "make it totally impossible to read them—on video it is hopeless, yet in an\n",
    "editing table they can be read, and now also in digital format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(Image(\"./media/img9.png\", width=800), metadata={\n",
    "    \"jdh\": {\n",
    "        \"module\": \"object\",\n",
    "        \"object\": {\n",
    "            \"tags\": [\"figure-military-*\"],\n",
    "            \"type\":\"image\",\n",
    "            \"source\": [\n",
    "                \"It is almost impossible to read the low resolution, and \"\n",
    "                \"mirrored flash intertitle, from _SF’s Weekly Review_ \"\n",
    "                \"(15 April 1929)—a day when prince Fredrik from Denmark \"\n",
    "                \"arrived by train. The intertitle for the skiing competition \"\n",
    "                \"in Oslo at Holmenkolmen is easier to understand \"\n",
    "                \"(at least, if you speak Swedish), taken from, \"\n",
    "                \"_SF’s Weekly Review_ \"\n",
    "                \"(26 February 1921). Mirrored flash intertitles are \"\n",
    "                \"today still default for film historians using the Journal \"\n",
    "                \"Digital collection at the National Library of Sweden; at \"\n",
    "                \"filmarkivet.se, however, restored and high resolution \"\n",
    "                \"intertitle meets the viewer.\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "hqu0v": [
       {
        "id": "22783102/PUAS8ZHL",
        "source": "zotero"
       }
      ]
     }
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "There is little previous research on newsreel intertitles using digital\n",
    "methods. A notable exemption is a study on the Jean Desmet collection\n",
    "(1907–16), housed by the EYE Film Museum in the Netherlands. The authors show\n",
    "the usefulness of deep learning methods to detect intertitles in audiovisual\n",
    "corpora as markers of narrative structure <cite id=\"hqu0v\"><a href=\"#zotero%7C22783102%2FPUAS8ZHL\">(Bhargav et al., 2019)</a></cite>. While\n",
    "Bhargav and colleagues demonstrate the technical feasibility of using deep\n",
    "learning to detect and analyse intertitles in early cinema, our study expands\n",
    "the approach by developing a multimodal transcription pipeline for a much\n",
    "larger corpus of Swedish newsreels spanning five decades. As briefly described\n",
    "in our introduction, we developed a custom transcription tool tool, _stum_, \n",
    "and deployed it to \n",
    "create individual .srt files for the 4,333(`TODO: confirm count`) film with intertitles. The exact\n",
    "amount of texts totaled 302,342(`TODO: confirm count`) words. Notably, the intertitles have a lower\n",
    "capacity for encoding text than a voice narrator, but still remains a vital\n",
    "source of information about the content of the films. Moreover, beside metadata\n",
    "about films, it is the only way to cover what the newsreels were actually\n",
    "about. It should be stressed, however, that given the major amount of catalogue\n",
    "work that Gardar Sahlberg (and other staff at SVT/SR) did during the 1960s and\n",
    "1970s, the metadata for most films within the SF-archive is affluent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "ig4sa": [
       {
        "id": "22783102/DBCYMUBB",
        "source": "zotero"
       }
      ],
      "ujid1": [
       {
        "id": "22783102/AZ3BT53M",
        "source": "zotero"
       }
      ],
      "v61pc": [
       {
        "id": "22783102/MFU9DVN9",
        "source": "zotero"
       }
      ]
     }
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hermeneutics"
    ]
   },
   "source": [
    "_stum_ leverages pre-existing algorithms/models to create a pipeline suitable for this video collection: specifically, it uses OpenCV2 <cite id=\"v61pc\"><a href=\"#zotero%7C22783102%2FMFU9DVN9\">(Bradski, 2000)</a></cite> and TesseractOCR <cite id=\"ujid1\"><a href=\"#zotero%7C22783102%2FAZ3BT53M\">(Smith, 2007)</a></cite> to detect intertitles and extract their textual contents. OCR quality of all the tested intertitles is excellent—the highest Character Error Rate we have encountered is below seven percent. The main problem to solve was intertitle detection. Since flash intertitles usually only comprise a single frame we needed to extract every single frame of each film. Afterwards, stum would group these frames on low levels of changes in pixels. Similar frames were treated as a single scene or segment, from which we use the middle frame for intertitle detection. In general, intertitles within the Journal Digital collection are typically white, but there is also a significant portion with a dark background, and a further portion where the intertitle is superimposed on the film. We hence decided to focus on the former, and use a combination of calculating the relative size of the largest contour (black or white) and an EAST model for detecting characters <cite id=\"ig4sa\"><a href=\"#zotero%7C22783102%2FDBCYMUBB\">(Zhou et al., 2017)</a></cite>.  Frames that were deemed relevant by both these approaches were then passed through an OCR engine—stored both as a mirrored and nonmirrored intertitle. The resulting text output was then combined with the timestamp calculated from the intertitle’s sequence’s of frames.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hermeneutics"
    ]
   },
   "source": [
    "// skotks terrier https://modern36.github.io/jdc_reader/#q=skottsk+terrier&fuzzy=1&video=sf%2F1928%2FSF604A.1.mpg\n",
    "// https://github.com/Modern36/journal_digital_corpus/blob/main/corpus/intertitle/sf/1928/SF604A.1.mpg.srt#L25-L28\n",
    "\n",
    "\n",
    "\n",
    "In the following cell we initiate a small video player that loads the sequance\n",
    "of .png files (frames 14540-14564 from 'SF604A.1.mpg').\n",
    "It has controls for playback speed, pausing, looping, as scrubber for selecting\n",
    "a specific frame an option to pause for 2 seconds at a specific frame.\n",
    "It defaults to the original playback speed of the video file (25fps) and pauses\n",
    "at the mirrored flash-intertitle frame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "one_second = Path('.') / 'media' / 'one_second'\n",
    "terrier_dir = Path('.') / 'media' / 'terrier_frames'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PNGSequencePlayer:\n",
    "    def __init__(self, file_paths, fps=24):\n",
    "        \"\"\"\n",
    "        file_paths: A list of string paths to the .png files (sorted).\n",
    "        fps: Default playback speed.\n",
    "        \"\"\"\n",
    "        # 1. Pre-load all PNG bytes into memory for zero-latency playback\n",
    "        self.frames_data = []\n",
    "        try:\n",
    "            for p in file_paths:\n",
    "                with open(p, 'rb') as f:\n",
    "                    self.frames_data.append(f.read())\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading files: {e}\")\n",
    "            return\n",
    "\n",
    "        self.n_frames = len(self.frames_data)\n",
    "        self.is_paused_event = False\n",
    "\n",
    "        # --- UI Components ---\n",
    "\n",
    "        # We tell the widget these are PNGs\n",
    "        self.image_widget = widgets.Image(\n",
    "            value=self.frames_data[0],\n",
    "            format='png',\n",
    "            layout=widgets.Layout(width='auto', max_width='250px')\n",
    "        )\n",
    "\n",
    "        # Animation Controller\n",
    "        self.play_widget = widgets.Play(\n",
    "            value=0, min=0, max=self.n_frames - 1,\n",
    "            step=1, interval=int(1000/fps),\n",
    "            description=\"Press play\", repeat=False\n",
    "        )\n",
    "\n",
    "        # Scrubber\n",
    "        self.slider = widgets.IntSlider(\n",
    "            value=0, min=0, max=self.n_frames - 1, description=\"Frame\"\n",
    "        )\n",
    "\n",
    "        # Controls\n",
    "        self.fps_input = widgets.BoundedIntText(\n",
    "            value=fps, min=1, max=120, step=1, description='FPS:',\n",
    "            layout=widgets.Layout(width='140px')\n",
    "        )\n",
    "\n",
    "        self.loop_box = widgets.Checkbox(value=False, description=\"Loop\")\n",
    "\n",
    "        # Pause Logic UI\n",
    "        self.pause_enable = widgets.Checkbox(value=True, description=\"Wait 2s @ Frame:\")\n",
    "        self.pause_idx = widgets.BoundedIntText(\n",
    "            value=12, min=0, max=self.n_frames-1, description='',\n",
    "            layout=widgets.Layout(width='60px')\n",
    "        )\n",
    "\n",
    "        # --- Logic Wiring ---\n",
    "\n",
    "        widgets.jslink((self.play_widget, 'value'), (self.slider, 'value'))\n",
    "\n",
    "        self.slider.observe(self.on_frame_change, names='value')\n",
    "        self.fps_input.observe(self.update_speed, names='value')\n",
    "        self.loop_box.observe(self.update_loop, names='value')\n",
    "\n",
    "        # --- Layout ---\n",
    "\n",
    "        # Group the specific pause controls\n",
    "        pause_group = widgets.HBox([self.pause_enable, self.pause_idx])\n",
    "\n",
    "        controls = widgets.VBox([\n",
    "            widgets.HBox([self.play_widget, self.slider]),\n",
    "            widgets.HBox([self.fps_input, self.loop_box, pause_group])\n",
    "        ])\n",
    "\n",
    "        self.ui = widgets.VBox([self.image_widget, controls])\n",
    "\n",
    "    def on_frame_change(self, change):\n",
    "        frame_idx = change['new']\n",
    "\n",
    "        # DIRECT BYTE SWAP - Extremely fast\n",
    "        self.image_widget.value = self.frames_data[frame_idx]\n",
    "\n",
    "        # Check for \"Magic Pause\"\n",
    "        if (self.play_widget.playing and\n",
    "            self.pause_enable.value and\n",
    "            frame_idx == self.pause_idx.value and\n",
    "            not self.is_paused_event):\n",
    "\n",
    "            self.trigger_pause()\n",
    "\n",
    "    def trigger_pause(self):\n",
    "        \"\"\"Stops animation, waits 2s in background thread, resumes.\"\"\"\n",
    "        self.is_paused_event = True\n",
    "        self.play_widget.playing = False # Stop\n",
    "\n",
    "        def resume_worker():\n",
    "            time.sleep(2)\n",
    "            self.play_widget.playing = True # Resume\n",
    "            # Tiny buffer to ensure we don't re-trigger on the same millisecond\n",
    "            time.sleep(0.2)\n",
    "            self.is_paused_event = False\n",
    "\n",
    "        threading.Thread(target=resume_worker).start()\n",
    "\n",
    "    def update_speed(self, change):\n",
    "        if change['new'] > 0:\n",
    "            self.play_widget.interval = int(1000 / change['new'])\n",
    "\n",
    "    def update_loop(self, change):\n",
    "        self.play_widget.repeat = change['new']\n",
    "\n",
    "    def show(self):\n",
    "        display(self.ui)\n",
    "terrier = PNGSequencePlayer(sorted(one_second.glob('*.png')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(terrier.show(), metadata={\n",
    "    \"jdh\": {\n",
    "        \"module\": \"object\",\n",
    "        \"tags\": [\"video-filmarkivet-*\",],\n",
    "        \"object\": {\n",
    "            \"type\":\"video\",\n",
    "            \"source\": [\n",
    "                \"Blink and you will miss it. We have cut 25 frames, equivalent to 1 second of playtime, from `SF604A.1.mpg` which showcases several of the technical obstacles of working with this this archival footage: 1) the videocompression is very lossy, leading to many blocky artefacts; 2) the flash intertitle is only fully visible in a single frame (frame 12)—making it not only impossible for a viewer to read it—but also very likely that they will miss it entirely;  3) the intertitle is left-right mirrored, making it more difficult to read—especially for an OCR software.\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hermeneutics"
    ]
   },
   "source": [
    "Due to the prevalence of `flash intertitles` we need to process every single\n",
    "frame. However, running every single frame through an intertitle-filter and\n",
    "subsequent  OCR engine is very inefficient.\n",
    "We therefore start by grouping the frames by sqeuential similarity—that is\n",
    "we group similar images together, and only pass the middle image through\n",
    "the two step intertitle-filter before passing them through the OCR engine.\n",
    "\n",
    "The default threshold of `stum` to declare two intertitles 'different' enough\n",
    "is a Mean Square Error of 10,000. This is a completely arbitrary number\n",
    "that seems to work well for journal digital—but might need to be updated\n",
    "before it can be successfully applied to other collections.\n",
    "\n",
    "TODO: add reference to original file for the MSE stuff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Mean Square Error—TODO\n",
    "\n",
    "MSE_THRESHOLD = 10_000\n",
    "\n",
    "def mse(im1, im2):\n",
    "    err = np.sum((im1.astype(\"float\") - im2.astype(\"float\")) ** 2)\n",
    "    err /= float(im1.shape[0] * im1.shape[1])\n",
    "    return err\n",
    "\n",
    "def detect_scene_change(im1, im2):\n",
    "    score = mse(im1, im2)\n",
    "    return score > MSE_THRESHOLD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Here we load a short selection of prepared .png images and pass them\n",
    "through the MSE threshold test to show how it is able to detect\n",
    "the scene -> intertitle changes.\n",
    "\n",
    "media/terrier_frames/frame_14550.png\n",
    "ends one scene\n",
    "\n",
    "media/terrier_frames/frame_14551.png media/terrier_frames/frame_14552.png False\n",
    "Are a 'scene'\n",
    "\n",
    "media/terrier_frames/frame_14553.png\n",
    "is on its own\n",
    "\n",
    "media/terrier_frames/frame_14554.png\n",
    "starts a new scene\n",
    "\n",
    "\n",
    "`stum` then uses the _middle_ image of each grouping as the example image --\n",
    "avoiding passing every single image throught the filters:\n",
    "\n",
    "1. contours\n",
    "2. EAST Text detection\n",
    "3. Tesseract OCR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "images = sorted(one_second.glob('frame_1455[0-4].png'))\n",
    "for path1, path2 in pairwise(images):\n",
    "    im1 = cv2.imread(str(path1))\n",
    "    im2 = cv2.imread(str(path2))\n",
    "    print(path1, path2, detect_scene_change(im1, im2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hermeneutics"
    ]
   },
   "source": [
    "Below is an excerpt from # Contours `stum/src/stum/contours.py` as of (date TODO)\n",
    "showing `stums`s `contour filter`:\n",
    "It converts the incoming image to grayscale—reducing the number of colour channels from three to a single channel.\n",
    "All our input images are already grayscale, but they are not all a single colour-channel—this step is therefore necessecary to homogenize the images\n",
    "for the upcoming processing.\n",
    "\n",
    "The contours filter step converts the grayscale images to binary --\n",
    "converting the values of teach pixel from the range(0, 255) to being\n",
    "a 0 (black) or 1 (white).\n",
    "\n",
    "It then checks for the largest white contour in the image and calculates its size\n",
    "relative to the entire image. If the size of the contour is larger than the\n",
    "threshold (default set to 0.9) the function flags it as a potential intertitle\n",
    "frame. If the frams was not flagged as an intertitle, there is a chance that\n",
    "the frame is an intertitle with white text and black background, and the image\n",
    "is therefore inverted and checked again.\n",
    "\n",
    "All the heavy lifting in the image-processing is handled by OpenCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Contours `stum/src/stum/contours.py`\n",
    "\n",
    "\n",
    "def largest_contour(binary_image: cv2.typing.MatLike):\n",
    "    \"\"\"Returns the relative area of the largest contour of the image\"\"\"\n",
    "    contours = cv2.findContours(\n",
    "        binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
    "    )[0]\n",
    "    largest = cv2.contourArea(max(contours, key=cv2.contourArea))\n",
    "\n",
    "    width, height = binary_image.shape\n",
    "    total_area = width * height\n",
    "    relative_area = largest / total_area\n",
    "\n",
    "    return relative_area\n",
    "\n",
    "def contour_filter(image: cv2.typing.MatLike, threshold=0.9) -> bool:\n",
    "    \"\"\"Check if image has one large contour\n",
    "\n",
    "    If the largest contour is smaller than the complement to the threshold,\n",
    "    it also calculates the largest contour of the inverted image. This is a\n",
    "    way to check for images with dark backgrounds and white text.\n",
    "\n",
    "    Parameters\n",
    "        image: cv2 image to check\n",
    "        threshold: threshold to check contour area against, default is 90%\n",
    "\n",
    "    Returns\n",
    "        True if image has one contour larger than given threshold\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    binary = cv2.threshold(\n",
    "        gray, 100, 255, cv2.THRESH_OTSU | cv2.THRESH_BINARY_INV\n",
    "    )[1]\n",
    "\n",
    "    relative_area = largest_contour(binary)\n",
    "\n",
    "    if (1 - relative_area) > threshold:\n",
    "        inverted = cv2.bitwise_not(binary)\n",
    "\n",
    "        inverteds_largest_area = largest_contour(inverted)\n",
    "\n",
    "        relative_area = max(relative_area, inverteds_largest_area)\n",
    "\n",
    "    return relative_area > threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The EAST model/algorithm {add ref}\n",
    "is a lightweight technique for detecting letters in images.\n",
    "In our experiments it has shown to be more reliable than tesseract in\n",
    "detecting whether an image has text in it—and we therefore use it as\n",
    "an in-between step to filter out monotone images without text before they\n",
    "are sent to the OCR engine—which is the last and slowest part of the\n",
    "pipeline.\n",
    "\n",
    "And this cropped part if the image is passed through the OCR engine twice:\n",
    "once in its original form and once more in its mirrored form—keeping the\n",
    "text output that contains the fewest special characters.\n",
    "\n",
    "The pipeline then merges adjacent (think frames) groups with text and use the\n",
    "numbering on the frames to calculate the timestamps (with a default 25fps)\n",
    "for the .srt files."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true,
    "raw_mimetype": "",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Do not run\n",
    "# East `stum/src/stum/east.py`\n",
    "# TODO: Load the model\n",
    "\n",
    "\"\"\"\n",
    "model_loc = (\n",
    "    Path('.').parents[2] / \"models\" / \"frozen_east_text_detection.pb\"\n",
    ")\n",
    "\n",
    "east_net = cv2.dnn.readNet(str(model_loc))\n",
    "\n",
    "layerNames = [\"feature_fusion/Conv_7/Sigmoid\", \"feature_fusion/concat_3\"]\n",
    "\"\"\"\n",
    "\n",
    "def east_filter(image: cv2.typing.MatLike) -> bool:\n",
    "    \"\"\"\n",
    "    This function filters an image using the EAST text detection model.\n",
    "\n",
    "    Parameters:\n",
    "    - image (cv2.typing\tMatLike): The input image to be filtered.\n",
    "\n",
    "    Returns:\n",
    "    - A boolean indicating whether the image contains any detected text or not.\n",
    "    \"\"\"\n",
    "\n",
    "    image_height, image_width = image.shape[:2]\n",
    "\n",
    "    blob = cv2.dnn.blobFromImage(\n",
    "        image,\n",
    "        1.0,\n",
    "        (image_width, image_height),\n",
    "        (123.68, 116.78, 103.94),\n",
    "        swapRB=True,\n",
    "        crop=False,\n",
    "    )\n",
    "\n",
    "    east_net.setInput(blob)\n",
    "    scores, _ = east_net.forward(layerNames)\n",
    "\n",
    "    num_scores = scores.shape[2]\n",
    "\n",
    "    score_data = [scores[0, 0, i] for i in range(num_scores)]\n",
    "\n",
    "    max_score = max([max(row) for row in score_data])\n",
    "    return max_score > 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Moving on the the next part of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "8ndtc": [
       {
        "id": "22783102/94IX6F2L",
        "source": "zotero"
       }
      ],
      "gm0zt": [
       {
        "id": "22783102/GVASD2H8",
        "source": "zotero"
       }
      ],
      "zpe4r": [
       {
        "id": "22783102/3TIXWE3B",
        "source": "zotero"
       }
      ]
     }
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Quantitative and computational approaches to film history have a longer history\n",
    "than is often assumed. Indeed, experiments with quantification and data\n",
    "visualization date back to the 1960s and 1970s, when scholars such as Barry\n",
    "Salt began measuring average shot lengths <cite id=\"gm0zt\"><a href=\"#zotero%7C22783102%2FGVASD2H8\">(Gaines, 2024)</a></cite>. Similarly,\n",
    "in the mid-2000s, the large-scale Cinemetrics database for statistical film\n",
    "style analysis was launched <cite id=\"gm0zt\"><a\n",
    "href=\"#zotero%7C22783102%2FGVASD2H8\">(Gaines, 2024: 22)</a></cite>. These early\n",
    "initiatives foreshadowed the current wave of digital film history, which tends\n",
    "to combine distant and close viewing <cite id=\"zpe4r\"><a\n",
    "href=\"#zotero%7C22783102%2F3TIXWE3B\">(Dang et al., 2024)</a></cite>. Our\n",
    "emphasis in the following lies on using the application Voyant to explore what\n",
    "role intertitles had in newsreels over time, as well as exploring (the now\n",
    "searchable) newsreel intertitles focusing on the theme of modernity. With\n",
    "regard to Swedish film history, scholars have noted that the introduction of\n",
    "sound soon had a significant impact on the newsreel genre. In December 1929,\n",
    "the film _Say it with music_ (directed by Edvin Adolphson and Julius Jaenzon) was\n",
    "advertised as the first Swedish sound film <cite id=\"8ndtc\"><a\n",
    "href=\"#zotero%7C22783102%2F94IX6F2L\">(Furhammar, 2003: 129)</a></cite>. Shortly\n",
    "thereafter, in April 1930, the first newsreel with authentic sound was released\n",
    "featuring the Guard Parade in Stockholm <cite id=\"8ndtc\"><a\n",
    "href=\"#zotero%7C22783102%2F94IX6F2L\">(Furhammar, 2003: 149)</a></cite>.\n",
    "However, due to the fact that the sound equipment was still difficult to\n",
    "handle, newsreels initially featured voice-over commentary with “brisk and\n",
    "witty texts” <cite id=\"8ndtc\"><a\n",
    "href=\"#zotero%7C22783102%2F94IX6F2L\">(Furhammar, 2003: 149)</a></cite>. Having\n",
    "extracted all intertitles from the Journal Digital collection it is possible to\n",
    "detect how the narrative usage of intertitles changed over time. While\n",
    "newsreels quickly became a sound medium—a “medium of the voice” <cite\n",
    "id=\"8ndtc\"><a href=\"#zotero%7C22783102%2F94IX6F2L\">(Furhammar, 2003:\n",
    "149)</a></cite>—a quantitative analysis reveals that the reliance on\n",
    "intertitles persisted longer than one might have imagined. The period from the\n",
    "mid 1910s to the early 1930s stands out as the period when most words per\n",
    "intertitle were used, which is hardly surprising given that films were then\n",
    "silent. More surprising is the fact that it was not until the late 1930s that\n",
    "the number of words per intertitle was reduced to the same levels as prior to\n",
    "the introduction of sound. Similarly, the total number of words in the\n",
    "intertitles grew in the 1920s, and then subsided toward the end of the 1930s.\n",
    "In addition—and worth stressing—is that intertitles remained in use throughout\n",
    "the whole newsreel era, up until the advent of television around 1960. In this\n",
    "sense, intertitles continued to play a narrative role also after newsreels\n",
    "became a “medium of the voice”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\[hermeneutic\\]\n",
    "\n",
    "The corpus of our transcriptions is published on GitHub, Zenodo and PyPI, which\n",
    "is the easiest way to get access to it within a notebook. All we have to do is\n",
    "install the library and load the `Corpus` class -- and we can use its \n",
    "iterator to access all the transcriptions in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from journal_digital import Corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO: Refactor to use journal_digita.Corpus\n",
    "\"\"\"\n",
    "def load_intertitles():\n",
    "    corpus = Corpus('txt', texts_to_include='intertitles')\n",
    "\n",
    "    for srt in corpus:\n",
    "        year = srt.year\n",
    "        if not year.isdigit():\n",
    "            continue\n",
    "        year = int(year)\n",
    "        if year < 1900:\n",
    "            continue\n",
    "\n",
    "        intertitles = srt.content.split('\\n')\n",
    "        tokens = srt.content.lower().split()\n",
    "        word_count = len(tokens)\n",
    "        words_per_intertitle = word_count / len(intertitles)\n",
    "\n",
    "        # Calculating for later\n",
    "        count_by =   tokens.count('by')\n",
    "        count_staden =   tokens.count('staden')\n",
    "        rel_by = count_by / word_count if word_count > 0 else 0.0\n",
    "        rel_staden = count_staden / word_count if word_count > 0 else 0.0\n",
    "\n",
    "\n",
    "        yield {\n",
    "            'file' : srt.filename,\n",
    "            'year': year,\n",
    "            'num_intertitles' : len(intertitles),\n",
    "            'num_words' : word_count,\n",
    "            'words_per_intertitle': words_per_intertitle,\n",
    "            # Calculating for later\n",
    "            'count_by': count_by,\n",
    "            'rel_by': rel_by,\n",
    "            'count_staden': count_staden,\n",
    "            'rel_staden': rel_staden\n",
    "        }\n",
    "\n",
    "corpus_df = pd.DataFrame(load_intertitles())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Bootstrapping confidence interval for visualization.\n",
    "\"\"\"\n",
    "def bootstrap_confidence_interval(data, n_bootstrap=1000, ci=95):\n",
    "    \"\"\"\n",
    "    Calculates the mean and bootstrapped confidence interval for a dataset.\n",
    "    \"\"\"\n",
    "    bootstrap_means = np.zeros(n_bootstrap)\n",
    "\n",
    "    for i in range(n_bootstrap):\n",
    "        # Create a resample of the data (sampling with replacement)\n",
    "        bootstrap_sample = np.random.choice(data, size=len(data), replace=True)\n",
    "        bootstrap_means[i] = np.mean(bootstrap_sample)\n",
    "\n",
    "    # Calculate the lower and upper bounds of the confidence interval\n",
    "    lower_bound = np.percentile(bootstrap_means, (100 - ci) / 2)\n",
    "    upper_bound = np.percentile(bootstrap_means, 100 - (100 - ci) / 2)\n",
    "\n",
    "    # Return the statistics in a pandas Series for easy aggregation\n",
    "    return pd.Series({\n",
    "        'mean': np.mean(data),\n",
    "        'ci_lower': lower_bound,\n",
    "        'ci_upper': upper_bound\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Fix X label, Y label and remove Legend\n",
    "def plot_with_ci(col, title):\n",
    "    summary_df = corpus_df.groupby('year')[col].apply(bootstrap_confidence_interval).unstack().reset_index()\n",
    "\n",
    "\n",
    "    # Create the figure with continuous error bands\n",
    "    fig = go.Figure([\n",
    "        # The mean line\n",
    "        go.Scatter(\n",
    "            x=summary_df['year'],\n",
    "            y=summary_df['mean'],\n",
    "            line=dict(color='rgb(0,100,80)'),\n",
    "            mode='lines',\n",
    "            name='Mean'\n",
    "        ),\n",
    "        # The confidence interval error band\n",
    "        go.Scatter(\n",
    "            x=np.concatenate([summary_df['year'], summary_df['year'][::-1]]), # x, then x reversed\n",
    "            y=np.concatenate([summary_df['ci_upper'], summary_df['ci_lower'][::-1]]), # upper, then lower reversed\n",
    "            fill='toself',\n",
    "            fillcolor='rgba(0,100,80,0.2)',\n",
    "            line=dict(color='rgba(255,255,255,0)'),\n",
    "            hoverinfo=\"skip\",\n",
    "            showlegend=True,\n",
    "            name='95% Confidence Interval'\n",
    "        )\n",
    "    ])\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis_title='X Value',\n",
    "        yaxis_title='Mean of Y Value',\n",
    "        showlegend=False\n",
    "    )\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "    plot_with_ci('words_per_intertitle', title='Average number of words per intertitle').show()\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "    plot_with_ci('num_words', title='Average number of words per video').show()\n",
    "\n",
    "display(widgets.VBox([out1, out2])    ,\n",
    "        metadata={\n",
    "        \"jdh\": {\n",
    "        \"module\": \"object\",\n",
    "        \"object\": {\n",
    "            \"tags\": [\"figure-military-*\"],\n",
    "            \"type\":\"image\",\n",
    "            \"source\": [\n",
    "                \"The chart above plots the number of words that were used in \"\n",
    "                \"each intertitle covering the entire Journal Digital \"\n",
    "                \"collection. The chart below charts the average number of \"\n",
    "                \"words in each film with intertitles, also in the entire \"\n",
    "                \"Journal Digital collection. \"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "            }\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "yon5m": [
       {
        "id": "22783102/685XZA23",
        "source": "zotero"
       }
      ]
     }
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "As noted, automatic speech recognition and other machine listening techniques\n",
    "have contributed toward making audiovisual archives searchable and computable.\n",
    "In this case, the transcription of the intertitles allowed us to explore the\n",
    "textual descriptions using a well-known tool such as Voyant. Some simple\n",
    "observations can be made with regard to topics covered. Not surprisingly,\n",
    "“Sweden”, “Swedish” and the capital of “Stockholm” are among the most\n",
    "frequently occurring words in the intertitle dataset. In terms of geography,\n",
    "other frequently mentioned cities are “Gothenburg”, “Paris” and “London”. This\n",
    "highlights the fact that the newsreel as a medium was highly transnational:\n",
    "reels depicting current events were bought, exchanged and distributed across\n",
    "Europe and the world <cite id=\"yon5m\"><a\n",
    "href=\"#zotero%7C22783102%2F685XZA23\">(Chambers et al., 2018)</a></cite> . In\n",
    "terms of thematic coverage, it is also noteworthy that the word “staden”, a\n",
    "city (117 counts), and the word “by”, village (118 counts)(`#TODO: verify\n",
    "count`), occur almost just as frequently in the intertitle corpus. Whereas the\n",
    "word city shows a concentration around the 1920s and 1930s, the word by is\n",
    "distributed more evenly across the corpus. This is thought-provoking given that\n",
    "the 1920s and 1930s have usually been described as a period of dramatic\n",
    "transformation, and a time when modernity arrived in Sweden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_frequency_scatter(df, word, color=\"salmon\"):\n",
    "    \"\"\"\n",
    "    Create a Plotly scatter plot showing relative frequencies (excluding zeros).\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with columns [filename, rel_frequency]\n",
    "        word: The word being plotted (for title)\n",
    "        color: Color for scatter points\n",
    "\n",
    "    Returns:\n",
    "        plotly.graph_objects.Figure\n",
    "    \"\"\"\n",
    "    # Filter out zero frequencies to focus on files that contain the word\n",
    "    df_nonzero = df[df[\"rel_frequency\"] > 0].copy()\n",
    "    df_nonzero = df_nonzero.sort_values(\"rel_frequency\").reset_index(drop=True)\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Create line plot\n",
    "    fig.add_trace(\n",
    "        go.Scattergl(\n",
    "            x=list(range(len(df_nonzero))),\n",
    "            y=df_nonzero[\"rel_frequency\"],\n",
    "            mode=\"lines\",\n",
    "            line=dict(color=color, width=2),\n",
    "            text=df_nonzero[\"filename\"],\n",
    "            hovertemplate=\"<b>%{text}</b><br>Relative Frequency: %{y:.5f}<extra></extra>\",\n",
    "            showlegend=False,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Configure layout\n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=f'Relative Frequencies for \"{word}\"<br><sub>{len(df_nonzero)} files (excluding {len(df) - len(df_nonzero)} files with zero frequency)</sub>',\n",
    "            x=0.5,\n",
    "            xanchor=\"center\",\n",
    "            font=dict(size=14),\n",
    "        ),\n",
    "        xaxis=dict(\n",
    "            title=\"Files (sorted by frequency)\",\n",
    "            rangemode=\"tozero\",\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=\"Relative Frequency\",\n",
    "            rangemode=\"tozero\",\n",
    "        ),\n",
    "        plot_bgcolor=\"white\",\n",
    "        height=600,\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def create_dual_axis_plot(color_by=\"#E57373\", color_staden=\"#7D8B7D\"):\n",
    "    \"\"\"\n",
    "    Create a Plotly dual y-axis plot showing relative frequencies for two words.\n",
    "\n",
    "    Returns:\n",
    "        plotly.graph_objects.Figure with dual y-axes\n",
    "    \"\"\"\n",
    "\n",
    "    corpus_df.sort_values(['year', 'file'], inplace=True)\n",
    "\n",
    "    # Calculate statistics for y-axis labels\n",
    "    total_by = int(corpus_df[f\"count_by\"].sum())\n",
    "    files_by = int((corpus_df[f\"count_by\"] > 0).sum())\n",
    "    total_staden = int(corpus_df[f\"count_staden\"].sum())\n",
    "    files_staden = int((corpus_df[f\"count_staden\"] > 0).sum())\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Trace 1: by on left y-axis with circle markers\n",
    "    fig.add_trace(\n",
    "        go.Scattergl(\n",
    "            x=corpus_df[\"file\"],\n",
    "            y=corpus_df['rel_by'],\n",
    "            mode=\"lines+markers\",\n",
    "            name='by',\n",
    "            line=dict(color=color_by, width=2),\n",
    "            marker=dict(symbol=\"circle\", size=6, color=color_by),\n",
    "            customdata=corpus_df[[\n",
    "                \"count_by\"\n",
    "            ]].values,\n",
    "            hovertemplate=(\n",
    "                f\"by: %{{y:.5f}} (%{{customdata[0]:.0f}} occurrences)\"\n",
    "                \"<extra></extra>\"\n",
    "            ),\n",
    "            yaxis=\"y1\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Trace 2: staden on right y-axis with triangle markers\n",
    "    fig.add_trace(\n",
    "        go.Scattergl(\n",
    "            x=corpus_df[\"file\"],\n",
    "            y=corpus_df[\"rel_staden\"],\n",
    "            mode=\"lines+markers\",\n",
    "            name='staden',\n",
    "            line=dict(color=color_staden, width=2),\n",
    "            marker=dict(symbol=\"triangle-up\", size=8, color=color_staden),\n",
    "            customdata=corpus_df[[\n",
    "                \"count_staden\"\n",
    "            ]].values,\n",
    "            hovertemplate=(\n",
    "                f\"staden: %{{y:.5f}} (%{{customdata[0]:.0f}} occurrences)\"\n",
    "                \"<extra></extra>\"\n",
    "            ),\n",
    "            yaxis=\"y2\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Configure layout with dual y-axes\n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=f'Relative Frequencies: \"by\" and \"staden\"<br>'\n",
    "                 f'<sub>{len(corpus_df)} files (where at least one word appears)</sub>',\n",
    "            x=0.5,\n",
    "            xanchor=\"center\",\n",
    "            font=dict(size=14),\n",
    "        ),\n",
    "        xaxis=dict(\n",
    "            title=\"Files (sorted by year → filename)\",\n",
    "            showticklabels=False,\n",
    "            type=\"category\",\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=dict(\n",
    "                text=f'<b style=\"color:{color_by}\">by</b> Relative Frequency<br><sub>{total_by} occurrences in {files_by} files</sub>',\n",
    "                font=dict(color=color_by)\n",
    "            ),\n",
    "            rangemode=\"tozero\",\n",
    "            side=\"left\",\n",
    "        ),\n",
    "        yaxis2=dict(\n",
    "            title=dict(\n",
    "                text=f'<b style=\"color:{color_staden}\">by</b> Relative Frequency<br><sub>{total_staden} occurrences in {files_staden} files</sub>',\n",
    "                font=dict(color=color_staden)\n",
    "            ),\n",
    "            rangemode=\"tozero\",\n",
    "            overlaying=\"y\",\n",
    "            side=\"right\",\n",
    "        ),\n",
    "        plot_bgcolor=\"white\",\n",
    "        height=400,\n",
    "        hovermode=\"x unified\",\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.02,\n",
    "            xanchor=\"center\",\n",
    "            x=0.5,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def make_by_stad_plot():\n",
    "    return create_dual_axis_plot(color_by=\"#E57373\", color_staden=\"#7D8B7D\")\n",
    "\n",
    "\n",
    "display(make_by_stad_plot(),\n",
    "       metadata={\n",
    "        \"jdh\": {\n",
    "        \"module\": \"object\",\n",
    "        \"object\": {\n",
    "            \"tags\": [\"figure-vehiclepairs-*\"],\n",
    "            \"type\":\"image\",\n",
    "            \"source\": [\n",
    "                \"Two charts display the occurrences of _staden_ (city) \"\n",
    "                \"and _by_ (village) in the intertitle dataset from the \"\n",
    "                \"Journal Digital collection—where they appear almost the \"\n",
    "                \"same amount of times. The films have archival names \"\n",
    "                \"(such as SF200) with an added year.\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "            }\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "bk7y7": [
       {
        "id": "22783102/6VFT9YB8",
        "source": "zotero"
       }
      ],
      "pw91f": [],
      "uoqlf": []
     }
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Intertitle transcriptions cannot only be used to mine the text material at\n",
    "scale, it is also makes it possible to navigate the material from a qualitative\n",
    "point-of-view. Zooming in on the issue of modernity, for example, it is\n",
    "feasible to use keyword search to find newsreels on distinct topics <cite\n",
    "id=\"pw91f\"><a href=\"#zotero%7C22783102%2FKADI4AFW\">(Armaselu &#38; Fickers,\n",
    "2024)</a></cite>. For instance, searching for the word “moderna” results in 36\n",
    "hits and “modern” in 29 hits. This allows one to detect specific newsreels with\n",
    "modern aspects of Swedish society, prevalent mostly during the 1920s and 1930s,\n",
    "with urban renewal projects, progressive social and employment reforms, and\n",
    "women gaining the right to vote (<cite id=\"bk7y7\"><a\n",
    "href=\"#zotero%7C22783102%2F6VFT9YB8\">Widenheim, 2002</a></cite>, <cite\n",
    "id=\"uoqlf\"><a href=\"#zotero%7C22783102%2F2X3XHIU7\">Habel, 2002</a></cite> ).\n",
    "Some parts of the intertitle dataset naturally mirror prior metadata. Yet, the\n",
    "dataset of intertitles (with its .srt-files) also allows you to find the exact\n",
    "filmic moment (via time stamps) when an intertitle appears—which, furthermore,\n",
    "holds true for all transcribed speech—making it possible to navigate the\n",
    "audiovisual material in new ways. Given our interest in issues around Swedish\n",
    "modernity, a search for _modern_ in the intertitle dataset, gives a vivid result\n",
    "of films—from a 1920s newsreel on defence exercise including “modern artillery\n",
    "weapons” (SF488B), and another newsreel item about a novel (and modern) fish\n",
    "market in the Stockholm Old Town (SF460.1), to a newsreel insert about\n",
    "state-of-art telephones in 1926 (SF519.1), the same year that the Swedish\n",
    "inventor Lars Magnusson Ericsson celebrated his 80th birthday (sadly he passed\n",
    "away later the same year). In this way, the intertitle dataset—gleaned from the\n",
    "Journal Digital corpus—indeed makes it possible to easily shift from\n",
    "large-scale text mining, to close qualitative analysis of particular newsreels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(Image(\"./media/img10.png\", width=800), metadata={\n",
    "    \"jdh\": {\n",
    "        \"module\": \"object\",\n",
    "        \"object\": {\n",
    "            \"tags\": [\"figure-telephone-*\"],\n",
    "            \"type\":\"image\",\n",
    "            \"source\": [\n",
    "                \"The difference between the first telephone ... and the \"\n",
    "                \"modern table telephone is of significance”, the \"\n",
    "                \"_SF’s Weekly Review_ \"\n",
    "                \"(10 May 1926) stated. Via search through the \"\n",
    "                \"intertitle dataset of the Journal Digital collection, it is \"\n",
    "                \"possible to detect films, or sections of newsreels, \"\n",
    "                \"otherwise hard to find—in this case using the search \"\n",
    "                \"phrase moderna.\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Staden Vs by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load NER and GEO stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Scalable Geographic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "37mld": [
       {
        "id": "22783102/AGCZGSA8",
        "source": "zotero"
       }
      ],
      "495hw": [
       {
        "id": "22783102/89SWX3HE",
        "source": "zotero"
       }
      ]
     }
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Since all newsreel production at Swedish Film Industry was headquartered in\n",
    "Stockholm—did this urban location dictate which parts of Sweden that appeared\n",
    "on cinema screens across the country? Which places were depicted, and how were\n",
    "they represented? Did the _SF Weekly Review_ favour certain locations? Was there\n",
    "an urban gaze framing modernity via the provincial, the rural, or the remote?\n",
    "Such geographic questions of representation have prior been difficult to answer\n",
    "in a systematic way. While individual newsreels can be analysed, the sheer\n",
    "volume of filmic material within the SF-archive—decades of weekly or bi-weekly\n",
    "releases —- has made corpus-level inquiry impractical. However, with the\n",
    "introduction of the Journal Digital corpus, it is possible to analyse filmic\n",
    "geographies of scale. If we previously described how signal archaeology was\n",
    "used to analyse the SF-archive via AST, and how we subsequently used the\n",
    "application stum to create a dataset of intertitles from silent newsreels, we\n",
    "created yet another application, _SweScribe_, based on sound extraction from the\n",
    "collection of newsreels. _SweScribe_ uses _WhisperX_ <cite id=\"495hw\"><a\n",
    "href=\"#zotero%7C22783102%2F89SWX3HE\">(Bain et al., 2023)</a></cite>, combined\n",
    "with a Whisper model from OpenAI—with a wav2vec2 model fine-tuned for\n",
    "Swedish—to transcribe and temporally align speech <cite id=\"37mld\"><a\n",
    "href=\"#zotero%7C22783102%2FAGCZGSA8\">(Malmsten et al., 2022)</a></cite>.\n",
    "Feeding SweScribe with a total 2,553(`# TODO: verify count`) sound newsreels\n",
    "containing—the previously mentioned speech, music and other—we were able to\n",
    "create a textual dataset comprising 2,229,854(`# TODO: verify count`) words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[hermeneutics]\n",
    "\n",
    "Running SweScribe requires access to a GPU, so we will not be able to show how it works here. Instead we have copied over some of the _key_ functions to explain briefly how it all works together.\n",
    "\n",
    "**SweScribe** works in the following 4 steps:\n",
    "\n",
    "1. Extract the audio stream from the video.\n",
    "2. Transcribing the audio with _WhisperX_\n",
    "3. Aligning the transcription with the audio, to create timestamps, with _WhisperX_ and a _wav2vec2_ model.\n",
    "4. Cleaning up and removing known mistakes.\n",
    "\n",
    "The contribution of SweScribe lies in the fourth step. Through iterations of tests and cleaning we have found some, but likely not all, the ways in which Whisper hallucinates in the transcriptions.\n",
    "Some errors are very simple to detect, such as URLs. Other's are more subtle: for example, it is entierly plausible that the narration utters 'Musik.' -- it turns out that it was never the case -- so we had to go back to the audio to make sure. We will not go into the full details of all the errors we found, the source code for this tool is [open](https://github.com/Modern36/swescribe). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "\"\"\"\n",
    "//mpg_to_wav.py\n",
    "import logging\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# 1. Extract audio\n",
    "def extract_audio(mpg_file_path: Path, wav_file_path: Path):\n",
    "    logging.debug(f\"Starting extraction for: {mpg_file_path.name}\")\n",
    "\n",
    "    if not mpg_file_path.exists():\n",
    "        raise FileNotFoundError(f\"The file {mpg_file_path} does not exist.\")\n",
    "    elif not wav_file_path.parent.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"The ouput directory {wav_file_path.parent} does not exist.\"\n",
    "        )\n",
    "\n",
    "    # Use ffmpeg to extract audio, targeting a sample rate of 16kHz and a single channel (mono)\n",
    "    command = [\n",
    "        \"ffmpeg\",\n",
    "        \"-y\",  # Overwrite output files without asking\n",
    "        \"-i\",\n",
    "        str(mpg_file_path),  # Input file\n",
    "        \"-ar\",\n",
    "        \"16000\",  # Set audio sample rate to 16kHz\n",
    "        \"-ac\",\n",
    "        \"1\",  # Set audio channels to mono\n",
    "        str(wav_file_path),\n",
    "    ]\n",
    "\n",
    "    subprocess.run(\n",
    "        command,\n",
    "        check=True,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "    )\n",
    "    logging.debug(f\"Successfully extracted audio to: {wav_file_path}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "//wavpath_to_srt.py\n",
    "\"\"\"\n",
    "\n",
    "def wavpath_to_srt(audio_file_path: Path):\n",
    "    \"\"\"Reads WAV file and generates a subtitle file in SRT format.\n",
    "\n",
    "    Args:\n",
    "        audio_file_path (Path): The path to the WAV file.\n",
    "\n",
    "    Returns:\n",
    "        str: The formatted SRT content.\n",
    "    \"\"\"\n",
    "    # 2. Transcribing audio\n",
    "    transcription_result = model_transcribe(audio_file_path)\n",
    "\n",
    "    # 3. Align results\n",
    "    aligned_result = align_results(audio_file_path, transcription_result)\n",
    "\n",
    "    # 4. Cleaning\n",
    "    srt_content = alignment_to_srt(aligned_result)\n",
    "    return srt_content\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "//transcribe.py\n",
    "model = whisperx.load_model(\n",
    "    \"large\",\n",
    "    device,\n",
    "    language=language_code,\n",
    "    asr_options={\n",
    "        \"multilingual\": False,\n",
    "        \"max_new_tokens\": None,\n",
    "        \"clip_timestamps\": None,\n",
    "        \"hallucination_silence_threshold\": None,\n",
    "        \"hotwords\": None,\n",
    "    },\n",
    ")\n",
    "\n",
    "logging.info(\"\\nLoading alignment model...\")\n",
    "alignment_model, metadata = whisperx.load_align_model(\n",
    "    language_code=language_code,\n",
    "    device=device,\n",
    "    model_name=\"KBLab/wav2vec2-large-voxrex-swedish\",\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# 2. Transcribing audio\n",
    "def model_transcribe(audio_file_path):\n",
    "    return model.transcribe(str(audio_file_path))\n",
    "\n",
    "# 3. Align results\n",
    "def align_results(audio_file_path, transcription_result):\n",
    "    return whisperx.align(\n",
    "        transcription_result[\"segments\"],\n",
    "        alignment_model,\n",
    "        metadata,\n",
    "        str(audio_file_path),\n",
    "        device,\n",
    "        preprocess=False,\n",
    "    )\n",
    "\n",
    "\"\"\"\n",
    "//alignment_to_srt.py\n",
    "import logging\n",
    "from typing import Generator, List\n",
    "\n",
    "from swescribe.clean_whisper import clean_text\n",
    "\"\"\"\n",
    "\n",
    "def format_timestamp(seconds):\n",
    "    \"\"\"Create srt-compliant timestamp\n",
    "\n",
    "    Args:\n",
    "        seconds (float): The time in seconds, with milisecond precision\n",
    "\n",
    "    Returns:\n",
    "        str: The formatted timestamp\n",
    "    \"\"\"\n",
    "    hours = int(seconds // 3600)\n",
    "    minutes = int((seconds % 3600) // 60)\n",
    "    secs = int(seconds % 60)\n",
    "    milliseconds = int((seconds - int(seconds)) * 1000)\n",
    "    return f\"{hours:02d}:{minutes:02d}:{secs:02d},{milliseconds:03d}\"\n",
    "\n",
    "\n",
    "def cleaned_segments(\n",
    "    segments: List[dict[str:any]],\n",
    ") -> Generator[dict[str:str], None, None]:\n",
    "    for segment in segments:\n",
    "        text = clean_text(segment[\"text\"].strip())\n",
    "\n",
    "        if not text:\n",
    "            logging.debug(f\"Skipping empty segment {segment=}\")\n",
    "            continue\n",
    "\n",
    "        yield {\n",
    "            \"start\": format_timestamp(segment[\"start\"]),\n",
    "            \"end\": format_timestamp(segment[\"end\"]),\n",
    "            \"text\": text,\n",
    "        }\n",
    "\n",
    "\n",
    "def alignment_to_srt(aligned_result: List[dict]):\n",
    "    \"\"\"Convert aligned result to SRT format.\n",
    "\n",
    "    Args:\n",
    "        aligned_result List of (dict): The aligned result from the Whisperx library.\n",
    "\n",
    "    Returns:\n",
    "        str: The formatted SRT content.\n",
    "    \"\"\"\n",
    "    srt_content = \"\"\n",
    "    for idx, segment in enumerate(\n",
    "        cleaned_segments(aligned_result[\"segments\"]), start=1\n",
    "    ):\n",
    "        srt_content += f\"{idx}\\n{segment['start']} --> {segment['end']}\\n{segment['text']}\\n\\n\"\n",
    "\n",
    "    return srt_content\n",
    "\n",
    "\"\"\"\n",
    "//clean_whisper.py\n",
    "\"\"\"\n",
    "\n",
    "# 4. Cleaning steps\n",
    "def clean_text(text):\n",
    "    result = text[:].strip()\n",
    "    for function in (\n",
    "        clean_elipsis,\n",
    "        clean_dashes,\n",
    "        clean_for,\n",
    "        clean_line_artefact,\n",
    "        clean_ja_repeat,\n",
    "        clean_urls,\n",
    "        clean_spaces,\n",
    "    ):\n",
    "        if result == \"\":\n",
    "            break\n",
    "        result = function(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "0va8r": [
       {
        "id": "22783102/WEZNWR96",
        "source": "zotero"
       }
      ],
      "p4u1o": [
       {
        "id": "22783102/5LZCK9F3",
        "source": "zotero"
       }
      ],
      "typgb": [
       {
        "id": "22783102/SSCKKMRE",
        "source": "zotero"
       }
      ],
      "uj108": []
     }
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "It should be noted that early testing revealed weaknesses: SweScribe struggled\n",
    "with noisy environments and overlapping speakers, and tended to hallucinate\n",
    "plausible but often incorrect content when speech was unintelligible. The model\n",
    "also produces artefacts likely leaked from training data, such as subtitle\n",
    "credits. We addressed these through iterative cleaning steps, expanding our\n",
    "ground truth dataset from 27 to 89 manual transcriptions (72,812 words)\n",
    "across three sampling rounds, which considerably reduced Word\n",
    "Error Rate (WER) from 17 percent to only seven percent <cite id=\"p4u1o\"><a\n",
    "href=\"#zotero%7C22783102%2F5LZCK9F3\">(Aspenskog et al., 2025)</a></cite>. As we\n",
    "have previously stated, when it came to sound newsreels were characterised by\n",
    "voice-over commentary, sometimes designated as voice-of-God narration, with a\n",
    "smaller proportion of interviews, and other speech events. Then again, with a\n",
    "low WER, the resulting Journal Digital corpus was deemed sufficiently accurate\n",
    "for the newsreels to be treated as textual sources. This transmediation enabled\n",
    "us to model a range of distant reading <cite id=\"typgb\"><a\n",
    "href=\"#zotero%7C22783102%2FSSCKKMRE\">(Moretti, 2000)</a></cite> and scalable\n",
    "reading approaches that hoovered between micro and macro level analyses (<cite\n",
    "id=\"uj108\"><a href=\"#zotero%7C22783102%2F8XFJBPEC\">Weitin 2017</a></cite>,\n",
    "<cite id=\"0va8r\"><a href=\"#zotero%7C22783102%2FWEZNWR96\">Fickers &#38;\n",
    "Clavert 2021</a></cite>)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[hermen]\n",
    "\n",
    "On the positive site, relying on WhisperX for transcriptions has shown to be deterministic (pending on hardware): every time we reran the pipeline on the same hardware we got identical outputs. This observation encouraged us to adapt somewhat to the model's behaviour. A non-obvious case where it diverged from our initial transcriptions was on regnal numbers: It would consistently spit out roman numerals. Since this is one of multiple, technically, correct ways to do it, we followed suit and made sure that all our _Ground Truth_ transcriptions used roman numerals too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "In this article precursory examples have stressed different ways to address\n",
    "modernity via newsreel representation. SweScribe can be used in a similar way,\n",
    "and in the following we will showcase an analysis of various relations between\n",
    "the provincial and the urban in Swedish newsreels during a period of immense\n",
    "modernisation. One way is to examine the general division between the Stockholm\n",
    "region on one hand, and mentions of other Swedish regions on the other. We have\n",
    "been primarily interested in the urban gaze of the provincial, hence we pay\n",
    "particular attention to regions in the south and west of Sweden, situated near\n",
    "the second largest cities respectively, Gothenburg and Malmö. It should be\n",
    "stressed that each newsreel has a corresponding catalogue entry in the Swedish\n",
    "Media Database. Metadata include title, year, description, people, identifiers,\n",
    "and so called SAB subject codes (a library classification system). Such SAB\n",
    "codes were once assigned to newsreels, representing geographic as well as\n",
    "topical keywords. Topical SAB spans broader categories such as Military, to\n",
    "more specific ones like Christmas customs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "In order to proceed, we scraped all metadata entries of each newsreel and\n",
    "extracted SAB codes from the catalogue. We discovered that the annotations\n",
    "varied in extensiveness, with some newsreels being carefully annotated, while\n",
    "others were poorly marked. This suggested that existing SAB metadata for the\n",
    "newsreels were sometimes unreliable for topic or geographic analysis. We\n",
    "continued our research set-up, and analysed the Journal Digital corpus through\n",
    "the application of Named Entity Recognition (NER), which—as is well known—can\n",
    "transform a corpus into a structured representation of who, what, and where.\n",
    "For our purposes, we applied off-the-shelf NER models primarily to extract\n",
    "toponyms from the corpus. When aggregated at the level of individual reels,\n",
    "years, or thematic groupings, these entities made it possible to pose questions\n",
    "such as which regions were most frequently mentioned in the commentary, and in\n",
    "what kinds of discursive surroundings. To compare these two approaches, we\n",
    "visualised both layers —- the geographic SAB codes and NER data —- through an\n",
    "interactive heatmap with a chronological slider. This visualisation confirmed\n",
    "that the differences between the SAB and NER datasets were substantial,\n",
    "reinforcing our earlier observation about the inconsistency of the SAB\n",
    "annotations. While functioning as a birds-eye comparison and verification tool,\n",
    "​the heatmap also made it possible to see broader geographical trends in\n",
    "newsreel coverage over time at a glance. Given the often unreliable SAB\n",
    "metadata, we chose NER data for our continued geographic analysis of the\n",
    "corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Placholder\n",
    "Named Entity Recognition is a forceful classifier, but it usually requires\n",
    "extensive data cleaning and processing in order to be analysable. The first\n",
    "step was word reconstruction. BERT’s subword tokenization often fragmented\n",
    "words, such as: “Stock” + “##holm”. We reconstructed complete words by merging\n",
    "adjacent fragments based on text position and joining multi-word locations\n",
    "(“New” + “York” → “New York”), reducing 7,648 raw entities to meaningful\n",
    "locations in a new table. The second step was gazetteer validation against\n",
    "GeoNames using exact matching, for high-confidence NER scores ≥0.95, and fuzzy\n",
    "matching (85 percent threshold). We normalised possessive forms—“Sveriges” →\n",
    "“Sverige”—and excluded false positives, such as fragments and more generic\n",
    "terms. This validation produced the verified_entity table with an 82 percent\n",
    "validation rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER hermen\n",
    "\n",
    "The first step in creating a list of mentioned locations to geocode is to extract all the locations from the transcriptions. For this we relied on a model trained for Named Entity Recognition (NER) on Swedish materials \\[TODO: reference \\]. The outputs are smalle, but rerunning the entire pipeline is slow, so we save all outputs from every file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "//extract_entities.py\n",
    "\n",
    "from transformers import pipeline\n",
    "from journal_digital import Corpus\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def NER_main():\n",
    "    # 1. Load NER model\n",
    "    nlp = pipeline(\n",
    "        \"ner\",\n",
    "        model=\"KB/bert-base-swedish-cased-ner\",\n",
    "        tokenizer=\"KB/bert-base-swedish-cased-ner\",\n",
    "        device=\"cuda:1\",\n",
    "    )\n",
    "\n",
    "    # 2. Prepare speech corpus\n",
    "    corpus = Corpus(\"txt\", texts_to_include=\"speech\")\n",
    "\n",
    "    for srt in tqdm(corpus):\n",
    "        file = srt.filename\n",
    "        text = srt.content\n",
    "        year = srt.year\n",
    "        out_file = out_dir / srt.path.with_suffix(\".json\").name\n",
    "\n",
    "        if out_file.exists():\n",
    "            continue\n",
    "\n",
    "        lines = split_pattern.split(text)\n",
    "        results = {\n",
    "            \"year\": year,\n",
    "            \"video_name\": file,\n",
    "            \"entities\": [outputs_to_json_friendly(nlp(line)) for line in lines],\n",
    "        }\n",
    "\n",
    "        # 3. Store a transcription level nested JSON file\n",
    "        with open(out_file, \"x\") as f:\n",
    "            json.dump(results, f, indent=1)\n",
    "\n",
    "\n",
    "def outputs_to_json_friendly(data: list[dict]):\n",
    "    return [output_to_json_friendly(small_dict) for small_dict in data]\n",
    "\n",
    "\n",
    "def output_to_json_friendly(small_dict):\n",
    "    return {\n",
    "        \"entity\": small_dict[\"entity\"],\n",
    "        \"score\": float(small_dict[\"score\"]),\n",
    "        \"index\": small_dict[\"index\"],\n",
    "        \"word\": small_dict[\"word\"],\n",
    "        \"start\": small_dict[\"start\"],\n",
    "        \"end\": small_dict[\"end\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning [hermen]\n",
    "The outputs from the model include the types of the entity, confidence scores, the word and indexi for where in the original text the entity was found.\n",
    "For better or worse, the entities are themselves split up into parts: \"Storatorget\"  --verbatim \"The large marketplace\" -- is divided into the entries \"Stora\" and \"##torget\". Whereas \"Gustav Adolfs torg\" is split into \"Gustav\", \"Adolfs\", and \"torg\". In short, before we can expect to get any reliable geocoding out of it, we need to do a lot of cleaning. The first being what we alluded to:\n",
    "merge 'LOC' entities based on immediate adjacency, removing ##-prefixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "//step2_clean_locations.py\n",
    "\"\"\"\n",
    "def reconstruct_locations(entities):\n",
    "    \"\"\"\n",
    "    Reconstruct locations by grouping entities using the pre-computed grp field.\n",
    "\n",
    "    The grp field is assigned by group_entities() in load_entities_to_db.py\n",
    "    and indicates which entities should be merged based on adjacency.\n",
    "\n",
    "    This function:\n",
    "    1. Groups entities by their grp field\n",
    "    2. Merges all words within each group (handling ## fragments)\n",
    "    3. Returns list of reconstructed location names\n",
    "\n",
    "    Args:\n",
    "        entities: List of (word, entity_type, score, entry_index, start, end, srt_idx, grp, year) tuples\n",
    "\n",
    "    Returns:\n",
    "        List of cleaned location strings\n",
    "    \"\"\"\n",
    "    if not entities:\n",
    "        return []\n",
    "\n",
    "    from collections import defaultdict\n",
    "\n",
    "    # Group entities by grp field\n",
    "    groups = defaultdict(list)\n",
    "    for entity in entities:\n",
    "        word, _, _, _, _, _, _, grp, _ = entity\n",
    "        groups[grp].append(word)\n",
    "\n",
    "    locations = []\n",
    "    for grp_id, words in groups.items():\n",
    "        # Merge ## fragments and join words\n",
    "        merged = []\n",
    "        for word in words:\n",
    "            if word.startswith(\"##\"):\n",
    "                if merged:\n",
    "                    # Merge with previous word (remove ##)\n",
    "                    merged[-1] += word[2:]\n",
    "                # else: skip standalone ##\n",
    "            else:\n",
    "                merged.append(word)\n",
    "\n",
    "        # Join with spaces and title-case\n",
    "        if merged:\n",
    "            location = \" \".join(merged).strip().title()\n",
    "            locations.append(location)\n",
    "\n",
    "    return locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# verification [hermen]\n",
    "Now we have a long list of locations -- but not necessarily locations that can be located, eg: \"_idrottsplats_\", ~= \"sports field\". In order to reduce the list of locations to those that could reasonably be expected to be geolocated we compared all location names against GeoNames -- an open-sourced gazetteer -- which contains an exhaustive list of Swedish locations. This list also contains a set of alternate names for most locations, such as alternative spellings or foreign namings. We supplement this list with a manually crafted set of occuring international locations that the list would miss and then we compare each uniqe location name to these lists in a series of more 'relaxed' comparisons from perfect match to a fuzzy string matching.\n",
    "\n",
    "We pass each location name through the following steps.\n",
    " - Step 0: Filter out locations from a short 'blacklist'\n",
    " - Step 0b: Apply normalization on known 'misspellings'.\n",
    " - Step 1: Locations with a high confidence score from the NER model are passed on as-is.\n",
    " - Step 2: Locations that exactly match an entry in the gazetteer are passed on.\n",
    " - Step 3: Try to apply the gazetteer normalizer, using aliases that point to a single location.\n",
    " - Step 4: Fuzzy matching in two steps:\n",
    "    - First against the gazetteers list of names\n",
    "    - Then against the list of aliases, and mapping it to the name.\n",
    "\n",
    "Locations that are not captured by any of these steps are discarded. The end result is a list of locations that we will use for two purposes: Firstly, to geocode and subsequently map all the mentioned swedish locations. Secondly, compare the mentions of Swedish Vs Foreign locations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "step3_filter_gazetteer\n",
    "\"\"\"\n",
    "\n",
    "# Verification\n",
    "def verify_locations(session):\n",
    "    \"\"\"\n",
    "    Verify locations from clean_entity table against gazetteer.\n",
    "\n",
    "    Process:\n",
    "    1. Check if location has high-confidence score → keep as-is\n",
    "    2. Check if location exactly matches gazetteer → keep as-is\n",
    "    3. Try fuzzy matching → if match >= threshold, use canonical name\n",
    "    4. Otherwise → discard\n",
    "\n",
    "    Args:\n",
    "        session: SQLModel session\n",
    "        gazetteer: Set of known place names\n",
    "\n",
    "    Returns:\n",
    "        List of (video_name, verified_location, year, mentions) tuples\n",
    "    \"\"\"\n",
    "    from journal_NER.models import CleanEntity\n",
    "\n",
    "    # 1. construct set of spellings and mapping to 'correct' spelling\n",
    "    gazetteer, gazetteer_normalizer = make_gazzeteer_and_normalizer()\n",
    "\n",
    "    # Get all locations from clean_entity using SQLModel\n",
    "    results = (\n",
    "        session.query(\n",
    "            CleanEntity.video_name,\n",
    "            CleanEntity.location,\n",
    "            CleanEntity.year,\n",
    "            CleanEntity.mentions,\n",
    "        )\n",
    "        .order_by(CleanEntity.location)\n",
    "        .all()\n",
    "    )\n",
    "\n",
    "    all_entries = results\n",
    "\n",
    "    # Get high-confidence words\n",
    "    high_conf_words = get_high_confidence_locations(session)\n",
    "\n",
    "    verified = []\n",
    "    exact_matches = 0\n",
    "    fuzzy_matches = 0\n",
    "    high_conf_matches = 0\n",
    "    rejected = 0\n",
    "    blacklisted = 0\n",
    "    normalized = 0\n",
    "\n",
    "    location_stats = defaultdict(\n",
    "        lambda: {\n",
    "            \"exact\": 0,\n",
    "            \"fuzzy\": 0,\n",
    "            \"high_conf\": 0,\n",
    "            \"rejected\": 0,\n",
    "            \"blacklisted\": 0,\n",
    "            \"normalized\": 0,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    for video_name, location, year, mentions in tqdm(\n",
    "        all_entries, desc=\"Processing high-confidence locations.\"\n",
    "    ):\n",
    "        verified_location = None\n",
    "        match_type = None\n",
    "\n",
    "        # Step 0: Check blacklist first\n",
    "        if location in constants.GAZETTEER_BLACKLIST:\n",
    "            match_type = \"blacklisted\"\n",
    "            blacklisted += 1\n",
    "            location_stats[location][match_type] += 1\n",
    "            continue\n",
    "\n",
    "        # Step 0b: Apply normalization\n",
    "        if location in constants.LOCATION_NORMALIZE:\n",
    "            normalized_loc = constants.LOCATION_NORMALIZE[location]\n",
    "            location = normalized_loc\n",
    "            normalized += 1\n",
    "\n",
    "        # Strategy 1: High confidence from original NER\n",
    "        if location in high_conf_words:\n",
    "            verified_location = location\n",
    "            match_type = \"high_conf\"\n",
    "            high_conf_matches += 1\n",
    "\n",
    "        # Strategy 2: Exact match in gazetteer\n",
    "        elif location in gazetteer:\n",
    "            verified_location = location\n",
    "            match_type = \"exact\"\n",
    "            exact_matches += 1\n",
    "\n",
    "        # Stragety 3: Map against known, clear aliases:\n",
    "        elif location in gazetteer:\n",
    "            verified_location = gazetteer_normalizer[location]\n",
    "            match_type = \"normalized\"\n",
    "\n",
    "        # Strategy 4: Fuzzy match\n",
    "        else:\n",
    "            matched_name, score = fuzzy_match_location(location, gazetteer)\n",
    "            if matched_name:\n",
    "                verified_location = matched_name  # Use canonical form\n",
    "                match_type = \"fuzzy\"\n",
    "                fuzzy_matches += 1\n",
    "            else:\n",
    "                matched_name, score = fuzzy_match_location(\n",
    "                    location, gazetteer_normalizer.keys()\n",
    "                )\n",
    "                if matched_name:\n",
    "                    verified_location = gazetteer_normalizer[matched_name]\n",
    "                    match_type = \"fuzzy\"\n",
    "                    fuzzy_matches += 1\n",
    "\n",
    "                else:\n",
    "                    match_type = \"rejected\"\n",
    "                    rejected += 1\n",
    "\n",
    "        if verified_location:\n",
    "            verified.append((video_name, verified_location, year, mentions))\n",
    "\n",
    "        location_stats[location][match_type] += 1\n",
    "\n",
    "    return verified, location_stats\n",
    "\n",
    "# 1. construct set of spellings and mapping to 'correct' spelling\n",
    "def make_gazzeteer_and_normalizer():\n",
    "    tmp_normalizer = defaultdict(set)\n",
    "    canons = constants.GAZETTEER_INTERNATIONAL_PLACES\n",
    "\n",
    "    # 1.1 Load Geonames data\n",
    "    for spelling, canon in load_geonames_variants():\n",
    "        canons.add(canon)\n",
    "        if spelling is not None:\n",
    "            tmp_normalizer[spelling].add(canon)\n",
    "\n",
    "    result = {}\n",
    "    for spelling, canon_set in tmp_normalizer.items():\n",
    "        if spelling in constants.LOCATION_NORMALIZE:\n",
    "            continue\n",
    "        if len(canon_set) == 0:\n",
    "            raise ValueError(f\"Empty canon_set for {spelling}\")\n",
    "        elif len(canon_set) == 1:\n",
    "            result[spelling] = list(canon_set)[0]\n",
    "    return canons, result\n",
    "\n",
    "# 1.1 Load Geonames data\n",
    "def load_geonames_variants():\n",
    "    with open(geonames_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            fields = line.strip().split(\"\\t\")\n",
    "            if len(fields) < 4:\n",
    "                continue\n",
    "\n",
    "            # Add primary name\n",
    "            name = fields[1].strip()\n",
    "            yield None, name\n",
    "\n",
    "            # Add ASCII name if different\n",
    "            asciiname = fields[2].strip()\n",
    "            if asciiname and asciiname != name:\n",
    "                yield asciiname, name\n",
    "\n",
    "            # Add alternate names\n",
    "            alternates = fields[3].strip()\n",
    "            if alternates:\n",
    "                for alt in alternates.split(\",\"):\n",
    "                    alt = alt.strip()\n",
    "                    if alt:\n",
    "                        yield alt, name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geocoding [Hermen]\n",
    "\n",
    "For geocoding we set up a locally run Nominatim [docker instance](https://github.com/mediagis/nominatim-docker/blob/master/howto.md#persistent-container-data) with the data for Sweden. This allowed us to query persistently without rate limits.\n",
    "For each location we query Nominatim, storing the results for later review and comparison.\n",
    "Many of the locations names in our list are fairly generic, or even when the name is distinct -- the toponym might be part of another toponym. Unfortunately, Nomination does not always produce the _correct_ location as the top results: so we ask for five results and apply a few simple automated steps to help make the process quicker, followed up by a manual review of suggestions.\n",
    "\n",
    "When there is but a single result -- it gets picked as _the correct_ geocoding of the location. When there are multiple results, we measure the distance between their central coordinates -- if the longest distance is <= 5km, we pick the first results. We also check for common patterns that selects the city, municipality, province if mentioned, in that order.\n",
    "The rest were resolved with a manual review of the suggestions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "geocode_locations.py\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def geocode_location(\n",
    "    location_name: str, session: Session, limit: int = 5\n",
    ") -> List[GeocodeCandidate]:\n",
    "    \"\"\"\n",
    "    Geocode a location using local Nominatim server.\n",
    "\n",
    "    Queries Nominatim and stores top results in GeocodeCandidate table.\n",
    "\n",
    "    Args:\n",
    "        location_name: Name to geocode\n",
    "        session: SQLModel session for database operations\n",
    "        limit: Maximum number of results to retrieve (default: 5)\n",
    "\n",
    "    Returns:\n",
    "        List of GeocodeCandidate objects (empty if no results)\n",
    "    \"\"\"\n",
    "    url = f\"{constants.NOMINATIM_BASE_URL}/search.php?q={location_name}&format=json&limit={limit}\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(\n",
    "            url, timeout=constants.NOMINATIM_TIMEOUT_SECONDS\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        results = response.json()\n",
    "\n",
    "        if not results:\n",
    "            logger.warning(f\"No geocoding results for: {location_name}\")\n",
    "            return []\n",
    "\n",
    "        candidates = []\n",
    "        for rank, result in enumerate(results):\n",
    "            # Extract bounding box if present\n",
    "            bbox_min_lat = None\n",
    "            bbox_max_lat = None\n",
    "            bbox_min_lon = None\n",
    "            bbox_max_lon = None\n",
    "\n",
    "            if \"boundingbox\" in result and len(result[\"boundingbox\"]) == 4:\n",
    "                bbox_min_lat = float(result[\"boundingbox\"][0])\n",
    "                bbox_max_lat = float(result[\"boundingbox\"][1])\n",
    "                bbox_min_lon = float(result[\"boundingbox\"][2])\n",
    "                bbox_max_lon = float(result[\"boundingbox\"][3])\n",
    "\n",
    "            candidate = GeocodeCandidate(\n",
    "                location_name=location_name,\n",
    "                place_id=result.get(\n",
    "                    \"place_id\", 0\n",
    "                ),  # Some responses lack place_id\n",
    "                osm_type=result.get(\n",
    "                    \"osm_type\", result.get(\"class\", \"\")\n",
    "                ),  # Fallback to class\n",
    "                osm_id=result.get(\"osm_id\", 0),  # Some responses lack osm_id\n",
    "                display_name=result.get(\n",
    "                    \"display_name\", result.get(\"name\", location_name)\n",
    "                ),\n",
    "                place_rank=result.get(\"place_rank\", 0),\n",
    "                category=result.get(\n",
    "                    \"category\", result.get(\"class\", \"\")\n",
    "                ),  # Fallback to class\n",
    "                type=result.get(\"type\", \"\"),\n",
    "                importance=result.get(\"importance\", 0.0),\n",
    "                lat=float(result[\"lat\"]),\n",
    "                lon=float(result[\"lon\"]),\n",
    "                boundingbox_lat_min=bbox_min_lat,\n",
    "                boundingbox_lat_max=bbox_max_lat,\n",
    "                boundingbox_lon_min=bbox_min_lon,\n",
    "                boundingbox_lon_max=bbox_max_lon,\n",
    "                raw_json=json.dumps(result),\n",
    "                result_rank=rank,\n",
    "            )\n",
    "            session.add(candidate)\n",
    "            candidates.append(candidate)\n",
    "\n",
    "        session.commit()\n",
    "        logger.info(\n",
    "            f\"Geocoded '{location_name}': {len(candidates)} candidates\"\n",
    "        )\n",
    "        return candidates\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        logger.error(f\"Nominatim request failed for {location_name}: {e}\")\n",
    "        return []\n",
    "    except (KeyError, ValueError, TypeError) as e:\n",
    "        logger.error(\n",
    "            f\"Error parsing Nominatim response for {location_name}: {e}\"\n",
    "        )\n",
    "        return []\n",
    "\n",
    "def auto_select_swedish_location(\n",
    "    query_name: str, candidates: List[GeocodeCandidate]\n",
    ") -> Optional[GeocodeCandidate]:\n",
    "    \"\"\"\n",
    "    Apply Swedish-specific rules to auto-select the correct candidate.\n",
    "\n",
    "    Priority order:\n",
    "    1. Result starting with \"query_name(s)?, query_name(s)? kommun\"\n",
    "    2. Result starting with \"query_name(s)? kommun\"\n",
    "    3. Result starting with \"Landskapet query_name(s)?\"\n",
    "\n",
    "    Possessives are handled: optional 's' at end of query_name.\n",
    "    If multiple candidates match the same rule, checks distance between them.\n",
    "    Only auto-selects if all matches are within the distance threshold.\n",
    "\n",
    "    All comparisons are case-insensitive using regex.\n",
    "\n",
    "    Args:\n",
    "        query_name: The original location name being queried\n",
    "        candidates: List of candidate results from Nominatim\n",
    "\n",
    "    Returns:\n",
    "        Selected candidate if a rule matches and distance check passes, None otherwise\n",
    "    \"\"\"\n",
    "    if not candidates:\n",
    "        return None\n",
    "\n",
    "    # Escape query_name for regex, make case-insensitive\n",
    "    query_escaped = re.escape(query_name.lower())\n",
    "\n",
    "    # Rule 1: Starts with \"query_name(s)?, query_name(s)? kommun\"\n",
    "    pattern1 = re.compile(\n",
    "        rf\"^{query_escaped}(s)?,\\s*{query_escaped}(s)?\\s+kommun\", re.IGNORECASE\n",
    "    )\n",
    "    matches = [c for c in candidates if pattern1.match(c.display_name.lower())]\n",
    "    if matches:\n",
    "        if len(matches) == 1:\n",
    "            logger.info(\n",
    "                f\"  → Auto-selected (Rule 1: '{query_name}(s)?, {query_name}(s)? kommun'): {matches[0].display_name}\"\n",
    "            )\n",
    "            return matches[0]\n",
    "        else:\n",
    "            # Multiple matches - check distance\n",
    "            max_dist = calculate_max_distance(matches)\n",
    "            if max_dist <= constants.GEOCODE_DISTANCE_THRESHOLD_KM:\n",
    "                logger.info(\n",
    "                    f\"  → Auto-selected (Rule 1: '{query_name}(s)?, {query_name}(s)? kommun', {len(matches)} within {max_dist:.1f}km): {matches[0].display_name}\"\n",
    "                )\n",
    "                return matches[0]\n",
    "            # Distance too high, try next rule\n",
    "\n",
    "    # Rule 2: Starts with \"query_name(s)? kommun\"\n",
    "    pattern2 = re.compile(rf\"^{query_escaped}(s)?\\s+kommun\", re.IGNORECASE)\n",
    "    matches = [c for c in candidates if pattern2.match(c.display_name.lower())]\n",
    "    if matches:\n",
    "        if len(matches) == 1:\n",
    "            logger.info(\n",
    "                f\"  → Auto-selected (Rule 2: '{query_name}(s)? kommun'): {matches[0].display_name}\"\n",
    "            )\n",
    "            return matches[0]\n",
    "        else:\n",
    "            # Multiple matches - check distance\n",
    "            max_dist = calculate_max_distance(matches)\n",
    "            if max_dist <= constants.GEOCODE_DISTANCE_THRESHOLD_KM:\n",
    "                logger.info(\n",
    "                    f\"  → Auto-selected (Rule 2: '{query_name}(s)? kommun', {len(matches)} within {max_dist:.1f}km): {matches[0].display_name}\"\n",
    "                )\n",
    "                return matches[0]\n",
    "            # Distance too high, try next rule\n",
    "\n",
    "    # Rule 3: Starts with \"Landskapet query_name(s)?\"\n",
    "    pattern3 = re.compile(rf\"^landskapet\\s+{query_escaped}(s)?\", re.IGNORECASE)\n",
    "    matches = [c for c in candidates if pattern3.match(c.display_name.lower())]\n",
    "    if matches:\n",
    "        if len(matches) == 1:\n",
    "            logger.info(\n",
    "                f\"  → Auto-selected (Rule 3: 'Landskapet {query_name}(s)?'): {matches[0].display_name}\"\n",
    "            )\n",
    "            return matches[0]\n",
    "        else:\n",
    "            # Multiple matches - check distance\n",
    "            max_dist = calculate_max_distance(matches)\n",
    "            if max_dist <= constants.GEOCODE_DISTANCE_THRESHOLD_KM:\n",
    "                logger.info(\n",
    "                    f\"  → Auto-selected (Rule 3: 'Landskapet {query_name}(s)?', {len(matches)} within {max_dist:.1f}km): {matches[0].display_name}\"\n",
    "                )\n",
    "                return matches[0]\n",
    "            # Distance too high, fall through\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## MAP widget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import http.server\n",
    "import socketserver\n",
    "import threading\n",
    "import time\n",
    "\n",
    "dist = data / 'dist'\n",
    "map_widget_port = 8822\n",
    "\n",
    "# Custom handler to serve from dist/\n",
    "class SilentHandler(http.server.SimpleHTTPRequestHandler):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, directory=str(dist.resolve()), **kwargs)\n",
    "\n",
    "    def log_message(self, format, *args):\n",
    "        pass  # Suppress server logs\n",
    "\n",
    "# Function to run server\n",
    "def run_server():\n",
    "    with socketserver.TCPServer((\"\", map_widget_port), SilentHandler) as httpd:\n",
    "        httpd.serve_forever()\n",
    "\n",
    "# Start server in background thread\n",
    "server_thread = threading.Thread(target=run_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "# Wait for server to start\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame, display, HTML\n",
    "\n",
    "display(IFrame(src=f'http://localhost:{map_widget_port}', width='100%', height='900'),\n",
    "       metadata={\n",
    "    \"jdh\": {\n",
    "        \"module\": \"object\",\n",
    "        \"object\": {\n",
    "            \"tags\": [\"figure-jdc-map-*\"],\n",
    "            \"type\":\"image\",\n",
    "            \"source\": [\n",
    "                \"A mapping of mentioned locations in the Journal Digital Corpus (red) \"\n",
    "                \"versus the locations referenced by the SAB-catalogization (blue). \"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hermeneutics"
    ]
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Explain map above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(Image(\"./media/img11.png\", width=800), metadata={\n",
    "    \"jdh\": {\n",
    "        \"module\": \"object\",\n",
    "        \"object\": {\n",
    "            \"tags\": [\"figure-sefyr-*\"],\n",
    "            \"type\":\"image\",\n",
    "            \"source\": [\n",
    "                \"During the mid 1930s Swedish Film Industry joined the \"\n",
    "                \"newspaper _Stockholm-Tidningen_ and purchased an airplane \"\n",
    "                \"—SE-FYR. The small aircraft was used extensively within \"\n",
    "                \"newsreel film production, both as a way to reach regional \"\n",
    "                \"locations in Sweden, as well as for aerial cinematography. \"\n",
    "                \"Illustrations from Svensk Flyghistorisk Förening.\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### A Flying Symbol of Modernity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "It did not come as a total surprise that _SF’s Weekly Review_ favoured the\n",
    "capital of Sweden. Extracted location data from SweScribe reveals a persistent\n",
    "over-representation of the Stockholm region. Yet, given SF’s metropolitan base,\n",
    "it raises questions if there were any attempts made to address this form of\n",
    "over-representation. While exploring the corpus, we encountered repeated\n",
    "references to an airplane named Sefyr—or SE-FYR—jointly owned by SF and the\n",
    "newspaper _Stockholms-Tidningen_; the aircraft was acquired in 1934. Sefyr was\n",
    "dispatched whenever important events occurred at distances from the capital,\n",
    "and required rapid coverage both in SF’s newsreels and in\n",
    "_Stockholms-Tidningen’s_ reports. This prompted us to investigate whether Sefyr\n",
    "might have functioned as an infrastructural response to the geographic\n",
    "concentration visible in our data—and whether its deployment had any measurable\n",
    "effect on the spatial distribution of newsreel coverage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "bduio": [
       {
        "id": "22783102/ZWKH378T",
        "source": "zotero"
       }
      ],
      "kyvdw": [
       {
        "id": "22783102/ERJVIH9J",
        "source": "zotero"
       }
      ],
      "o5ttz": [
       {
        "id": "22783102/YQMWCYSZ",
        "source": "zotero"
       }
      ]
     }
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "There is no prior academic inquiry on Sefyr’s function in SF’s newsreels.\n",
    "However, the company’s twenty-fifth anniversary book described the aircraft’s\n",
    "role within newsreel production. Besides the fact that the airplane “always\n",
    "lent something of the allure of speed and the wide open spaces,” its merits as\n",
    "an express carrier of news were habitually emphasised. Momentum and tempo —- chief\n",
    "characteristics of modernity -— were often associated with Sefyr. One vivid\n",
    "example was the marriage of the Swedish Princess Ingrid to the Danish Prince\n",
    "Frederik in late May 1935. Stockholm cinemas were packed with audiences who\n",
    "wanted to see daily audiovisual reports surrounding the wedding, which took\n",
    "place in the capital. The SF-newsreel department had even promised coverage of\n",
    "the princely couple’s arrival in Copenhagen aboard the Danish royal yacht:\n",
    "“There was nothing for it but to advertise it. Without reservation. Sefyr was\n",
    "the guarantor.” In fact, Sefyr’s “popular pilot,” Åke Söderberg, arrived in\n",
    "Copenhagen the day before to make preparations, together with a reporter and a\n",
    "cameraman. As soon as the team had shot their material from both air and land,\n",
    "the airplane took off for Stockholm with the reels. Sefyr landed in Stockholm\n",
    "at 18:40; the reels were then developed and edited, and a few hours later the\n",
    "sequences were shown to audiences by the end of that evening’s last screenings\n",
    "in Stockholm <cite id=\"bduio\"><a\n",
    "href=\"#zotero%7C22783102%2FZWKH378T\">(Skoglund, 1944: 159–160)</a></cite>.\n",
    "Moreover, in December 1936, _Stockholms-Tidningen_ proudly proclaimed with a\n",
    "large header in Aftonbladet the aircraft’s faithful service: “the only\n",
    "newspaper in the Nordic countries with its own news aircraft ... Sefyr will\n",
    "guarantee the best film material also in 1937” <cite id=\"o5ttz\"><a href=\"#zotero%7C22783102%2FYQMWCYSZ\">(Aftonbladet, 1936)</a></cite>. \n",
    "In a similar vein, readers were\n",
    "reassured that “wherever something happens, someone from our wide circle of\n",
    "colleagues is always there to relay the news back to us by telephone, car or\n",
    "airplane” <cite id=\"kyvdw\"><a href=\"#zotero%7C22783102%2FERJVIH9J\">(Aftonbladet, 1936)</a></cite>.\n",
    "Sefyr was, hence, not merely a tool for speeding\n",
    "up news distribution. It also formed part of a broader self-image of a\n",
    "technologically well-equipped, mobile and tempo-prone news organisation—a\n",
    "flying symbol of journalistic modernity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "esoxw": [
       {
        "id": "22783102/M22JCP98",
        "source": "zotero"
       }
      ]
     }
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Within the Journal Digital corpus, searches for the term Sefyr yields 19 hits across 12 video\n",
    "transcriptions <cite id=\"esoxw\"><a href=\"#zotero%7C22783102%2FM22JCP98\">(Modern Times 1936, 2025)</a></cite>, all dating from the period\n",
    "1934–36. Sefyr was first mentioned in passing in a 1934 newsreel where the\n",
    "narrator spoke of it as a successor to another SF-airplane. Sefyr then debuted\n",
    "in February 1935, which was reported in another newsreel (SF855B); as the\n",
    "airplane flew over SF’s studios in the outskirts of Stockholm on its maiden\n",
    "flight, carrying the directors of the two owning companies, the narrator\n",
    "proclaimed: “Surely our engine noise interrupts the final scene \\[below\\] but\n",
    "we dare to indulge ourselves on this first day with Sefyr.” Given the\n",
    "celebratory meta-reporting, the airplane appears to have functioned both as a\n",
    "media-infrastructural solution and as well as a media event (in itself) -— a\n",
    "technological novelty that SF and _Stockholms-Tidningen_ could foreground to\n",
    "visualise modern, mobile news-gathering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(Video(\"./media/vid4.mp4\", width=500), metadata={\n",
    "    \"jdh\": {\n",
    "        \"module\": \"object\",\n",
    "        \"tags\": [\"video-filmarkivet-*\",],\n",
    "        \"object\": {\n",
    "            \"type\":\"video\",\n",
    "            \"source\": [\n",
    "                \"SF’s Weekly Review in December 1935 displayed the Sefyr \"\n",
    "                \"aircraft, in which the ten finalists of \"\n",
    "                \"_Stockholms-Tidningen’s_ Saint Lucy’s Day pageant were flown \"\n",
    "                \"from Bromma airfield in Stockholm.\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "By the end of 1935, Sefyr had been further integrated into promotional\n",
    "activities beyond journalistic duties. Sometimes these were rather curious (to\n",
    "say the least): in one newsreel Sefyr was depicted flying all ten (young women)\n",
    "finalists of _Stockholms-Tidningen’s_ Saint Lucy’s Day pageant, with the narrator\n",
    "playfully referring to the aircraft as “the winged steed Sefyr. He is the one\n",
    "who is going to ride off with that lovely cargo”. While the shivering\n",
    "contestants “brave the biting cold wind,” the sequence devoted considerable\n",
    "attention to displaying the young women before the camera, transforming a\n",
    "straightforward competition announcement into an elaborate publicity stunt.\n",
    "Observing two of the finalists, the narrator remarked that “number nine on the\n",
    "left and number eight on the right have apparently really fallen in love with\n",
    "Sefyr,” before adding that “Sefyr is a popular machine in the best sense of the\n",
    "word.” This sequence makes apparent what the deployment signaled: Sefyr did not\n",
    "serve merely as a piece of journalistic equipment, but also as a promotional\n",
    "asset, and a recognisable attraction in its own right; a celebrity status\n",
    "lending modernity and excitement to events sponsored by two media companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(Image(\"./media/img12.png\", width=800), metadata={\n",
    "    \"jdh\": {\n",
    "        \"module\": \"object\",\n",
    "        \"object\": {\n",
    "            \"tags\": [\"figure-advert-*\"],\n",
    "            \"type\":\"image\",\n",
    "            \"source\": [\n",
    "                \"“Boys! Would you like the big or the little Sefyr?” \"\n",
    "                \"Advertisement in _Dagens Nyheter_ during autumn 1937 for the \"\n",
    "                \"Sefyr model airplane, available for purchase at Nordiska \"\n",
    "                \"Kompaniet department store.\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hermeneutics"
    ]
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hermeneutics"
    ]
   },
   "source": [
    "For manual classification, we extracted the top thirty most-mentioned locations\n",
    "per year for 1930–1965, yielding 352 unique locations. We then generated a\n",
    "classification template and manually classified each location by type—city,\n",
    "region, country—assigning hierarchical parent locations where appropriate; such\n",
    "as Södermalm (Stockholm neighborhood) → Stockholm, and Göteborg →\n",
    "Västergötland. This parent-child structure enabled aggregation without\n",
    "modifying source data. The final location data was generated by summing\n",
    "mentions per location per year, applying parent-child aggregation (e.g.,\n",
    "neighbourhood → city → region), filtering by classification type, and ranking\n",
    "to produce annual top-ten lists. This five-stage pipeline transformed the noisy\n",
    "NER output into reliable, structured data suitable for historical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hermeneutics"
    ]
   },
   "source": [
    "Moreover, all foreign locations were aggregated into a single category,\n",
    "Foreign. For Swedish locations, we classified cities and towns into their\n",
    "respective historical regions (landskap), with one exception: Stockholm. The\n",
    "capital presents a unique case for several reasons. First, the city straddles\n",
    "two regions, Uppland and Södermanland, making regional classification\n",
    "ambiguous. Second, SF-newsreel voice-over frequently referenced localities\n",
    "within the Greater Stockholm metropolitan area—such as Lidingö—that, while\n",
    "formally a distinct municipality and town, function as an integrated part of\n",
    "the capital. When the voice-over mentioned Lidingö, the viewer would understand\n",
    "this as a reference to Stockholm rather than to a separate town or municipality\n",
    "in Uppland. We therefore classified Stockholm and its surrounding localities\n",
    "under Stockholm County (Stockholms län). Treating Stockholm as a distinct\n",
    "regional category also serves an analytical purpose: since SF and its newsreel\n",
    "production was headquartered in Stockholm, we wished to examine whether this\n",
    "might have resulted in a discernible capital-centricity in the geographic\n",
    "coverage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hermeneutics"
    ]
   },
   "source": [
    "We also retained certain historical regions that, while lacking formal\n",
    "administrative boundaries, carry significant cultural and economic meaning.\n",
    "Bergslagen, the traditional mining district spanning parts of five regions,\n",
    "appears frequently in the newsreels as a coherent entity, typically in\n",
    "connection with industrial reporting. Collapsing Bergslagen into its\n",
    "constituent provinces would obscure this discursive unity, and lose\n",
    "analytically valuable information about how the newsreels conceptualised\n",
    "Swedish geography. Similarly, we retained the north of Sweden, Norrland, as a\n",
    "category, given its frequent appearance in the material as a distinct region in\n",
    "its own right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_proportion_line_chart(data):\n",
    "    \"\"\"\n",
    "    Create line chart showing Stockholm's percentage over time using Plotly.\n",
    "\n",
    "    Args:\n",
    "        data: DataFrame with columns [year, region, mentions, total_mentions, percentage]\n",
    "              OR dict from calculate_proportions with keys ['years', 'stockholm_pct', ...]\n",
    "\n",
    "    Returns:\n",
    "        plotly.graph_objects.Figure\n",
    "    \"\"\"\n",
    "    # Handle dict vs DataFrame\n",
    "    if isinstance(data, dict):\n",
    "        # Dict format from calculate_proportions\n",
    "        years = data[\"years\"]\n",
    "        percentages = data[\"stockholm_pct\"]\n",
    "    elif isinstance(data, pd.DataFrame):\n",
    "        # Handle different DataFrame formats\n",
    "        if \"region\" in data.columns:\n",
    "            # Format from load_stockholm_data\n",
    "            stockholm_df = data[data[\"region\"] == \"Stockholm\"].sort_values(\n",
    "                \"year\"\n",
    "            )\n",
    "            years = stockholm_df[\"year\"]\n",
    "            percentages = stockholm_df[\"percentage\"]\n",
    "        elif \"stockholm_pct\" in data.columns:\n",
    "            # DataFrame with stockholm_pct column\n",
    "            data = data.sort_values(\"year\") if \"year\" in data.columns else data\n",
    "            years = data[\"year\"] if \"year\" in data.columns else data.index\n",
    "            percentages = data[\"stockholm_pct\"]\n",
    "        else:\n",
    "            # Assume Stockholm-only data with year and percentage columns\n",
    "            data = data.sort_values(\"year\")\n",
    "            years = data[\"year\"]\n",
    "            percentages = data[\"percentage\"]\n",
    "    else:\n",
    "        raise ValueError(\"data must be either a dict or pandas DataFrame\")\n",
    "\n",
    "    # Calculate mean percentage\n",
    "    mean_pct = (\n",
    "        percentages.mean()\n",
    "        if hasattr(percentages, \"mean\")\n",
    "        else sum(percentages) / len(percentages)\n",
    "    )\n",
    "\n",
    "    # Create figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add filled area\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=years,\n",
    "            y=percentages,\n",
    "            fill=\"tozeroy\",\n",
    "            fillcolor=\"rgba(46, 134, 171, 0.3)\",\n",
    "            line=dict(color=\"#2E86AB\", width=3),\n",
    "            mode=\"lines+markers+text\",\n",
    "            marker=dict(size=8),\n",
    "            text=[f\"{pct:.1f}%\" for pct in percentages],\n",
    "            textposition=\"top center\",\n",
    "            textfont=dict(color=\"#2E86AB\", size=10),\n",
    "            name=\"Stockholm\",\n",
    "            hovertemplate=\"Year: %{x}<br>Percentage: %{y:.1f}%<extra></extra>\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Add average line\n",
    "    fig.add_hline(\n",
    "        y=mean_pct,\n",
    "        line_dash=\"dash\",\n",
    "        line_color=\"red\",\n",
    "        line_width=2,\n",
    "        opacity=0.5,\n",
    "        annotation_text=f\"Average: {mean_pct:.1f}%\",\n",
    "        annotation_position=\"right\",\n",
    "    )\n",
    "\n",
    "    # Update layout\n",
    "    year_min = int(min(years))\n",
    "    year_max = int(max(years))\n",
    "    pct_max = (\n",
    "        max(percentages)\n",
    "        if isinstance(percentages, list)\n",
    "        else percentages.max()\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"Stockholm's Share of Regional Mentions ({year_min}-{year_max})\",\n",
    "        xaxis_title=\"Year\",\n",
    "        yaxis_title=\"Percentage of Total Mentions (%)\",\n",
    "        yaxis=dict(range=[0, pct_max * 1.15]),\n",
    "        hovermode=\"x unified\",\n",
    "        showlegend=False,\n",
    "    )\n",
    "\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_top_n_data(csv_file, start_year=None, end_year=None):\n",
    "    \"\"\"\n",
    "    Load top-N mention data from CSV.\n",
    "\n",
    "    Returns:\n",
    "        dict: {year: {location: mentions}}\n",
    "    \"\"\"\n",
    "    data_by_year = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    with open(csv_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            year = int(row[\"year\"])\n",
    "            if start_year is not None and year < start_year:\n",
    "                continue\n",
    "            if end_year is not None and year > end_year:\n",
    "                continue\n",
    "\n",
    "            location = row[\"location\"]\n",
    "            mentions = int(row[\"mentions\"])\n",
    "            data_by_year[year][location] += mentions\n",
    "\n",
    "    return data_by_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_proportions(data_by_year):\n",
    "    \"\"\"\n",
    "    Calculate Stockholm's proportion and other categories.\n",
    "\n",
    "    Returns:\n",
    "        dict: {\n",
    "            'years': [...],\n",
    "            'stockholm_pct': [...],\n",
    "            'stockholm_mentions': [...],\n",
    "            'other_mentions': [...],\n",
    "            'total_mentions': [...]\n",
    "        }\n",
    "    \"\"\"\n",
    "    years = sorted(data_by_year.keys())\n",
    "    stockholm_pct = []\n",
    "    stockholm_mentions = []\n",
    "    other_mentions = []\n",
    "    total_mentions = []\n",
    "\n",
    "    for year in years:\n",
    "        locations = data_by_year[year]\n",
    "        stockholm = locations.get(\"Stockholm\", 0)\n",
    "        total = sum(locations.values())\n",
    "        other = total - stockholm\n",
    "\n",
    "        stockholm_mentions.append(stockholm)\n",
    "        other_mentions.append(other)\n",
    "        total_mentions.append(total)\n",
    "\n",
    "        if total > 0:\n",
    "            pct = (stockholm / total) * 100\n",
    "        else:\n",
    "            pct = 0\n",
    "\n",
    "        stockholm_pct.append(pct)\n",
    "\n",
    "    return {\n",
    "        \"years\": years,\n",
    "        \"stockholm_pct\": stockholm_pct,\n",
    "        \"stockholm_mentions\": stockholm_mentions,\n",
    "        \"other_mentions\": other_mentions,\n",
    "        \"total_mentions\": total_mentions,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    create_proportion_line_chart(\n",
    "    calculate_proportions(load_top_n_data(\n",
    "        data / 'top10_regions_by_year_1930-1964.csv',\n",
    "        1930,\n",
    "        1964\n",
    "    )),\n",
    "),\n",
    "metadata={\n",
    "    \"jdh\": {\n",
    "        \"module\": \"object\",\n",
    "        \"object\": {\n",
    "            \"tags\": [\"figure-STHLM-share-*\"],\n",
    "            \"type\":\"image\",\n",
    "            \"source\": [\n",
    "                \"A chart detailing Stockholm’s proportional mentions across speech in newsreels within the Journal Digital corpus, 1930–64.\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Gazing on Lappland—from Stockholm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "As is evident from the chart above, there were few observable declines in\n",
    "Stockholm’s proportional representation in the SF-newsreels during the 1930s.\n",
    "Only in 1936 did the representation of Stockholm decrease, a fact that\n",
    "interestingly coincided with the usage of the airplane Sefyr. It prompted\n",
    "further inquiry into a potential causality. In order to further investigate the\n",
    "representation of Sweden in _SF’s Weekly Review_ we visualised the ten most\n",
    "mentioned regions per year 1933–1939, starting two years before and ending two\n",
    "years after the beforementioned decline—or rather the introduction of Sefyr.\n",
    "However, zooming in on the data did not present evidence to link Sefyr to any\n",
    "representational pattern. It did, however, bring to light another apparent\n",
    "pattern: how certain peripheral regions were represented, not as ordinary parts\n",
    "of Sweden, but as exotic and almost foreign. Lappland offered the most vivid\n",
    "example. Sweden’s largest and northernmost region—historically, also with the\n",
    "lowest population density—was actually the fifth most mentioned domestic region\n",
    "in SF-newsreels produced in 1935. What brought this remote periphery into such\n",
    "sudden visibility? The answer was not due to the flying capabilities of Sefyr,\n",
    "but a broader cultural fascination with the north as a space of leisure and\n",
    "national renewal. Lappland makes up a part of Norrland, the northern of the\n",
    "three lands (_landsdelar_) of Sweden. The mere fact that Norrland was mentioned\n",
    "106 times in the entire corpus was in itself revealing. In contrast, the other\n",
    "two lands, Svealand and Götaland, were only mentioned by name two times each\n",
    "throughout the corpus. It should be noted, however, that the term Norrland is\n",
    "semantically correct, but is nevertheless commonly used as a sweeping\n",
    "designation that reduces a large number of northern regions—together accounting\n",
    "for almost two-thirds of Sweden’s area—into a single, largely undifferentiated\n",
    "category. A linguistic pattern that suggests an urban gaze perceiving the\n",
    "northern parts of Sweden as a distant, homogenous other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## TODO: Figure 58"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_percentage_stacked_bars(data_by_year, regions):\n",
    "    \"\"\"\n",
    "    Create 100% stacked bar chart showing percentage distribution using Plotly.\n",
    "\n",
    "    Shows top 10 domestic Swedish regions, excluding Foreign.\n",
    "    Returns a Plotly Figure object.\n",
    "\n",
    "    Args:\n",
    "        data_by_year: Dict of {year: {location: mentions}}\n",
    "        regions: List of all region names\n",
    "\n",
    "    Returns:\n",
    "        plotly.graph_objects.Figure\n",
    "    \"\"\"\n",
    "    years = sorted(data_by_year.keys())\n",
    "\n",
    "    # Filter out \"Foreign\" from regions\n",
    "    domestic_regions = [r for r in regions if r != \"Foreign\"]\n",
    "\n",
    "    # Build percentage data (percentages calculated from total including Foreign)\n",
    "    percentages = defaultdict(list)\n",
    "    for year in years:\n",
    "        # Calculate total INCLUDING Foreign\n",
    "        total = sum(data_by_year[year].values())\n",
    "        for region in regions:  # All regions including Foreign\n",
    "            mentions = data_by_year[year].get(region, 0)\n",
    "            pct = (mentions / total * 100) if total > 0 else 0\n",
    "            percentages[region].append(pct)\n",
    "\n",
    "    # Use Plotly's built-in color palettes combined for maximum distinguishability\n",
    "    # Combine multiple qualitative color scales for better differentiation\n",
    "    all_colors = (\n",
    "        colors.qualitative.Safe + colors.qualitative.Antique\n",
    "    )\n",
    "\n",
    "    # Determine which regions are top 10 in each year\n",
    "    top10_by_year = {}\n",
    "    for year in years:\n",
    "        year_data = {\n",
    "            region: mentions\n",
    "            for region, mentions in data_by_year[year].items()\n",
    "            if region != \"Foreign\"\n",
    "        }\n",
    "        top10_this_year = sorted(\n",
    "            year_data.items(), key=lambda x: x[1], reverse=True\n",
    "        )[:10]\n",
    "        top10_by_year[year] = {region for region, _ in top10_this_year}\n",
    "\n",
    "    # Get all regions that appear in top 10 at least once\n",
    "    all_top10_regions = set()\n",
    "    for regions_set in top10_by_year.values():\n",
    "        all_top10_regions.update(regions_set)\n",
    "\n",
    "    # Calculate total mentions for each region across all years for sorting\n",
    "    region_totals = defaultdict(int)\n",
    "    for year_data in data_by_year.values():\n",
    "        for region, mentions in year_data.items():\n",
    "            if region != \"Foreign\" and region in all_top10_regions:\n",
    "                region_totals[region] += mentions\n",
    "\n",
    "    # Sort regions by total mentions (descending)\n",
    "    sorted_by_total = sorted(\n",
    "        region_totals.items(), key=lambda x: x[1], reverse=False\n",
    "    )\n",
    "\n",
    "    # Reverse the order so most abundant is at bottom (just below Foreign at top)\n",
    "    # Stack from bottom to top: least common, ..., 3rd, 2nd, 1st (then Foreign on top)\n",
    "    ordered_regions = [r for r, _ in reversed(sorted_by_total)]\n",
    "\n",
    "    # Create figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Track cumulative heights for positioning\n",
    "    cumulative = {year: 0.0 for year in years}\n",
    "    region_positions = defaultdict(list)\n",
    "\n",
    "    # Add bars for top 10 regions in custom order\n",
    "    color_idx = 0\n",
    "    for region in ordered_regions:\n",
    "        # For each year, only show bar if region is in top 10 AND >= 0.5%\n",
    "        visible_percentages = []\n",
    "        hover_texts = []\n",
    "\n",
    "        for j, year in enumerate(years):\n",
    "            pct = percentages[region][j]\n",
    "            mentions = data_by_year[year].get(region, 0)\n",
    "\n",
    "            # Get rank for this year\n",
    "            year_data = {\n",
    "                r: m for r, m in data_by_year[year].items() if r != \"Foreign\"\n",
    "            }\n",
    "            sorted_regions = sorted(\n",
    "                year_data.items(), key=lambda x: x[1], reverse=True\n",
    "            )\n",
    "            rank = next(\n",
    "                (\n",
    "                    i + 1\n",
    "                    for i, (r, _) in enumerate(sorted_regions)\n",
    "                    if r == region\n",
    "                ),\n",
    "                None,\n",
    "            )\n",
    "\n",
    "            if region in top10_by_year[year] and pct >= 0.5:\n",
    "                visible_percentages.append(pct)\n",
    "                hover_texts.append(\n",
    "                    f\"<b>{rank}. {region}</b><br>\"\n",
    "                    f\"Percentage: {pct:.1f}%<br>\"\n",
    "                    f\"Mentions: {mentions}\"\n",
    "                )\n",
    "            else:\n",
    "                visible_percentages.append(0)\n",
    "                hover_texts.append(\"\")\n",
    "\n",
    "        # Only add trace if region actually appears in the plot\n",
    "        if max(visible_percentages) > 0:\n",
    "            # Add bar trace with legend\n",
    "            fig.add_trace(\n",
    "                go.Bar(\n",
    "                    name=region,\n",
    "                    x=years,\n",
    "                    y=visible_percentages,\n",
    "                    marker=dict(\n",
    "                        color=all_colors[color_idx % len(all_colors)],\n",
    "                        line=dict(color=\"white\", width=0.5),\n",
    "                    ),\n",
    "                    showlegend=True,\n",
    "                    hovertemplate=\"%{text}<extra></extra>\",\n",
    "                    text=hover_texts,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Track positions for reference (not used for labels anymore)\n",
    "            for j, year in enumerate(years):\n",
    "                if visible_percentages[j] >= 0.5:\n",
    "                    midpoint = cumulative[year] + (visible_percentages[j] / 2)\n",
    "                    region_positions[region].append(\n",
    "                        (year, midpoint, visible_percentages[j])\n",
    "                    )\n",
    "                    cumulative[year] += visible_percentages[j]\n",
    "                else:\n",
    "                    region_positions[region].append((year, 0, 0))\n",
    "\n",
    "            color_idx += 1\n",
    "\n",
    "    # Add Foreign bar at the top (only if it has visible values)\n",
    "    if \"Foreign\" in percentages and max(percentages[\"Foreign\"]) > 0:\n",
    "        foreign_hover_texts = []\n",
    "        for j, year in enumerate(years):\n",
    "            foreign_pct = percentages[\"Foreign\"][j]\n",
    "            foreign_mentions = data_by_year[year].get(\"Foreign\", 0)\n",
    "            foreign_hover_texts.append(\n",
    "                f\"<b>Foreign</b><br>\"\n",
    "                f\"Percentage: {foreign_pct:.1f}%<br>\"\n",
    "                f\"Mentions: {foreign_mentions}\"\n",
    "            )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                name=\"Foreign\",\n",
    "                x=years,\n",
    "                y=percentages[\"Foreign\"],\n",
    "                marker=dict(\n",
    "                    color=\"#CCCCCC\", line=dict(color=\"white\", width=0.5)\n",
    "                ),\n",
    "                showlegend=True,\n",
    "                hovertemplate=\"%{text}<extra></extra>\",\n",
    "                text=foreign_hover_texts,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Add text annotations for total mentions only\n",
    "    annotations = []\n",
    "\n",
    "    # Add total mentions at top\n",
    "    for year in years:\n",
    "        total_mentions = sum(data_by_year[year].values())\n",
    "        annotations.append(\n",
    "            dict(\n",
    "                x=year,\n",
    "                y=103,\n",
    "                text=f\"n = {total_mentions}\",\n",
    "                showarrow=False,\n",
    "                font=dict(size=11, color=\"black\", family=\"Arial Black\"),\n",
    "                yanchor=\"bottom\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=f\"Top 10 Domestic Regional Distribution – Percentage Share (1933–1939)\",\n",
    "            font=dict(size=18, family=\"Arial Black\"),\n",
    "        ),\n",
    "        xaxis=dict(\n",
    "            title=dict(text=\"Year\", font=dict(size=14, family=\"Arial Black\")),\n",
    "            tickmode=\"linear\",\n",
    "            dtick=1,\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=dict(\n",
    "                text=\"Percentage of Mentions (%)\",\n",
    "                font=dict(size=14, family=\"Arial Black\"),\n",
    "            ),\n",
    "            range=[0, 108],\n",
    "            gridcolor=\"rgba(128,128,128,0.3)\",\n",
    "        ),\n",
    "        barmode=\"stack\",\n",
    "        height=800,\n",
    "        width=1000,\n",
    "        annotations=annotations,\n",
    "        plot_bgcolor=\"white\",\n",
    "        paper_bgcolor=\"white\",\n",
    "        margin=dict(t=100, b=150, l=80, r=50),\n",
    "        legend=dict(\n",
    "            title=dict(\n",
    "                text=\"Region\", font=dict(size=12, family=\"Arial Black\")\n",
    "            ),\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"top\",\n",
    "            y=-0.15,\n",
    "            xanchor=\"center\",\n",
    "            x=0.5,\n",
    "            font=dict(size=10),\n",
    "            traceorder=\"normal\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_regional_data_for_viz(csv_path: Path, start_year, end_year):\n",
    "    \"\"\"\n",
    "    Load and prepare regional data for visualization.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (data_by_year dict, regions list, df)\n",
    "        - data_by_year: {year: {region: mentions}}\n",
    "        - regions: sorted list of all regions\n",
    "        - df: pandas DataFrame with columns [year, region, mentions]\n",
    "    \"\"\"\n",
    "    data_by_year = load_top_n_data(csv_path, start_year, end_year)\n",
    "\n",
    "    # Get all regions\n",
    "    all_regions = set()\n",
    "    for year_data in data_by_year.values():\n",
    "        all_regions.update(year_data.keys())\n",
    "    regions = sorted(all_regions)\n",
    "\n",
    "    # Create DataFrame for easier manipulation\n",
    "    rows = []\n",
    "    for year, region_data in data_by_year.items():\n",
    "        for region, mentions in region_data.items():\n",
    "            rows.append({\"year\": year, \"region\": region, \"mentions\": mentions})\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    return data_by_year, regions, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    create_percentage_stacked_bars(\n",
    "*load_regional_data_for_viz(\n",
    "data / 'top10_regions_by_year_1930-1964.csv',\n",
    "        1930,\n",
    "        1939)[:2]), width=1000,\n",
    "    metadata={\n",
    "    \"jdh\": {\n",
    "        \"module\": \"object\",\n",
    "        \"object\": {\n",
    "            \"tags\": [\"figure-Regions-share-*\"],\n",
    "            \"type\":\"image\",\n",
    "            \"source\": [\n",
    "                \"Relative mentions of locations in swedish regions versus foreign countries 1933-1939.\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "l7pmr": [
       {
        "id": "22783102/XYQCBCR3",
        "source": "zotero"
       }
      ]
     }
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Norrland as a filmic other can also be revealed by a close reading of newsreel\n",
    "transcriptions mentioning Lappland. The region’s first appearance in the corpus\n",
    "occurred in 1930 (SF2789), when the Swedish Prince Wilhelm (1884–1965) read a\n",
    "self-composed prose poem about the Lapplandian nature, entitled _As the boat\n",
    "glides_ (_Medan båten glider_). In the poem, Lappland is portrayed through a\n",
    "distinctly romantic lens, formed as an exotic counterpart to modern\n",
    "civilisation. Prince Wilhelm—later a prolific documentary filmmaker and\n",
    "director—described the northern nature in colourful terms: mountains,\n",
    "waterfalls and wildlife. Alongside this sublime treatment of nature, the poem\n",
    "centres on Gottfrid, a local homesteader who guides the narrator on fishing\n",
    "trips. Gottfrid is characterised as a “calm, steady, and honest” man; a\n",
    "representative of the Norrbottnians’ fine, healthy stock”. The last phrase was\n",
    "not incidental; prince Wilhelm’s language was clearly linked to a broader\n",
    "discourse of racial vitalism prevalent in 1930s Sweden, in which the rural and\n",
    "peripheral populations—untouched by urban degeneration—were imagined as\n",
    "repositories of national vigour. The framing in the newsreels, hence, echoed\n",
    "contemporary debates around racial biology, which frequently idealised the\n",
    "Nordic peasant as a biological and moral ideal <cite id=\"l7pmr\"><a\n",
    "href=\"#zotero%7C22783102%2FXYQCBCR3\">(Broberg &#38; Tydén, 2005)</a></cite>.\n",
    "Furthermore, the poem was read by a prince—a figure of national continuity—lent\n",
    "these sentiments an authoritative, almost official, character. The wilderness\n",
    "of Lappland was not merely scenic, but regenerative: a space where the modern\n",
    "Swede might reconnect with an imagined ancestral vitality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(Image(\"./media/img13.png\", width=800), metadata={\n",
    "    \"jdh\": {\n",
    "        \"module\": \"object\",\n",
    "        \"object\": {\n",
    "            \"tags\": [\"figure-random-*\"],\n",
    "            \"type\":\"image\",\n",
    "            \"source\": [\n",
    "                \"Royalty and exotism—a popular combination. In the film As \"\n",
    "                \"the boat glides (Medan båten glider) from 1931, the Swedish \"\n",
    "                \"Prince Wilhelm read a prose poem celebrating the Lapplandian \"\n",
    "                \"nature, or as the subtitle of the film stated: \"\n",
    "                \"“an atmospheric image of Lapland’s sparkling lakes and miles \"\n",
    "                \"of desolate forests with their strange and captivating mystery”.\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "k06nf": [
       {
        "id": "22783102/KLXX4FNU",
        "source": "zotero"
       }
      ]
     }
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "In another travel feature from 1934 (SF3188), Lappland again appears as an\n",
    "exotic and curious periphery of Sweden. The narrator frames the journey as an\n",
    "adventure “towards the North,” yet continually expresses how accessible it has\n",
    "become by buses, trains, tourist stations, and experienced “Lapp guides”\n",
    "(“Lappar” is a historically used derogatory term for the Sami; the people\n",
    "indigenous to the area). The natural environment is again described in an\n",
    "exotifying sense—“eternal snowdrifts,” “wild landscapes,” associations with the\n",
    "tropics at a suspension bridge, the “feeling of superiority when one-eleventh\n",
    "of Sweden lies at one’s feet”—while at the same time being tamed into a\n",
    "recreational landscape for hiking, excursions, and having coffee on a veranda\n",
    "in the midnight sun. The Sami appear primarily as a picturesque and reliable\n",
    "service staff within the tourist infrastructure, almost like elements of the\n",
    "scenery in a similar way as the natural landscape <cite id=\"k06nf\"><a\n",
    "href=\"#zotero%7C22783102%2FKLXX4FNU\">(Nilsson et al., 2024)</a></cite>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Returning to our dataset, Lappland, as a geographical location, was not\n",
    "mentioned once in 1936 and 1937; and in 1938 and 1939, the region only appeared\n",
    "in passing. In one example from 1938, about the Swedes’ “industriousness for\n",
    "generations,” the voice-over laments: “From Skåne in the south to Lappland in\n",
    "the north, our national heritage meets us. We live in the era of industrialism.\n",
    "Our country can boast a strong, well-organised industry where capable workers\n",
    "\\[...\\] understand how to create products of great national economic value\n",
    "through cooperation” (SF3170A.1). In this newsreel, it is thus evident that\n",
    "Lappland was regarded as a fully integrated part of the Swedish industrial\n",
    "economy with strong nationalist overtones. The contrast between these\n",
    "representations during the 1930s is in many ways striking. In 1930 and\n",
    "1934–1935, Lappland appeared as a space apart—romantic, exotic, a destination\n",
    "for leisure and spiritual renewal. By 1938, the same region had been\n",
    "rhetorically absorbed into the national industrial project, its distinctiveness\n",
    "flattened into an allegory for Swedish territory as a whole. The shift suggests\n",
    "two parallel but not incompatible framings: Lappland as an escape from\n",
    "industrial modernity, and Lappland as proof of industrial modernity’s reach. In\n",
    "the first, the region’s value lay in its perceived distance from the modern; in\n",
    "the second, that distance had been overcome, and the north was now legible as\n",
    "part of a unified, productive nation. Tourism and industrialism were thus both\n",
    "articulations of the same modernisation project evident in _SF’s Weekly\n",
    "Review_—one offering recovery from its stresses, the other celebrating its\n",
    "triumphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Which locations? trend?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "7osps": [
       {
        "id": "22783102/NEKZIEHT",
        "source": "zotero"
       }
      ]
     }
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "If the peak in interest for Lappland in _SF’s Weekly Review_ stood out in 1935 it\n",
    "was perhaps due to an interest in the leisurely exploitation of the region.\n",
    "Scalable readings of the newsreel transcriptions, toward a background of\n",
    "previous historical research, gives evidence on how different geographic areas\n",
    "were represented. The 1930s was a formative period for mass tourism in Europe,\n",
    "largely facilitated by rapid infrastructural changes <cite id=\"7osps\"><a\n",
    "href=\"#zotero%7C22783102%2FNEKZIEHT\">(Barton, 2011)</a></cite>. By applying\n",
    "scalable reading of the Journal Digital collection, we have shown that this\n",
    "development was reflected in the case of Lappland in Swedish newsreels from the\n",
    "era. Newsreels mediated tourism not merely as a new practical possibility, but\n",
    "also as a cultural product of modern progress itself. Moreover, Sefyr\n",
    "exemplifies another dimension of how Swedish newsreels mediated modernity to\n",
    "their audiences: the technological conquest of distance, with the airplane\n",
    "serving as both practical infrastructure and promotional spectacle. Still,\n",
    "Sefyr’s flights were centripetal—bringing news back to Stockholm, reinforcing\n",
    "the capital as the hub through which national and international events were\n",
    "processed and distributed. A second, complementary dimension of this modern\n",
    "gaze moved in the opposite direction: centrifugal journeys outward from urban\n",
    "centres into Sweden’s peripheral regions, now made accessible through the same\n",
    "type of modern infrastructural developments that made rapid news delivery\n",
    "possible. If Sefyr symbolised the speed with which the modern press could\n",
    "collapse geographic distance, then the touristic representation of Lappland\n",
    "illustrated what that collapsed distance made available—not for journalists,\n",
    "but for ordinary Swedes seeking leisure and adventure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "In this article we have demonstrated how AI-powered transmediation of newsreels\n",
    "from video and audio to text opens up research avenues that were previously\n",
    "inaccessible. We have shown how scalable geographical reading of a\n",
    "transmediated corpus can be practically implemented: moving from data-driven\n",
    "overviews of geographic frequency to closer readings of individual\n",
    "transcriptions, and back again. This approach has allowed us to chart not only\n",
    "which places were made visible and how often, but also in what narrative\n",
    "contexts they appeared—thereby reconstructing a Swedish cultural landscape\n",
    "formed in part by mass news reporting, in a way that would have been\n",
    "prohibitively difficult using merely qualitative or quantitative methods alone\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Bibliography\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"cite2c-biblio\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<!-- BIBLIOGRAPHY START -->\n",
    "<div class=\"csl-bib-body\">\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|22783102/928EZRZ4\"></i>ALB. (1996). <i>Ledningsgrupp minnesanteckningar</i>.</div>\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|22783102/R86YMEXD\"></i>ALB. (1997). <i>Ledningsgrupp minnesanteckningar</i>.</div>\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|22783102/M63TBZ7V\"></i>ALB. (1998). <i>Det digitala journalfilmsarkivet, application</i>.</div>\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|22783102/BXDUCBBT\"></i>ALB. (1999). <i>IT-insatser och ökad tillgänglighet</i>.</div>\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|22783102/NEKVNJJD\"></i>Arnheim, R. (1957). <i>Film as Art</i> (2nd ed.). University of California Press.</div>\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|22783102/DSU6R48K\"></i>Asp, J. (2014). <i>Film för folket: Om Folkets Hus och filmen</i>. Premiss.</div>\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|22783102/2IBV4AM6\"></i>Balázs, B. (2010). <i>Early Film Theory: Visible Man and The Spirit of Film</i> (E. Carter, Ed.; R. Livingstone, Trans.). Berghahn Books.</div>\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|22783102/TEPVQNE3\"></i>Beck, J. (2011). The Evolution of Sound in Cinema. In W. H. Guynn (Ed.), <i>The Routledge Companion to Film History</i> (1st ed., pp. 64–76). Routledge.</div>\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|22783102/DW4S37Y5\"></i>Bergman, O. (1986). Hon ska göra TV:s guldgruva lönsam. <i>Dagens Nyheter</i>.</div>\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|22783102/PUAS8ZHL\"></i>Bhargav, S., Van Noord, N., &#38; Kamps, J. (2019). Deep Learning as a Tool for Early Cinema Analysis. <i>Proceedings of the 1st Workshop on Structuring and Understanding of Multimedia heritAge Contents</i>, 61–68. <a href=\"https://doi.org/10.1145/3347317.3357240\">https://doi.org/10.1145/3347317.3357240</a></div>\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|22783102/MFU9DVN9\"></i>Bradski, G. (2000). The OpenCV Library. <i>Dr. Dobb’s Journal of Software Tools</i>, <i>120</i>, 122–125.</div>\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|22783102/6AUNIX6I\"></i>Brownell, B. A. (1972). A Symbol of Modernity: Attitudes Toward the Automobile in Southern Cities in the 1920s. <i>American Quarterly</i>, <i>24</i>(1), 20–44. <a href=\"https://doi.org/10.2307/2711913\">https://doi.org/10.2307/2711913</a></div>\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|22783102/8BSVNXZQ\"></i>Chaume, F. (2020). Dubbing. In Ł. Bogucki &#38; M. Deckert (Eds.), <i>The Palgrave Handbook of Audiovisual Translation and Media Accessibility</i> (pp. 103–132). Palgrave Macmillan.</div>\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|22783102/JSI4ETR6\"></i>Dagend Nyheter. (1928). Tonfilmen fick en fin premiär i Konserthuset. <i>Dagens Nyheter</i>, 21.</div>\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|22783102/FDPKQ7P8\"></i>Dahlstedt, S. (1947). Utvidgad ljudavdelning vid SF:s ateljéer i Råsunda. <i>AGA-Nyheter</i>, <i>1</i>.</div>\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|22783102/8P2FL3R2\"></i>Dupré La Tour, C. (2005). Intertitles and Titles. In R. Abel (Ed.), <i>Encyclopedia of Early Cinema</i> (pp. 326–331).</div>\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|22783102/VL4UHIWF\"></i>Eriksson, M. (2024). <i>Zur Bedeutung des Skalierens beim Upscaling digitaler Bilder</i>. <i>33</i>(1).</div>\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|22783102/2RDITF4M\"></i>Eriksson, M., Skotare, T., &#38; Snickars, P. (2022). Understanding Gardar Sahlberg with neural nets: On algorithmic reuse of the Swedish SF archive. <i>Journal of Scandinavian Cinema</i>, <i>12</i>(3), 225–247. <a href=\"https://doi.org/10.1386/jsca_00075_1\">https://doi.org/10.1386/jsca_00075_1</a></div>\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|22783102/24HEASLG\"></i>Eriksson, M., Skotare, T., &#38; Snickars, P. (2024). Tracking and tracing audiovisual reuse: Introducing the Video Reuse Detector. <i>Journal of Digital History</i>, <i>3</i>(1). <a href=\"https://doi.org/10.1515/jdh-2024-0009\">https://doi.org/10.1515/jdh-2024-0009</a></div>\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|22783102/GVASD2H8\"></i>Gaines, J. M. (2024). The DH Dilemma: Knowing More &#38; Knowing for Sure vs. Never Knowing At All. In S.-M. Dang, T. Van Der Heijden, &#38; C. G. Olesen (Eds.), <i>Doing Digital Film History: Concepts, Tools, Practices</i> (pp. 17–46). De Gruyter.</div>\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|22783102/WHSQVQAB\"></i>Guyot, P., Malon, T., Roman-Jimenez, G., Chambon, S., Charvillat, V., Crouzil, A., Péninou, A., Pinquier, J., Sèdes, F., &#38; Sénac, C. (2019). Audiovisual Annotation Procedure for Multi-View Field Recordings. In I. Kompatsiaris, B. Huet, V. Mezaris, C. Gurrin, W.-H. Cheng, &#38; S. Vrochidis (Eds.), <i>MultiMedia Modeling</i> (Vol. 11295, pp. 399–410). Springer International Publishing. <a href=\"https://doi.org/10.1007/978-3-030-05710-7_33\">https://doi.org/10.1007/978-3-030-05710-7_33</a></div>\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|22783102/K4N8PIGZ\"></i>Malmstedt, J. (2025). <i>Sound Out of Time: Signal Archaeology of Swedish Public Service Radio 1980–1999</i> [Doctoral dissertation, Umeå University]. <a href=\"https://urn.kb.se/resolve?urn=urn:nbn:se:umu:diva-236882\">https://urn.kb.se/resolve?urn=urn:nbn:se:umu:diva-236882</a></div>\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|22783102/DVKUX2N4\"></i>Norrlander, S. (1964). <i>PM med plan och instruktion rörande SF:s journalarkiv</i>.</div>\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|22783102/UK5SM8C2\"></i>Offert, F. (2023). On the Concept of History (in Foundation Models). <i>IMAGE. Zeitschrift Für Interdisziplinäre Bildwissenschaft</i>, <i>19</i>(1), 121–134. <a href=\"https://doi.org/10.25969/MEDIAREP/22316\">https://doi.org/10.25969/MEDIAREP/22316</a></div>\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|22783102/6APYGVMY\"></i>Olsson, J. (2022). <i>The Life and Afterlife of Swedish Biograph: From Commercial Circulation to Archival Practices</i> (1st ed.). University of Wisconsin Press.</div>\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|22783102/NTI97CTV\"></i>Pronay, N. (1971). British Newsreels in the 1930s: Audience and Producers. <i>History</i>, <i>56</i>(188), 411–417. <a href=\"https://doi.org/10.1111/j.1468-229X.1971.tb02124.x\">https://doi.org/10.1111/j.1468-229X.1971.tb02124.x</a></div>\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|22783102/WQEJMM77\"></i>Reeves, N. (1986). <i>Official British Film Propaganda During the First World War</i>. Croom Helm.</div>\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|22783102/AZ3BT53M\"></i>Smith, R. (2007). An Overview of the Tesseract OCR Engine. <i>Ninth International Conference on Document Analysis and Recognition (ICDAR 2007) Vol 2</i>, 629–633. <a href=\"https://doi.org/10.1109/ICDAR.2007.4376991\">https://doi.org/10.1109/ICDAR.2007.4376991</a></div>\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|22783102/583554VC\"></i>Snickars, P. (2015). Remarks on a Failed Film Archival Project. <i>Journal of Scandinavian Cinema</i>, <i>5</i>(1), 63–67. <a href=\"https://doi.org/10.1386/jsca.5.1.63_1\">https://doi.org/10.1386/jsca.5.1.63_1</a></div>\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|22783102/BNUCIK7T\"></i>Snickars, P. (2024). <i>Audiovisuella arkiv: En svensk mediehistoria 1930–1990</i>. Mediehistoriskt arkiv.</div>\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|22783102/7HB6GAHN\"></i>Stjernholm, E., Eriksson, M., &#38; Mohammadi Norén, F. (2025). On the Historical Gaze of Generative AI: Visions of Scandinavia in Stable Diffusion. <i>Scandinavian Journal of History</i>, <i>50</i>(4), 458–488. <a href=\"https://doi.org/10.1080/03468755.2025.2511644\">https://doi.org/10.1080/03468755.2025.2511644</a></div>\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|22783102/BQLCIUEW\"></i>Stjernholm, E., &#38; Florin Persson, E. (2019). Ett filmbolag i det allmännas tjänst?: Svensk Filmindustri och skolfilmens flytande gränser. In F. Norén &#38; E. Stjernholm (Eds.), <i>Efterkrigstidens samhällskontakter</i> (pp. 41–72). Mediehistoriskt arkiv.</div>\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|22783102/URY9RMYJ\"></i>Thompson, E. (2004). <i>The Soundscape of Modernity: Architectural Acoustics and the Culture of Listening in America, 1900–1933</i>. MIT Press.</div>\n",
    "  <div class=\"csl-entry\"><i id=\"zotero|22783102/DBCYMUBB\"></i>Zhou, X., Yao, C., Wen, H., Wang, Y., Zhou, S., He, W., &#38; Liang, J. (2017). EAST: An Efficient and Accurate Scene Text Detector. <i>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>, 5551–5560. <a href=\"https://doi.org/10.1109/CVPR.2017.283\">https://doi.org/10.1109/CVPR.2017.283</a></div>\n",
    "</div>\n",
    "<!-- BIBLIOGRAPHY END -->"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "citation-manager": {
   "items": {
    "zotero": {
     "22783102/24HEASLG": {
      "DOI": "10.1515/jdh-2024-0009",
      "URL": "https://www.degruyter.com/document/doi/10.1515/jdh-2024-0009/html",
      "accessed": {
       "date-parts": [
        [
         2025,
         12,
         23
        ]
       ]
      },
      "author": [
       {
        "family": "Eriksson",
        "given": "Maria"
       },
       {
        "family": "Skotare",
        "given": "Tomas"
       },
       {
        "family": "Snickars",
        "given": "Pelle"
       }
      ],
      "container-title": "Journal of Digital History",
      "id": "22783102/24HEASLG",
      "issue": "1",
      "issued": {
       "date-parts": [
        [
         2024,
         6
        ]
       ]
      },
      "language": "en",
      "shortTitle": "Tracking and tracing audiovisual reuse",
      "system_id": "zotero|22783102/24HEASLG",
      "title": "Tracking and tracing audiovisual reuse: Introducing the Video Reuse Detector",
      "type": "article-journal",
      "volume": "3"
     },
     "22783102/2IBV4AM6": {
      "ISBN": "978-1-84545-660-3",
      "author": [
       {
        "family": "Balázs",
        "given": "Béla"
       }
      ],
      "editor": [
       {
        "family": "Carter",
        "given": "Erica"
       }
      ],
      "event-place": "New York and Oxford",
      "id": "22783102/2IBV4AM6",
      "issued": {
       "date-parts": [
        [
         2010
        ]
       ]
      },
      "publisher": "Berghahn Books",
      "publisher-place": "New York and Oxford",
      "shortTitle": "Early Film Theory",
      "system_id": "zotero|22783102/2IBV4AM6",
      "title": "Early Film Theory: Visible Man and The Spirit of Film",
      "translator": [
       {
        "family": "Livingstone",
        "given": "Rodney"
       }
      ],
      "type": "book"
     },
     "22783102/2RDITF4M": {
      "DOI": "10.1386/jsca_00075_1",
      "URL": "https://intellectdiscover.com/content/journals/10.1386/jsca_00075_1",
      "accessed": {
       "date-parts": [
        [
         2025,
         12,
         23
        ]
       ]
      },
      "author": [
       {
        "family": "Eriksson",
        "given": "Maria"
       },
       {
        "family": "Skotare",
        "given": "Tomas"
       },
       {
        "family": "Snickars",
        "given": "Pelle"
       }
      ],
      "container-title": "Journal of Scandinavian Cinema",
      "id": "22783102/2RDITF4M",
      "issue": "3",
      "issued": {
       "date-parts": [
        [
         2022,
         9
        ]
       ]
      },
      "language": "en",
      "page": "225–247",
      "shortTitle": "Understanding Gardar Sahlberg with neural nets",
      "system_id": "zotero|22783102/2RDITF4M",
      "title": "Understanding Gardar Sahlberg with neural nets: On algorithmic reuse of the Swedish SF archive",
      "type": "article-journal",
      "volume": "12"
     },
     "22783102/3TIXWE3B": {
      "ISBN": "978-3-11-108248-6",
      "accessed": {
       "date-parts": [
        [
         2025,
         12,
         19
        ]
       ]
      },
      "editor": [
       {
        "family": "Dang",
        "given": "Sarah-Mai"
       },
       {
        "family": "Van Der Heijden",
        "given": "Tim"
       },
       {
        "family": "Olesen",
        "given": "Christian Gosvig"
       }
      ],
      "event-place": "Berlin",
      "id": "22783102/3TIXWE3B",
      "issued": {
       "date-parts": [
        [
         2024
        ]
       ]
      },
      "publisher": "De Gruyter",
      "publisher-place": "Berlin",
      "shortTitle": "Doing Digital Film History",
      "system_id": "zotero|22783102/3TIXWE3B",
      "title": "Doing Digital Film History: Concepts, Tools, Practices",
      "type": "book"
     },
     "22783102/583554VC": {
      "DOI": "10.1386/jsca.5.1.63_1",
      "URL": "https://intellectdiscover.com/content/journals/10.1386/jsca.5.1.63_1",
      "abstract": "Abstract This brief article assesses an ongoing infrastructural research project focusing on filmarkivet.se, a website devoted to historical Swedish non-fiction film. As a collaboration between film researchers and film heritage institutions, the project has to date failed to overcome conflicting archival interests. Film scholars in general need open online archives and contextual resources, while some heritage institutions seek to give access solely to a curated filmic past. While cooperation between the heritage sector and scholars is regularly envisioned as being mutually beneficial, it also faces difficulties that need to be addressed and overcome.",
      "accessed": {
       "date-parts": [
        [
         2025,
         12,
         26
        ]
       ]
      },
      "author": [
       {
        "family": "Snickars",
        "given": "Pelle"
       }
      ],
      "container-title": "Journal of Scandinavian Cinema",
      "id": "22783102/583554VC",
      "issue": "1",
      "issued": {
       "date-parts": [
        [
         2015,
         3,
         1
        ]
       ]
      },
      "page": "63–67",
      "system_id": "zotero|22783102/583554VC",
      "title": "Remarks on a Failed Film Archival Project",
      "type": "article-journal",
      "volume": "5"
     },
     "22783102/5LZCK9F3": {
      "DOI": "10.5334/johd.344",
      "URL": "http://openhumanitiesdata.metajnl.com/articles/10.5334/johd.344/",
      "accessed": {
       "date-parts": [
        [
         2025,
         12,
         19
        ]
       ]
      },
      "author": [
       {
        "family": "Aspenskog",
        "given": "Robert"
       },
       {
        "family": "Johansson",
        "given": "Mathias"
       },
       {
        "family": "Snickars",
        "given": "Pelle"
       }
      ],
      "container-title": "Journal of Open Humanities Data",
      "id": "22783102/5LZCK9F3",
      "issued": {
       "date-parts": [
        [
         2025,
         8
        ]
       ]
      },
      "page": "44",
      "shortTitle": "Journal Digital Corpus",
      "system_id": "zotero|22783102/5LZCK9F3",
      "title": "Journal Digital Corpus: Swedish Newsreel Transcriptions",
      "type": "article-journal",
      "volume": "11"
     },
     "22783102/685XZA23": {
      "ISBN": "978-3-319-91920-1",
      "editor": [
       {
        "family": "Chambers",
        "given": "Ciara"
       },
       {
        "family": "Jönsson",
        "given": "Mats"
       },
       {
        "family": "Winkel",
        "given": "Roel vande"
       }
      ],
      "event-place": "Cham",
      "id": "22783102/685XZA23",
      "issued": {
       "date-parts": [
        [
         2018
        ]
       ]
      },
      "language": "eng",
      "publisher": "Palgrave Macmillan",
      "publisher-place": "Cham",
      "shortTitle": "Researching Newsreels",
      "system_id": "zotero|22783102/685XZA23",
      "title": "Researching Newsreels: Local, National and Transnational Case Studies",
      "type": "book"
     },
     "22783102/6APYGVMY": {
      "ISBN": "978-0-299-33990-6",
      "author": [
       {
        "family": "Olsson",
        "given": "Jan"
       }
      ],
      "edition": "1",
      "event-place": "Madison",
      "id": "22783102/6APYGVMY",
      "issued": {
       "date-parts": [
        [
         2022
        ]
       ]
      },
      "language": "eng",
      "publisher": "University of Wisconsin Press",
      "publisher-place": "Madison",
      "shortTitle": "The Life and Afterlife of Swedish Biograph",
      "system_id": "zotero|22783102/6APYGVMY",
      "title": "The Life and Afterlife of Swedish Biograph: From Commercial Circulation to Archival Practices",
      "type": "book"
     },
     "22783102/6AUNIX6I": {
      "DOI": "10.2307/2711913",
      "URL": "https://www.jstor.org/stable/2711913?origin=crossref",
      "accessed": {
       "date-parts": [
        [
         2025,
         12,
         23
        ]
       ]
      },
      "author": [
       {
        "family": "Brownell",
        "given": "B. A."
       }
      ],
      "container-title": "American Quarterly",
      "id": "22783102/6AUNIX6I",
      "issue": "1",
      "issued": {
       "date-parts": [
        [
         1972
        ]
       ]
      },
      "page": "20–44",
      "shortTitle": "A Symbol of Modernity",
      "system_id": "zotero|22783102/6AUNIX6I",
      "title": "A Symbol of Modernity: Attitudes Toward the Automobile in Southern Cities in the 1920s",
      "type": "article-journal",
      "volume": "24"
     },
     "22783102/6VFT9YB8": {
      "ISBN": "978-0-300-09359-9",
      "editor": [
       {
        "family": "Widenheim",
        "given": "Cecilia"
       }
      ],
      "event-place": "New Haven",
      "id": "22783102/6VFT9YB8",
      "issued": {
       "date-parts": [
        [
         2002
        ]
       ]
      },
      "language": "eng",
      "publisher": "Yale University Press",
      "publisher-place": "New Haven",
      "shortTitle": "Utopia & Reality",
      "system_id": "zotero|22783102/6VFT9YB8",
      "title": "Utopia & Reality: Modernity in Sweden 1900–1960",
      "type": "book"
     },
     "22783102/7HB6GAHN": {
      "DOI": "10.1080/03468755.2025.2511644",
      "URL": "https://www.tandfonline.com/doi/full/10.1080/03468755.2025.2511644",
      "author": [
       {
        "family": "Stjernholm",
        "given": "Emil"
       },
       {
        "family": "Eriksson",
        "given": "Maria"
       },
       {
        "family": "Mohammadi Norén",
        "given": "Fredrik"
       }
      ],
      "container-title": "Scandinavian Journal of History",
      "id": "22783102/7HB6GAHN",
      "issue": "4",
      "issued": {
       "date-parts": [
        [
         2025
        ]
       ]
      },
      "page": "458–488",
      "shortTitle": "On the Historical Gaze of Generative AI",
      "system_id": "zotero|22783102/7HB6GAHN",
      "title": "On the Historical Gaze of Generative AI: Visions of Scandinavia in Stable Diffusion",
      "type": "article-journal",
      "volume": "50"
     },
     "22783102/89SWX3HE": {
      "DOI": "10.21437/Interspeech.2023-78",
      "URL": "https://www.isca-archive.org/interspeech_2023/bain23_interspeech.html",
      "accessed": {
       "date-parts": [
        [
         2025,
         12,
         27
        ]
       ]
      },
      "author": [
       {
        "family": "Bain",
        "given": "Max"
       },
       {
        "family": "Huh",
        "given": "Jaesung"
       },
       {
        "family": "Han",
        "given": "Tengda"
       },
       {
        "family": "Zisserman",
        "given": "Andrew"
       }
      ],
      "container-title": "INTERSPEECH 2023",
      "event": "INTERSPEECH 2023",
      "id": "22783102/89SWX3HE",
      "issued": {
       "date-parts": [
        [
         2023,
         8,
         20
        ]
       ]
      },
      "language": "en",
      "page": "4489-4493",
      "publisher": "ISCA",
      "shortTitle": "WhisperX",
      "system_id": "zotero|22783102/89SWX3HE",
      "title": "WhisperX: Time-Accurate Speech Transcription of Long-Form Audio",
      "type": "paper-conference"
     },
     "22783102/8BSVNXZQ": {
      "ISBN": "978-3-030-42104-5",
      "abstract": "Intro – Contents – Notes on Contributors – List of Figures – List of Tables – 1: Capturing AVT and MA: Rationale, Facets and Objectives – References – Part I: Audiovisual Translation and Media Accessibility Within and Beyond Translation Studies – 2: An Excursus on Audiovisual Translation – References – 3: Audiovisual Translation through the Ages – 1 Introduction – 2 What Is AVT – 3 AVT through the Ages – 3.1 Silent Movies, Intertitles and Film Explainers – 3.2 Early Talkies and the Birth of the Main AVT Modalities – 3.3 Multilingual Films – 3.4 Dubbing – 3.5 Subtitling – 3.6 AVT and Accessibility: SDH and AD – 3.7 Audio Description – 3.8 Subtitles for the Deaf and Hard of Hearing Viewers – 3.9 AVT and the Internet: Fansubbing and Non-professional Subtitling, Crowdsourcing, Fandubbing – 4 Taking Stock: Current and New Trajectories – References – 4: Media Accessibility Within and Beyond Audiovisual Translation – 1 Introduction – 2 Theoretical Foundations: Accessibility and Human Rights – 3 Media Accessibility: Accounts and Definitions – 4 Media Accessibility: Epistemological and Methodological Shifts – 5 A First Classification of Media Accessibility Modalities and Services – 5.1 Translation-based – 5.2 Nontranslation-based – 6 Theoretical and Pedagogical Implications – 7 Future Prospects: Towards Accessibility Studies – 8 Suggested Reading – References – 5: Multimodality and Intersemiotic Translation – 1 Introduction – 2 Definitions – 3 A Historical View – 4 Theoretical Foundations – 5 Research on Multimodality and AVT – 6 Intersemiotic Translation – 7 Audio Description – 8 Tactile Exploration – 9 Subtitles for the Deaf and Hard of Hearing – 10 Conclusions – References – Filmography – Part II: Modes of Audiovisual Translation and Media Accessibility – 6: Dubbing",
      "author": [
       {
        "family": "Chaume",
        "given": "Frederic"
       }
      ],
      "container-title": "The Palgrave Handbook of Audiovisual Translation and Media Accessibility",
      "editor": [
       {
        "family": "Bogucki",
        "given": "Łukasz"
       },
       {
        "family": "Deckert",
        "given": "Mikołaj"
       }
      ],
      "event-place": "Cham",
      "id": "22783102/8BSVNXZQ",
      "issued": {
       "date-parts": [
        [
         2020
        ]
       ]
      },
      "language": "eng",
      "page": "103–132",
      "publisher": "Palgrave Macmillan",
      "publisher-place": "Cham",
      "system_id": "zotero|22783102/8BSVNXZQ",
      "title": "Dubbing",
      "type": "chapter"
     },
     "22783102/8P2FL3R2": {
      "author": [
       {
        "family": "Dupré La Tour",
        "given": "Claire"
       }
      ],
      "container-title": "Encyclopedia of Early Cinema",
      "editor": [
       {
        "family": "Abel",
        "given": "Richard"
       }
      ],
      "id": "22783102/8P2FL3R2",
      "issued": {
       "date-parts": [
        [
         2005
        ]
       ]
      },
      "page": "326-331",
      "system_id": "zotero|22783102/8P2FL3R2",
      "title": "Intertitles and Titles",
      "type": "chapter"
     },
     "22783102/928EZRZ4": {
      "author": [
       {
        "family": "ALB",
        "given": ""
       }
      ],
      "id": "22783102/928EZRZ4",
      "issued": {
       "date-parts": [
        [
         1996,
         11
        ]
       ]
      },
      "system_id": "zotero|22783102/928EZRZ4",
      "title": "Ledningsgrupp minnesanteckningar",
      "type": "article"
     },
     "22783102/94IX6F2L": {
      "ISBN": "978-91-7504-158-2",
      "author": [
       {
        "family": "Furhammar",
        "given": "Leif"
       }
      ],
      "edition": "3",
      "event-place": "Stockholm",
      "id": "22783102/94IX6F2L",
      "issued": {
       "date-parts": [
        [
         2003
        ]
       ]
      },
      "publisher": "Dialogos i samarbete med Svenska filminstitutet",
      "publisher-place": "Stockholm",
      "shortTitle": "Filmen i Sverige",
      "system_id": "zotero|22783102/94IX6F2L",
      "title": "Filmen i Sverige: En historia i tio kapitel och en fortsättning",
      "type": "book"
     },
     "22783102/AGCZGSA8": {
      "DOI": "10.48550/ARXIV.2205.03026",
      "URL": "https://arxiv.org/abs/2205.03026",
      "abstract": "This paper explains our work in developing new acoustic models for automated speech recognition (ASR) at KBLab, the infrastructure for data-driven research at the National Library of Sweden (KB). We evaluate different approaches for a viable speech-to-text pipeline for audiovisual resources in Swedish, using the wav2vec 2.0 architecture in combination with speech corpuses created from KB's collections. These approaches include pretraining an acoustic model for Swedish from the ground up, and fine-tuning existing monolingual and multilingual models. The collections-based corpuses we use have been sampled from millions of hours of speech, with a conscious attempt to balance regional dialects to produce a more representative, and thus more democratic, model. The acoustic model this enabled, \"VoxRex\", outperforms existing models for Swedish ASR. We also evaluate combining this model with various pretrained language models, which further enhanced performance. We conclude by highlighting the potential of such technology for cultural heritage institutions with vast collections of previously unlabelled audiovisual data. Our models are released for further exploration and research here: https://huggingface.co/KBLab.",
      "accessed": {
       "date-parts": [
        [
         2025,
         12,
         27
        ]
       ]
      },
      "author": [
       {
        "family": "Malmsten",
        "given": "Martin"
       },
       {
        "family": "Haffenden",
        "given": "Chris"
       },
       {
        "family": "Börjeson",
        "given": "Love"
       }
      ],
      "id": "22783102/AGCZGSA8",
      "issued": {
       "date-parts": [
        [
         2022
        ]
       ]
      },
      "publisher": "arXiv",
      "system_id": "zotero|22783102/AGCZGSA8",
      "title": "Hearing voices at the National Library -- a speech corpus and acoustic model for the Swedish language",
      "type": "article"
     },
     "22783102/AZ3BT53M": {
      "DOI": "10.1109/ICDAR.2007.4376991",
      "ISBN": "978-0-7695-2822-9",
      "URL": "http://ieeexplore.ieee.org/document/4376991/",
      "accessed": {
       "date-parts": [
        [
         2025,
         12,
         23
        ]
       ]
      },
      "author": [
       {
        "family": "Smith",
        "given": "Ray"
       }
      ],
      "container-title": "Ninth International Conference on Document Analysis and Recognition (ICDAR 2007) Vol 2",
      "event-place": "Curitiba, Parana, Brazil",
      "id": "22783102/AZ3BT53M",
      "issued": {
       "date-parts": [
        [
         2007
        ]
       ]
      },
      "page": "629–633",
      "publisher": "IEEE",
      "publisher-place": "Curitiba, Parana, Brazil",
      "system_id": "zotero|22783102/AZ3BT53M",
      "title": "An Overview of the Tesseract OCR Engine",
      "type": "paper-conference"
     },
     "22783102/BNUCIK7T": {
      "ISBN": "978-91-985802-8-0",
      "author": [
       {
        "family": "Snickars",
        "given": "Pelle"
       }
      ],
      "event-place": "Lund",
      "id": "22783102/BNUCIK7T",
      "issued": {
       "date-parts": [
        [
         2024
        ]
       ]
      },
      "publisher": "Mediehistoriskt arkiv",
      "publisher-place": "Lund",
      "shortTitle": "Audiovisuella arkiv",
      "system_id": "zotero|22783102/BNUCIK7T",
      "title": "Audiovisuella arkiv: En svensk mediehistoria 1930–1990",
      "type": "book"
     },
     "22783102/BQLCIUEW": {
      "ISBN": "978-91-985045-6-9",
      "author": [
       {
        "family": "Stjernholm",
        "given": "Emil"
       },
       {
        "family": "Florin Persson",
        "given": "Erik"
       }
      ],
      "container-title": "Efterkrigstidens samhällskontakter",
      "editor": [
       {
        "family": "Norén",
        "given": "Fredrik"
       },
       {
        "family": "Stjernholm",
        "given": "Emil"
       }
      ],
      "event-place": "Lund",
      "id": "22783102/BQLCIUEW",
      "issued": {
       "date-parts": [
        [
         2019
        ]
       ]
      },
      "page": "41–72",
      "publisher": "Mediehistoriskt arkiv",
      "publisher-place": "Lund",
      "shortTitle": "Ett filmbolag i det allmännas tjänst?",
      "system_id": "zotero|22783102/BQLCIUEW",
      "title": "Ett filmbolag i det allmännas tjänst?: Svensk Filmindustri och skolfilmens flytande gränser",
      "type": "chapter"
     },
     "22783102/BXDUCBBT": {
      "author": [
       {
        "family": "ALB",
        "given": ""
       }
      ],
      "id": "22783102/BXDUCBBT",
      "issued": {
       "date-parts": [
        [
         1999,
         6
        ]
       ]
      },
      "system_id": "zotero|22783102/BXDUCBBT",
      "title": "IT-insatser och ökad tillgänglighet",
      "type": "article"
     },
     "22783102/DBCYMUBB": {
      "DOI": "10.1109/CVPR.2017.283",
      "URL": "http://ieeexplore.ieee.org/document/8099766/",
      "abstract": "Previous approaches for scene text detection have already achieved promising performances across various benchmarks. However, they usually fall short when dealing with challenging scenarios, even when equipped with deep neural network models, because the overall performance is determined by the interplay of multiple stages and components in the pipelines. In this work, we propose a simple yet powerful pipeline that yields fast and accurate text detection in natural scenes. The pipeline directly predicts words or text lines of arbitrary orientations and quadrilateral shapes in full images, eliminating unnecessary intermediate steps (e.g., candidate aggregation and word partitioning), with a single neural network. The simplicity of our pipeline allows concentrating efforts on designing loss functions and neural network architecture. Experiments on standard datasets including ICDAR 2015, COCO-Text and MSRA-TD500 demonstrate that the proposed algorithm signiﬁcantly outperforms state-of-the-art methods in terms of both accuracy and efﬁciency. On the ICDAR 2015 dataset, the proposed algorithm achieves an F-score of 0.7820 at 13.2fps at 720p resolution.",
      "accessed": {
       "date-parts": [
        [
         2025,
         12,
         23
        ]
       ]
      },
      "author": [
       {
        "family": "Zhou",
        "given": "Xinyu"
       },
       {
        "family": "Yao",
        "given": "Cong"
       },
       {
        "family": "Wen",
        "given": "He"
       },
       {
        "family": "Wang",
        "given": "Yuzhi"
       },
       {
        "family": "Zhou",
        "given": "Shuchang"
       },
       {
        "family": "He",
        "given": "Weiran"
       },
       {
        "family": "Liang",
        "given": "Jiajun"
       }
      ],
      "container-title": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "event-place": "Honolulu, HI",
      "id": "22783102/DBCYMUBB",
      "issued": {
       "date-parts": [
        [
         2017
        ]
       ]
      },
      "page": "5551–5560",
      "publisher": "IEEE",
      "publisher-place": "Honolulu, HI",
      "shortTitle": "EAST",
      "system_id": "zotero|22783102/DBCYMUBB",
      "title": "EAST: An Efficient and Accurate Scene Text Detector",
      "type": "paper-conference"
     },
     "22783102/DSU6R48K": {
      "ISBN": "978-91-86743-26-0",
      "author": [
       {
        "family": "Asp",
        "given": "Jon"
       }
      ],
      "event-place": "Stockholm",
      "id": "22783102/DSU6R48K",
      "issued": {
       "date-parts": [
        [
         2014
        ]
       ]
      },
      "publisher": "Premiss",
      "publisher-place": "Stockholm",
      "shortTitle": "Film för folket",
      "system_id": "zotero|22783102/DSU6R48K",
      "title": "Film för folket: Om Folkets Hus och filmen",
      "type": "book"
     },
     "22783102/DVKUX2N4": {
      "author": [
       {
        "family": "Norrlander",
        "given": "S"
       }
      ],
      "id": "22783102/DVKUX2N4",
      "issued": {
       "date-parts": [
        [
         1964,
         4
        ]
       ]
      },
      "system_id": "zotero|22783102/DVKUX2N4",
      "title": "PM med plan och instruktion rörande SF:s journalarkiv",
      "type": "article"
     },
     "22783102/DW4S37Y5": {
      "author": [
       {
        "family": "Bergman",
        "given": "O"
       }
      ],
      "container-title": "Dagens Nyheter",
      "id": "22783102/DW4S37Y5",
      "issued": {
       "date-parts": [
        [
         1986,
         11
        ]
       ]
      },
      "system_id": "zotero|22783102/DW4S37Y5",
      "title": "Hon ska göra TV:s guldgruva lönsam",
      "type": "article-journal"
     },
     "22783102/ERJVIH9J": {
      "author": [
       {
        "family": "Aftonbladet",
        "given": ""
       }
      ],
      "container-title": "Aftonbladet",
      "id": "22783102/ERJVIH9J",
      "issued": {
       "date-parts": [
        [
         1936,
         12,
         26
        ]
       ]
      },
      "page": "27",
      "system_id": "zotero|22783102/ERJVIH9J",
      "title": "Stockholms-Tidningen skrives av unga begåvningar med tidens takt i blodet!",
      "type": "article-journal"
     },
     "22783102/FDPKQ7P8": {
      "author": [
       {
        "family": "Dahlstedt",
        "given": "Stellan"
       }
      ],
      "container-title": "AGA-nyheter",
      "id": "22783102/FDPKQ7P8",
      "issue": "1",
      "issued": {
       "date-parts": [
        [
         1947
        ]
       ]
      },
      "system_id": "zotero|22783102/FDPKQ7P8",
      "title": "Utvidgad ljudavdelning vid SF:s ateljéer i Råsunda",
      "type": "article-journal"
     },
     "22783102/GVASD2H8": {
      "ISBN": "978-3-11-108248-6",
      "accessed": {
       "date-parts": [
        [
         2025,
         12,
         19
        ]
       ]
      },
      "author": [
       {
        "family": "Gaines",
        "given": "J. M."
       }
      ],
      "container-title": "Doing Digital Film History: Concepts, Tools, Practices",
      "editor": [
       {
        "family": "Dang",
        "given": "Sarah-Mai"
       },
       {
        "family": "Van Der Heijden",
        "given": "Tim"
       },
       {
        "family": "Olesen",
        "given": "Christian Gosvig"
       }
      ],
      "event-place": "Berlin",
      "id": "22783102/GVASD2H8",
      "issued": {
       "date-parts": [
        [
         2024
        ]
       ]
      },
      "page": "17–46",
      "publisher": "De Gruyter",
      "publisher-place": "Berlin",
      "system_id": "zotero|22783102/GVASD2H8",
      "title": "The DH Dilemma: Knowing More & Knowing for Sure vs. Never Knowing At All",
      "type": "chapter"
     },
     "22783102/JSI4ETR6": {
      "author": [
       {
        "family": "Dagend Nyheter",
        "given": ""
       }
      ],
      "container-title": "Dagens Nyheter",
      "id": "22783102/JSI4ETR6",
      "issued": {
       "date-parts": [
        [
         1928,
         10,
         14
        ]
       ]
      },
      "page": "21",
      "system_id": "zotero|22783102/JSI4ETR6",
      "title": "Tonfilmen fick en fin premiär i Konserthuset",
      "type": "article-journal"
     },
     "22783102/K4N8PIGZ": {
      "URL": "https://urn.kb.se/resolve?urn=urn:nbn:se:umu:diva-236882",
      "author": [
       {
        "family": "Malmstedt",
        "given": "Johan"
       }
      ],
      "event-place": "Umeå",
      "genre": "Doctoral dissertation",
      "id": "22783102/K4N8PIGZ",
      "issued": {
       "date-parts": [
        [
         2025
        ]
       ]
      },
      "publisher": "Umeå University",
      "publisher-place": "Umeå",
      "system_id": "zotero|22783102/K4N8PIGZ",
      "title": "Sound Out of Time: Signal Archaeology of Swedish Public Service Radio 1980–1999",
      "type": "thesis"
     },
     "22783102/KLXX4FNU": {
      "ISBN": "978-91-8070-132-7",
      "editor": [
       {
        "family": "Nilsson",
        "given": "Ragnhild"
       },
       {
        "family": "Rohdin",
        "given": "Mats"
       },
       {
        "family": "Mörkenstam",
        "given": "Ulf"
       }
      ],
      "event-place": "Umeå",
      "id": "22783102/KLXX4FNU",
      "issued": {
       "date-parts": [
        [
         2024
        ]
       ]
      },
      "publisher": "Várdduo, Centrum för samisk forskning",
      "publisher-place": "Umeå",
      "system_id": "zotero|22783102/KLXX4FNU",
      "title": "Sápmi på film och TV",
      "type": "book"
     },
     "22783102/M22JCP98": {
      "URL": "https://modern36.github.io/jdc_reader/#q=sefyr",
      "accessed": {
       "date-parts": [
        [
         2026,
         1,
         1
        ]
       ]
      },
      "author": [
       {
        "family": "Modern Times 1936",
        "given": ""
       }
      ],
      "container-title": "Journal Digital Corpus Reader",
      "id": "22783102/M22JCP98",
      "issued": {
       "date-parts": [
        [
         2025,
         12,
         31
        ]
       ]
      },
      "system_id": "zotero|22783102/M22JCP98",
      "title": "Journal Digital Corpus Reader",
      "type": "webpage"
     },
     "22783102/M63TBZ7V": {
      "author": [
       {
        "family": "ALB",
        "given": ""
       }
      ],
      "id": "22783102/M63TBZ7V",
      "issued": {
       "date-parts": [
        [
         1998,
         7
        ]
       ]
      },
      "system_id": "zotero|22783102/M63TBZ7V",
      "title": "Det digitala journalfilmsarkivet, application",
      "type": "article"
     },
     "22783102/MFU9DVN9": {
      "author": [
       {
        "family": "Bradski",
        "given": "Gary"
       }
      ],
      "container-title": "Dr. Dobb’s Journal of Software Tools",
      "id": "22783102/MFU9DVN9",
      "issue": "120",
      "issued": {
       "date-parts": [
        [
         2000
        ]
       ]
      },
      "page": "122–125",
      "system_id": "zotero|22783102/MFU9DVN9",
      "title": "The OpenCV Library",
      "type": "article-journal"
     },
     "22783102/NEKVNJJD": {
      "ISBN": "978-0-520-24837-3",
      "author": [
       {
        "family": "Arnheim",
        "given": "Rudolf"
       }
      ],
      "edition": "2",
      "event-place": "Berkeley and Los Angeles",
      "id": "22783102/NEKVNJJD",
      "issued": {
       "date-parts": [
        [
         1957
        ]
       ]
      },
      "language": "eng",
      "publisher": "University of California Press",
      "publisher-place": "Berkeley and Los Angeles",
      "system_id": "zotero|22783102/NEKVNJJD",
      "title": "Film as Art",
      "type": "book"
     },
     "22783102/NEKZIEHT": {
      "ISBN": "978-0-7190-6591-0",
      "author": [
       {
        "family": "Barton",
        "given": "Susan"
       }
      ],
      "event-place": "Manchester and New York",
      "id": "22783102/NEKZIEHT",
      "issued": {
       "date-parts": [
        [
         2011
        ]
       ]
      },
      "language": "en",
      "publisher": "Manchester University Press",
      "publisher-place": "Manchester and New York",
      "shortTitle": "Working-Class Organisations and Popular Tourism",
      "system_id": "zotero|22783102/NEKZIEHT",
      "title": "Working-Class Organisations and Popular Tourism, 1840–1940",
      "type": "book"
     },
     "22783102/NTI97CTV": {
      "DOI": "10.1111/j.1468-229X.1971.tb02124.x",
      "URL": "https://onlinelibrary.wiley.com/doi/10.1111/j.1468-229X.1971.tb02124.x",
      "author": [
       {
        "family": "Pronay",
        "given": "Nicholas"
       }
      ],
      "container-title": "History",
      "id": "22783102/NTI97CTV",
      "issue": "188",
      "issued": {
       "date-parts": [
        [
         1971
        ]
       ]
      },
      "page": "411–417",
      "shortTitle": "British Newsreels in the 1930s",
      "system_id": "zotero|22783102/NTI97CTV",
      "title": "British Newsreels in the 1930s: Audience and Producers",
      "type": "article-journal",
      "volume": "56"
     },
     "22783102/PUAS8ZHL": {
      "DOI": "10.1145/3347317.3357240",
      "ISBN": "978-1-4503-6910-7",
      "URL": "https://dl.acm.org/doi/10.1145/3347317.3357240",
      "accessed": {
       "date-parts": [
        [
         2025,
         12,
         19
        ]
       ]
      },
      "author": [
       {
        "family": "Bhargav",
        "given": "Samarth"
       },
       {
        "family": "Van Noord",
        "given": "Nanne"
       },
       {
        "family": "Kamps",
        "given": "Jaap"
       }
      ],
      "container-title": "Proceedings of the 1st Workshop on Structuring and Understanding of Multimedia heritAge Contents",
      "event-place": "Nice",
      "id": "22783102/PUAS8ZHL",
      "issued": {
       "date-parts": [
        [
         2019,
         10
        ]
       ]
      },
      "language": "en",
      "page": "61–68",
      "publisher": "ACM",
      "publisher-place": "Nice",
      "system_id": "zotero|22783102/PUAS8ZHL",
      "title": "Deep Learning as a Tool for Early Cinema Analysis",
      "type": "paper-conference"
     },
     "22783102/R86YMEXD": {
      "author": [
       {
        "family": "ALB",
        "given": ""
       }
      ],
      "id": "22783102/R86YMEXD",
      "issued": {
       "date-parts": [
        [
         1997,
         8
        ]
       ]
      },
      "system_id": "zotero|22783102/R86YMEXD",
      "title": "Ledningsgrupp minnesanteckningar",
      "type": "article"
     },
     "22783102/SSCKKMRE": {
      "DOI": "10.64590/hxj",
      "URL": "https://newleftreview.org/issues/ii1/articles/franco-moretti-conjectures-on-world-literature",
      "accessed": {
       "date-parts": [
        [
         2025,
         12,
         23
        ]
       ]
      },
      "author": [
       {
        "family": "Moretti",
        "given": "Franco"
       }
      ],
      "container-title": "New Left Review",
      "id": "22783102/SSCKKMRE",
      "issued": {
       "date-parts": [
        [
         2000,
         1
        ]
       ]
      },
      "page": "54–68",
      "system_id": "zotero|22783102/SSCKKMRE",
      "title": "Conjectures on World Literature",
      "type": "article-journal"
     },
     "22783102/TEPVQNE3": {
      "ISBN": "978-0-415-77657-8",
      "author": [
       {
        "family": "Beck",
        "given": "Jay"
       }
      ],
      "container-title": "The Routledge Companion to Film History",
      "edition": "1",
      "editor": [
       {
        "family": "Guynn",
        "given": "William Howard"
       }
      ],
      "event-place": "London",
      "id": "22783102/TEPVQNE3",
      "issued": {
       "date-parts": [
        [
         2011
        ]
       ]
      },
      "language": "en",
      "page": "64–76",
      "publisher": "Routledge",
      "publisher-place": "London",
      "system_id": "zotero|22783102/TEPVQNE3",
      "title": "The Evolution of Sound in Cinema",
      "type": "chapter"
     },
     "22783102/UK5SM8C2": {
      "DOI": "10.25969/MEDIAREP/22316",
      "URL": "https://mediarep.org/entities/article/fcc4a926-1e5d-4434-a55e-cf779477cf67",
      "author": [
       {
        "family": "Offert",
        "given": "Fabian"
       }
      ],
      "container-title": "IMAGE. Zeitschrift für interdisziplinäre Bildwissenschaft",
      "id": "22783102/UK5SM8C2",
      "issue": "1",
      "issued": {
       "date-parts": [
        [
         2023
        ]
       ]
      },
      "page": "121–134",
      "system_id": "zotero|22783102/UK5SM8C2",
      "title": "On the Concept of History (in Foundation Models)",
      "type": "article-journal",
      "volume": "19"
     },
     "22783102/URY9RMYJ": {
      "ISBN": "0-262-70106-5",
      "author": [
       {
        "family": "Thompson",
        "given": "Emily"
       }
      ],
      "event-place": "Cambridge",
      "id": "22783102/URY9RMYJ",
      "issued": {
       "date-parts": [
        [
         2004
        ]
       ]
      },
      "publisher": "MIT Press",
      "publisher-place": "Cambridge",
      "shortTitle": "The Soundscape of Modernity",
      "system_id": "zotero|22783102/URY9RMYJ",
      "title": "The Soundscape of Modernity: Architectural Acoustics and the Culture of Listening in America, 1900–1933",
      "type": "book"
     },
     "22783102/VL4UHIWF": {
      "author": [
       {
        "family": "Eriksson",
        "given": "Maria"
       }
      ],
      "id": "22783102/VL4UHIWF",
      "issue": "1",
      "issued": {
       "date-parts": [
        [
         2024
        ]
       ]
      },
      "system_id": "zotero|22783102/VL4UHIWF",
      "title": "Zur Bedeutung des Skalierens beim Upscaling digitaler Bilder",
      "type": "article-journal",
      "volume": "33"
     },
     "22783102/WEZNWR96": {
      "DOI": "10.1515/jdh-2021-1008",
      "URL": "https://www.degruyter.com/document/doi/10.1515/jdh-2021-1008/html",
      "accessed": {
       "date-parts": [
        [
         2025,
         12,
         23
        ]
       ]
      },
      "author": [
       {
        "family": "Fickers",
        "given": "Andreas"
       },
       {
        "family": "Clavert",
        "given": "Frédéric"
       }
      ],
      "container-title": "Journal of Digital History",
      "id": "22783102/WEZNWR96",
      "issue": "1",
      "issued": {
       "date-parts": [
        [
         2021,
         9
        ]
       ]
      },
      "language": "en",
      "system_id": "zotero|22783102/WEZNWR96",
      "title": "On pyramids, prisms, and scalable reading",
      "type": "article-journal",
      "volume": "1"
     },
     "22783102/WHSQVQAB": {
      "ISBN": "978-3-030-05709-1 978-3-030-05710-7",
      "URL": "http://link.springer.com/10.1007/978-3-030-05710-7_33",
      "accessed": {
       "date-parts": [
        [
         2025,
         12,
         23
        ]
       ]
      },
      "author": [
       {
        "family": "Guyot",
        "given": "Patrice"
       },
       {
        "family": "Malon",
        "given": "Thierry"
       },
       {
        "family": "Roman-Jimenez",
        "given": "Geoffrey"
       },
       {
        "family": "Chambon",
        "given": "Sylvie"
       },
       {
        "family": "Charvillat",
        "given": "Vincent"
       },
       {
        "family": "Crouzil",
        "given": "Alain"
       },
       {
        "family": "Péninou",
        "given": "André"
       },
       {
        "family": "Pinquier",
        "given": "Julien"
       },
       {
        "family": "Sèdes",
        "given": "Florence"
       },
       {
        "family": "Sénac",
        "given": "Christine"
       }
      ],
      "container-title": "MultiMedia Modeling",
      "editor": [
       {
        "family": "Kompatsiaris",
        "given": "Ioannis"
       },
       {
        "family": "Huet",
        "given": "Benoit"
       },
       {
        "family": "Mezaris",
        "given": "Vasileios"
       },
       {
        "family": "Gurrin",
        "given": "Cathal"
       },
       {
        "family": "Cheng",
        "given": "Wen-Huang"
       },
       {
        "family": "Vrochidis",
        "given": "Stefanos"
       }
      ],
      "event-place": "Cham",
      "id": "22783102/WHSQVQAB",
      "issued": {
       "date-parts": [
        [
         2019
        ]
       ]
      },
      "language": "en",
      "note": "DOI: 10.1007/978-3-030-05710-7_33",
      "page": "399–410",
      "publisher": "Springer International Publishing",
      "publisher-place": "Cham",
      "system_id": "zotero|22783102/WHSQVQAB",
      "title": "Audiovisual Annotation Procedure for Multi-View Field Recordings",
      "type": "chapter",
      "volume": "11295"
     },
     "22783102/WQEJMM77": {
      "ISBN": "978-0-7099-4225-2",
      "author": [
       {
        "family": "Reeves",
        "given": "Nicholas"
       }
      ],
      "event-place": "London",
      "id": "22783102/WQEJMM77",
      "issued": {
       "date-parts": [
        [
         1986
        ]
       ]
      },
      "publisher": "Croom Helm",
      "publisher-place": "London",
      "system_id": "zotero|22783102/WQEJMM77",
      "title": "Official British Film Propaganda During the First World War",
      "type": "book"
     },
     "22783102/XYQCBCR3": {
      "ISBN": "978-91-7504-183-4",
      "author": [
       {
        "family": "Broberg",
        "given": "Gunnar"
       },
       {
        "family": "Tydén",
        "given": "Mattias"
       }
      ],
      "edition": "2",
      "event-place": "Stockholm",
      "id": "22783102/XYQCBCR3",
      "issued": {
       "date-parts": [
        [
         2005
        ]
       ]
      },
      "publisher": "Dialogos",
      "publisher-place": "Stockholm",
      "shortTitle": "Oönskade i folkhemmet",
      "system_id": "zotero|22783102/XYQCBCR3",
      "title": "Oönskade i folkhemmet: Rashygien och sterilisering i Sverige",
      "type": "book"
     },
     "22783102/YQMWCYSZ": {
      "author": [
       {
        "family": "Aftonbladet",
        "given": ""
       }
      ],
      "container-title": "Aftonbladet",
      "id": "22783102/YQMWCYSZ",
      "issued": {
       "date-parts": [
        [
         1936,
         12,
         26
        ]
       ]
      },
      "page": "30",
      "system_id": "zotero|22783102/YQMWCYSZ",
      "title": "Stockholms-Tidningen enda tidning i Norden med reportage-flygmaskin",
      "type": "article-journal"
     },
     "22783102/ZWKH378T": {
      "author": [
       {
        "family": "Skoglund",
        "given": "Gunnar"
       }
      ],
      "container-title": "Svensk Filmindustri tjugufem år: En bok om filmproduktion och biografrörelse / utgiven till jubileet av Aktiebolaget Svensk Filmindustri",
      "editor": [
       {
        "family": "Idestam-Almquist",
        "given": "Bengt"
       }
      ],
      "event-place": "Stockholm",
      "id": "22783102/ZWKH378T",
      "issued": {
       "date-parts": [
        [
         1944
        ]
       ]
      },
      "page": "149–160",
      "publisher-place": "Stockholm",
      "system_id": "zotero|22783102/ZWKH378T",
      "title": "Om kortfilm och journalreportage",
      "type": "chapter"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "jdh",
   "language": "python",
   "name": "jdh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
